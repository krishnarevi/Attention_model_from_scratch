{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiMYiTIe0BwB/2qc9WJ3Yy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/TSAI_END2.0_Session11/blob/main/NLP%20From%20Scratch%20Translation%20With%20A%20Sequence%20To%20Sequence%20Network%20And%20Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084kr_BCMrmE"
      },
      "source": [
        "## NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CeLBUXM2zL"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR-ov5I7M5w0",
        "outputId": "afc38838-7dc4-4317-c6cb-d675f8b16fa0"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-24 14:58:43--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.227.211.3, 13.227.211.25, 13.227.211.92, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.227.211.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip.2’\n",
            "\n",
            "\rdata.zip.2            0%[                    ]       0  --.-KB/s               \rdata.zip.2          100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-24 14:58:43 (81.0 MB/s) - ‘data.zip.2’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZEZefFZM79O",
        "outputId": "04c273ef-af23-4475-e707-9823ffcf5b10"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self,name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0:'SOS',1: 'EOS' }\n",
        "    self.n_words = 2 # Count SOS and EOS \n",
        "\n",
        "  #tokenize given sentence\n",
        "  def addSentence(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  # create vocabulary\n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2index: #if word is not present in word2index dictionary\n",
        "      self.word2index[word]=self.n_words\n",
        "      self.word2count[word]=1\n",
        "      self.index2word[self.n_words]=word\n",
        "      self.n_words += 1\n",
        "    else :\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# apply filterpair ,readLangs create vocabulary from pairs\n",
        "def prepareData(lang1,lang2,reverse = False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['nous sommes tous des laches .', 'we re all cowards .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG9NE_ziNE8L"
      },
      "source": [
        "After all of these steps, our data is ready. Let's explore it a bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYwVtlViNAnv",
        "outputId": "218fa5a5-e60c-4a17-8f87-400e970860d9"
      },
      "source": [
        "pairs[15:20]"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['je suis revenu .', 'i m back .'],\n",
              " ['me revoila .', 'i m back .'],\n",
              " ['je suis chauve .', 'i m bald .'],\n",
              " ['je suis occupe .', 'i m busy .'],\n",
              " ['je suis occupee .', 'i m busy .']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_yX_IbMNVwj",
        "outputId": "2df06193-aedc-4d15-8d57-64ec84ff9253"
      },
      "source": [
        "type(pairs)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCaR6robNbkB"
      },
      "source": [
        "Cool, so our data is actually a list of list, and each innter list element consists of fra to eng translation. \n",
        "\n",
        "# The architecture we are building\n",
        "\n",
        "![image](https://miro.medium.com/max/1838/1*tXchCn0hBSUau3WO0ViD7w.jpeg)\n",
        "\n",
        "As we can see here, we will have an encoder, an attention mechanism block and decoder. In the final code the attention mechanicm block and decoder will be merged into single block as we need both to work together. \n",
        "\n",
        "As we can see here, we need to create a copy of h1, h2, h3 and h4. These are encoder outputs for a sentence with 4 words. \n",
        "\n",
        "# Encoder\n",
        "\n",
        "We will build our encoder with a LSTM, but that's all we know. Let's NOT strait away build a class, but see how to come up with one for the Encoder. We need to answer few questions first:\n",
        "1. what would be the hidden size of our LSTM\n",
        "2. What would be the input size\n",
        "3. What would be the embedding dimesions. \n",
        "\n",
        "For simplicity, lets keep 1. and 3. to be 256. \n",
        "\n",
        "We can't feed our input directly to LSTM, we need to tensorize it, convert to embeddings first. \n",
        "\n",
        "`embedding = nn.Embedding(input_size, hidden_size) `\n",
        "\n",
        "## What is input_size?\n",
        "\n",
        "Remember the line below?\n",
        "\n",
        "`input_lang, output_lang, pairs = prepareData('eng', 'fra', True)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQWDgbeNnAg"
      },
      "source": [
        "We want to create an embedding layer that has embedding value for each of the word we have in out input layer.Also we will create an encoder LSTM layer.We need to place each layer we create in GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zk4U-UrNb_s"
      },
      "source": [
        "input_size = input_lang.n_words\n",
        "embedding_size =256\n",
        "hidden_size = 256\n",
        "embedding = nn. Embedding(input_size,embedding_size).to(device)\n",
        "lstm = nn. LSTM(embedding_size, hidden_size ).to(device)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivcFcyM3Okrq"
      },
      "source": [
        "Cool, now we need to feed data to our LSTM. We have input in the form of pairs already. Let's start from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iALoovJPOV9l",
        "outputId": "91dc6079-56b9-4282-d720-30cff6d126c8"
      },
      "source": [
        "sample = random.choice(pairs)\n",
        "sample"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['je ne vais pas jouer a ce jeu .', 'i m not going to play this game .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ycKTVVPtsw"
      },
      "source": [
        "Let's tokenize, convert into indices and append EOS token to input, output sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoBqh7q6PBIa",
        "outputId": "26dd2913-3bcd-45b5-dcec-7e7b4b054193"
      },
      "source": [
        "input_sentence = sample[0]\n",
        "output_sentence = sample[1]\n",
        "input_indices = [input_lang.word2index[x] for x in input_sentence.split(' ')]\n",
        "output_indices = [output_lang.word2index[x] for x in output_sentence.split(' ')]\n",
        "input_indices.append(EOS_token)\n",
        "output_indices.append(EOS_token)\n",
        "input_indices,output_indices"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 297, 7, 246, 2194, 115, 528, 2568, 5, 1],\n",
              " [2, 3, 147, 61, 532, 2070, 797, 1519, 4, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaM_XByRQVvx"
      },
      "source": [
        "Let's convert input and output samples to tensor before feeding to embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GI5vdnmPM5h",
        "outputId": "4084723c-4171-4d73-b636-b4382db77c51"
      },
      "source": [
        "input_tensor =torch.tensor(input_indices,dtype=torch.long ,device=device)\n",
        "output_tensor = torch.tensor(output_indices,dtype=torch.long,device=device)\n",
        "input_tensor,output_tensor "
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   6,  297,    7,  246, 2194,  115,  528, 2568,    5,    1],\n",
              "        device='cuda:0'),\n",
              " tensor([   2,    3,  147,   61,  532, 2070,  797, 1519,    4,    1],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKHrkmerQq_p",
        "outputId": "4e5d4e0c-96a3-4f57-de25-52b2d03fcf20"
      },
      "source": [
        "device"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHNJBCdhRvHX"
      },
      "source": [
        "We are working with 1 sample, but we would be working for a batch. Let's fix that by converting our input_tensor into a fake batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAZ0c4vxReyd",
        "outputId": "c558aed9-a846-4069-c3ea-82658946813f"
      },
      "source": [
        "embedded_input = embedding(input_tensor[0].view(-1,1)) #first word only\n",
        "embedded_input.shape"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wno9GUoSFoX"
      },
      "source": [
        "Let's build our LSTM now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmJmF4-ZRkVq",
        "outputId": "b3353526-6b91-4f27-ff16-ac2ac0ce5548"
      },
      "source": [
        "\n",
        "encoder_hidden,encoder_cell = torch.zeros((1,1,256),device=device), torch.zeros((1,1,256),device=device) # initialize encoder hidden and cell state with zero tensor\n",
        "encoder_outputs = torch.zeros(MAX_LENGTH, 256, device=device)\n",
        "\n",
        "\n",
        "for i in range(input_tensor.shape[0]) :\n",
        "    embedded_input = embedding(input_tensor[i].view(-1, 1))\n",
        "    output, (encoder_hidden,encoder_cell) = lstm(embedded_input, (encoder_hidden,encoder_cell))\n",
        "    encoder_outputs[i] += output[0,0]\n",
        "\n",
        "    print('\\033[1m' +\"Time step {}  \\033[0m\".format(i))\n",
        "    if (i<input_tensor.shape[0]-1):\n",
        "      print('Actual input word = {}'.format(input_sentence.split(\" \")[i]))\n",
        "    else:\n",
        "      print('Actual input word = {}'.format(\"<EOS>\"))\n",
        "    print('Embedding of input word {} = {}'.format(i, output[0,0]))\n",
        "    print('Encoder output at this time step = {}'.format(output[0,0]))\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTime step 0  \u001b[0m\n",
            "Actual input word = je\n",
            "Embedding of input word 0 = tensor([-0.0077, -0.0126, -0.0797, -0.2254,  0.2022, -0.0396,  0.1483, -0.1217,\n",
            "        -0.0240,  0.0375, -0.1355,  0.0307, -0.0029, -0.1346,  0.0156,  0.1302,\n",
            "         0.0217, -0.1207,  0.0344,  0.2652, -0.2008, -0.0738, -0.1199,  0.0748,\n",
            "         0.0500, -0.0573, -0.0034,  0.0999, -0.0279,  0.1133, -0.0982,  0.0235,\n",
            "        -0.1620,  0.2825,  0.0005,  0.0184,  0.1625,  0.0254, -0.0923,  0.0996,\n",
            "         0.0432,  0.0148,  0.0212, -0.1219, -0.1130,  0.0411,  0.0106,  0.0193,\n",
            "        -0.1754, -0.0707, -0.0491, -0.0064,  0.0845,  0.1418,  0.0909,  0.1922,\n",
            "         0.2086,  0.0315,  0.0747,  0.0184,  0.0332,  0.0288,  0.0548, -0.1106,\n",
            "         0.0132, -0.1752, -0.1316, -0.1528, -0.0625, -0.0524,  0.0842,  0.3359,\n",
            "         0.0046,  0.0319,  0.0549,  0.1653,  0.1270, -0.1793,  0.0486,  0.1240,\n",
            "        -0.0778, -0.0685, -0.0747, -0.2492,  0.0661,  0.0679,  0.1164,  0.1155,\n",
            "        -0.1437, -0.0858,  0.0388,  0.1976,  0.0901,  0.1093,  0.0618,  0.1188,\n",
            "        -0.1815, -0.0101, -0.2593, -0.1319, -0.0466, -0.0971, -0.0037,  0.0490,\n",
            "        -0.0314, -0.3508,  0.0582, -0.1232,  0.0231,  0.0581,  0.1549,  0.1195,\n",
            "         0.1678,  0.0559,  0.0257,  0.0503, -0.1909, -0.0311, -0.1089,  0.0159,\n",
            "        -0.1566,  0.0876,  0.1625,  0.0658, -0.1257, -0.1106, -0.0531,  0.0745,\n",
            "         0.0153,  0.1013, -0.0605,  0.0170, -0.1687,  0.2209, -0.0216,  0.0471,\n",
            "         0.0885,  0.0046,  0.0285, -0.0479,  0.0324, -0.0456, -0.1128,  0.0498,\n",
            "         0.1531,  0.1887, -0.0946, -0.1109, -0.0297,  0.0518, -0.1668, -0.0162,\n",
            "         0.1886, -0.1642, -0.1578,  0.1408, -0.0098,  0.0222,  0.2992, -0.2159,\n",
            "         0.2022, -0.1507, -0.1199, -0.0628, -0.0397,  0.0579, -0.0074,  0.0472,\n",
            "         0.0147,  0.0078, -0.0025,  0.0950,  0.0651,  0.1321, -0.0537,  0.0543,\n",
            "         0.0350, -0.1274,  0.0414, -0.0431, -0.1601, -0.1185, -0.1492, -0.0554,\n",
            "        -0.0243,  0.1467,  0.1071, -0.1163,  0.0833,  0.0429, -0.0741,  0.0197,\n",
            "         0.0914, -0.0065,  0.0513,  0.0446, -0.0627,  0.1364, -0.1727, -0.0595,\n",
            "        -0.0954, -0.0932, -0.0084,  0.1674,  0.0359,  0.0036,  0.2540,  0.0346,\n",
            "        -0.0142, -0.0119,  0.0149,  0.0315,  0.1465, -0.1333,  0.1616,  0.1040,\n",
            "         0.1973,  0.0593, -0.3240, -0.2452,  0.1663, -0.1018, -0.1727, -0.0533,\n",
            "        -0.1599, -0.1716,  0.0833, -0.3110, -0.0321, -0.2424, -0.0181,  0.0594,\n",
            "         0.0880,  0.1059, -0.2027, -0.1560, -0.0877,  0.1652,  0.1272, -0.1348,\n",
            "        -0.0584,  0.2235,  0.0239,  0.1515, -0.1425, -0.0410, -0.0322,  0.0339,\n",
            "        -0.1855, -0.0239, -0.0367, -0.1353,  0.0312,  0.1317,  0.0559,  0.1009],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.0077, -0.0126, -0.0797, -0.2254,  0.2022, -0.0396,  0.1483, -0.1217,\n",
            "        -0.0240,  0.0375, -0.1355,  0.0307, -0.0029, -0.1346,  0.0156,  0.1302,\n",
            "         0.0217, -0.1207,  0.0344,  0.2652, -0.2008, -0.0738, -0.1199,  0.0748,\n",
            "         0.0500, -0.0573, -0.0034,  0.0999, -0.0279,  0.1133, -0.0982,  0.0235,\n",
            "        -0.1620,  0.2825,  0.0005,  0.0184,  0.1625,  0.0254, -0.0923,  0.0996,\n",
            "         0.0432,  0.0148,  0.0212, -0.1219, -0.1130,  0.0411,  0.0106,  0.0193,\n",
            "        -0.1754, -0.0707, -0.0491, -0.0064,  0.0845,  0.1418,  0.0909,  0.1922,\n",
            "         0.2086,  0.0315,  0.0747,  0.0184,  0.0332,  0.0288,  0.0548, -0.1106,\n",
            "         0.0132, -0.1752, -0.1316, -0.1528, -0.0625, -0.0524,  0.0842,  0.3359,\n",
            "         0.0046,  0.0319,  0.0549,  0.1653,  0.1270, -0.1793,  0.0486,  0.1240,\n",
            "        -0.0778, -0.0685, -0.0747, -0.2492,  0.0661,  0.0679,  0.1164,  0.1155,\n",
            "        -0.1437, -0.0858,  0.0388,  0.1976,  0.0901,  0.1093,  0.0618,  0.1188,\n",
            "        -0.1815, -0.0101, -0.2593, -0.1319, -0.0466, -0.0971, -0.0037,  0.0490,\n",
            "        -0.0314, -0.3508,  0.0582, -0.1232,  0.0231,  0.0581,  0.1549,  0.1195,\n",
            "         0.1678,  0.0559,  0.0257,  0.0503, -0.1909, -0.0311, -0.1089,  0.0159,\n",
            "        -0.1566,  0.0876,  0.1625,  0.0658, -0.1257, -0.1106, -0.0531,  0.0745,\n",
            "         0.0153,  0.1013, -0.0605,  0.0170, -0.1687,  0.2209, -0.0216,  0.0471,\n",
            "         0.0885,  0.0046,  0.0285, -0.0479,  0.0324, -0.0456, -0.1128,  0.0498,\n",
            "         0.1531,  0.1887, -0.0946, -0.1109, -0.0297,  0.0518, -0.1668, -0.0162,\n",
            "         0.1886, -0.1642, -0.1578,  0.1408, -0.0098,  0.0222,  0.2992, -0.2159,\n",
            "         0.2022, -0.1507, -0.1199, -0.0628, -0.0397,  0.0579, -0.0074,  0.0472,\n",
            "         0.0147,  0.0078, -0.0025,  0.0950,  0.0651,  0.1321, -0.0537,  0.0543,\n",
            "         0.0350, -0.1274,  0.0414, -0.0431, -0.1601, -0.1185, -0.1492, -0.0554,\n",
            "        -0.0243,  0.1467,  0.1071, -0.1163,  0.0833,  0.0429, -0.0741,  0.0197,\n",
            "         0.0914, -0.0065,  0.0513,  0.0446, -0.0627,  0.1364, -0.1727, -0.0595,\n",
            "        -0.0954, -0.0932, -0.0084,  0.1674,  0.0359,  0.0036,  0.2540,  0.0346,\n",
            "        -0.0142, -0.0119,  0.0149,  0.0315,  0.1465, -0.1333,  0.1616,  0.1040,\n",
            "         0.1973,  0.0593, -0.3240, -0.2452,  0.1663, -0.1018, -0.1727, -0.0533,\n",
            "        -0.1599, -0.1716,  0.0833, -0.3110, -0.0321, -0.2424, -0.0181,  0.0594,\n",
            "         0.0880,  0.1059, -0.2027, -0.1560, -0.0877,  0.1652,  0.1272, -0.1348,\n",
            "        -0.0584,  0.2235,  0.0239,  0.1515, -0.1425, -0.0410, -0.0322,  0.0339,\n",
            "        -0.1855, -0.0239, -0.0367, -0.1353,  0.0312,  0.1317,  0.0559,  0.1009],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 1  \u001b[0m\n",
            "Actual input word = ne\n",
            "Embedding of input word 1 = tensor([-3.0305e-02,  3.7694e-02,  3.0864e-02, -3.9056e-02,  2.8671e-02,\n",
            "         1.7517e-01,  1.2107e-01, -8.0150e-02, -1.9217e-01,  3.3253e-03,\n",
            "        -2.9697e-01, -7.7618e-02,  1.8089e-01, -2.8413e-01,  1.4238e-01,\n",
            "         4.4699e-02,  6.8107e-02,  1.0066e-01, -5.4100e-02,  5.6840e-02,\n",
            "         2.7732e-02, -3.1711e-01,  1.4865e-01, -3.7703e-02,  8.5816e-02,\n",
            "        -1.1863e-01, -8.6223e-03, -9.6734e-03, -5.8366e-04,  3.8014e-02,\n",
            "        -1.7592e-01,  2.2739e-02,  1.3150e-01,  1.4589e-01, -3.0228e-03,\n",
            "         7.6171e-02, -4.3913e-02,  8.0587e-02, -2.9274e-02, -1.2337e-01,\n",
            "         1.1036e-01, -1.8046e-01,  1.3142e-01, -3.4802e-01, -3.7229e-02,\n",
            "        -1.3292e-01, -4.9211e-02,  2.6290e-02, -3.1135e-01,  1.2645e-01,\n",
            "        -1.5080e-01,  4.3625e-02,  3.3105e-02,  4.9387e-02,  1.1138e-02,\n",
            "         3.8271e-01,  5.0407e-03, -7.3291e-02, -7.8971e-02, -1.0280e-01,\n",
            "        -3.1646e-02,  2.3424e-01,  2.6274e-01, -1.8468e-01, -1.5207e-01,\n",
            "        -1.2150e-01, -1.1243e-01,  2.1658e-01, -1.2677e-02,  1.7980e-02,\n",
            "         6.2932e-02,  1.4329e-01,  5.9587e-03, -1.0345e-01,  1.7881e-01,\n",
            "         2.4014e-02, -3.8641e-02,  5.5746e-02,  8.7145e-02,  2.3612e-01,\n",
            "        -5.0616e-02, -3.8400e-02, -1.5322e-01, -1.8202e-01,  5.8360e-02,\n",
            "         3.2825e-01,  2.4026e-01,  2.0451e-01, -3.0773e-01,  1.2567e-01,\n",
            "         1.5091e-01,  2.2167e-01,  6.4176e-02, -2.0219e-02,  2.1282e-01,\n",
            "         3.6229e-01,  9.2716e-02, -5.2712e-02, -2.4072e-01,  7.5824e-02,\n",
            "        -5.9769e-02, -2.2925e-01, -2.3508e-01,  2.2204e-01,  3.2184e-02,\n",
            "        -2.2884e-01, -2.0680e-01, -6.0944e-03, -1.5827e-01,  9.1670e-02,\n",
            "         1.5052e-01,  8.2806e-02,  2.0282e-01,  5.3975e-02,  3.0020e-02,\n",
            "        -2.5687e-01, -8.2558e-02, -2.1634e-01,  1.1622e-01, -2.5890e-01,\n",
            "        -1.1661e-01,  3.8121e-03,  6.4024e-02,  6.9124e-02, -9.7166e-02,\n",
            "        -3.5579e-02,  9.7996e-02,  1.4634e-01,  7.3206e-02,  1.2615e-01,\n",
            "        -1.2315e-01, -1.1725e-01, -1.1920e-01,  3.2937e-01,  2.8553e-02,\n",
            "        -2.8121e-02,  5.5615e-02,  1.0382e-01,  4.7851e-02,  2.1057e-03,\n",
            "         3.3733e-02,  1.0336e-01, -6.1787e-02, -1.2162e-02,  1.6010e-01,\n",
            "         1.0411e-01, -1.4344e-01, -1.9172e-01,  2.8203e-02,  2.4970e-01,\n",
            "        -8.2186e-02,  5.0877e-02, -2.5270e-02, -1.0048e-01, -4.0504e-02,\n",
            "         7.8087e-02,  8.4244e-02, -8.5973e-02,  2.1341e-01, -6.1240e-02,\n",
            "         3.5512e-02, -1.6859e-01,  4.4553e-02, -1.7007e-01,  4.3023e-02,\n",
            "         3.8577e-03, -2.4335e-01, -6.9042e-02,  9.5058e-02, -2.0611e-02,\n",
            "         1.3292e-01,  1.5261e-01,  2.6665e-01,  3.9614e-02, -1.5694e-01,\n",
            "         4.3606e-02, -8.4237e-02,  1.5954e-02, -2.3135e-01, -1.3377e-03,\n",
            "        -3.2588e-02, -6.5666e-02, -2.6581e-01,  1.7667e-01, -7.5400e-02,\n",
            "        -5.6797e-02, -1.9771e-01, -2.6984e-01, -1.2775e-01,  2.4865e-01,\n",
            "        -1.1503e-01,  9.2206e-02,  6.6984e-02, -7.0141e-02, -2.8172e-01,\n",
            "         2.1681e-01, -1.9547e-01,  7.6768e-03, -2.8772e-02, -1.5661e-01,\n",
            "         1.1968e-01,  4.2182e-02,  5.8420e-02,  2.4670e-02,  8.9106e-02,\n",
            "         1.2659e-01,  2.6097e-01,  6.9148e-02, -1.7640e-01,  1.1584e-01,\n",
            "        -2.2605e-02, -1.4990e-01,  1.1148e-01,  2.7394e-04, -3.7362e-02,\n",
            "        -1.0834e-01,  4.4021e-01, -7.6258e-02,  5.7872e-02, -5.0859e-02,\n",
            "         1.1922e-01,  5.0538e-02, -3.7229e-01, -1.1490e-02, -9.2234e-02,\n",
            "        -1.0123e-01, -8.2394e-02, -1.5059e-01,  1.1023e-01, -1.7246e-01,\n",
            "        -5.3477e-02,  2.9653e-03, -1.6153e-01,  8.1870e-02, -1.3147e-01,\n",
            "        -2.8181e-01, -1.4494e-01,  2.9916e-01, -6.3313e-02, -7.8265e-02,\n",
            "        -3.8483e-02,  2.5539e-02, -1.8500e-01,  1.2806e-01, -1.7966e-01,\n",
            "         4.4629e-03,  4.0111e-03,  7.2859e-02,  1.1091e-01,  1.5510e-01,\n",
            "        -4.5166e-02,  1.1891e-01, -4.7062e-02,  5.7472e-02,  1.3160e-01,\n",
            "         3.8889e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-3.0305e-02,  3.7694e-02,  3.0864e-02, -3.9056e-02,  2.8671e-02,\n",
            "         1.7517e-01,  1.2107e-01, -8.0150e-02, -1.9217e-01,  3.3253e-03,\n",
            "        -2.9697e-01, -7.7618e-02,  1.8089e-01, -2.8413e-01,  1.4238e-01,\n",
            "         4.4699e-02,  6.8107e-02,  1.0066e-01, -5.4100e-02,  5.6840e-02,\n",
            "         2.7732e-02, -3.1711e-01,  1.4865e-01, -3.7703e-02,  8.5816e-02,\n",
            "        -1.1863e-01, -8.6223e-03, -9.6734e-03, -5.8366e-04,  3.8014e-02,\n",
            "        -1.7592e-01,  2.2739e-02,  1.3150e-01,  1.4589e-01, -3.0228e-03,\n",
            "         7.6171e-02, -4.3913e-02,  8.0587e-02, -2.9274e-02, -1.2337e-01,\n",
            "         1.1036e-01, -1.8046e-01,  1.3142e-01, -3.4802e-01, -3.7229e-02,\n",
            "        -1.3292e-01, -4.9211e-02,  2.6290e-02, -3.1135e-01,  1.2645e-01,\n",
            "        -1.5080e-01,  4.3625e-02,  3.3105e-02,  4.9387e-02,  1.1138e-02,\n",
            "         3.8271e-01,  5.0407e-03, -7.3291e-02, -7.8971e-02, -1.0280e-01,\n",
            "        -3.1646e-02,  2.3424e-01,  2.6274e-01, -1.8468e-01, -1.5207e-01,\n",
            "        -1.2150e-01, -1.1243e-01,  2.1658e-01, -1.2677e-02,  1.7980e-02,\n",
            "         6.2932e-02,  1.4329e-01,  5.9587e-03, -1.0345e-01,  1.7881e-01,\n",
            "         2.4014e-02, -3.8641e-02,  5.5746e-02,  8.7145e-02,  2.3612e-01,\n",
            "        -5.0616e-02, -3.8400e-02, -1.5322e-01, -1.8202e-01,  5.8360e-02,\n",
            "         3.2825e-01,  2.4026e-01,  2.0451e-01, -3.0773e-01,  1.2567e-01,\n",
            "         1.5091e-01,  2.2167e-01,  6.4176e-02, -2.0219e-02,  2.1282e-01,\n",
            "         3.6229e-01,  9.2716e-02, -5.2712e-02, -2.4072e-01,  7.5824e-02,\n",
            "        -5.9769e-02, -2.2925e-01, -2.3508e-01,  2.2204e-01,  3.2184e-02,\n",
            "        -2.2884e-01, -2.0680e-01, -6.0944e-03, -1.5827e-01,  9.1670e-02,\n",
            "         1.5052e-01,  8.2806e-02,  2.0282e-01,  5.3975e-02,  3.0020e-02,\n",
            "        -2.5687e-01, -8.2558e-02, -2.1634e-01,  1.1622e-01, -2.5890e-01,\n",
            "        -1.1661e-01,  3.8121e-03,  6.4024e-02,  6.9124e-02, -9.7166e-02,\n",
            "        -3.5579e-02,  9.7996e-02,  1.4634e-01,  7.3206e-02,  1.2615e-01,\n",
            "        -1.2315e-01, -1.1725e-01, -1.1920e-01,  3.2937e-01,  2.8553e-02,\n",
            "        -2.8121e-02,  5.5615e-02,  1.0382e-01,  4.7851e-02,  2.1057e-03,\n",
            "         3.3733e-02,  1.0336e-01, -6.1787e-02, -1.2162e-02,  1.6010e-01,\n",
            "         1.0411e-01, -1.4344e-01, -1.9172e-01,  2.8203e-02,  2.4970e-01,\n",
            "        -8.2186e-02,  5.0877e-02, -2.5270e-02, -1.0048e-01, -4.0504e-02,\n",
            "         7.8087e-02,  8.4244e-02, -8.5973e-02,  2.1341e-01, -6.1240e-02,\n",
            "         3.5512e-02, -1.6859e-01,  4.4553e-02, -1.7007e-01,  4.3023e-02,\n",
            "         3.8577e-03, -2.4335e-01, -6.9042e-02,  9.5058e-02, -2.0611e-02,\n",
            "         1.3292e-01,  1.5261e-01,  2.6665e-01,  3.9614e-02, -1.5694e-01,\n",
            "         4.3606e-02, -8.4237e-02,  1.5954e-02, -2.3135e-01, -1.3377e-03,\n",
            "        -3.2588e-02, -6.5666e-02, -2.6581e-01,  1.7667e-01, -7.5400e-02,\n",
            "        -5.6797e-02, -1.9771e-01, -2.6984e-01, -1.2775e-01,  2.4865e-01,\n",
            "        -1.1503e-01,  9.2206e-02,  6.6984e-02, -7.0141e-02, -2.8172e-01,\n",
            "         2.1681e-01, -1.9547e-01,  7.6768e-03, -2.8772e-02, -1.5661e-01,\n",
            "         1.1968e-01,  4.2182e-02,  5.8420e-02,  2.4670e-02,  8.9106e-02,\n",
            "         1.2659e-01,  2.6097e-01,  6.9148e-02, -1.7640e-01,  1.1584e-01,\n",
            "        -2.2605e-02, -1.4990e-01,  1.1148e-01,  2.7394e-04, -3.7362e-02,\n",
            "        -1.0834e-01,  4.4021e-01, -7.6258e-02,  5.7872e-02, -5.0859e-02,\n",
            "         1.1922e-01,  5.0538e-02, -3.7229e-01, -1.1490e-02, -9.2234e-02,\n",
            "        -1.0123e-01, -8.2394e-02, -1.5059e-01,  1.1023e-01, -1.7246e-01,\n",
            "        -5.3477e-02,  2.9653e-03, -1.6153e-01,  8.1870e-02, -1.3147e-01,\n",
            "        -2.8181e-01, -1.4494e-01,  2.9916e-01, -6.3313e-02, -7.8265e-02,\n",
            "        -3.8483e-02,  2.5539e-02, -1.8500e-01,  1.2806e-01, -1.7966e-01,\n",
            "         4.4629e-03,  4.0111e-03,  7.2859e-02,  1.1091e-01,  1.5510e-01,\n",
            "        -4.5166e-02,  1.1891e-01, -4.7062e-02,  5.7472e-02,  1.3160e-01,\n",
            "         3.8889e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 2  \u001b[0m\n",
            "Actual input word = vais\n",
            "Embedding of input word 2 = tensor([-0.2015, -0.0056, -0.0352,  0.0612,  0.0474,  0.1854,  0.0421, -0.0747,\n",
            "        -0.0846,  0.0747, -0.0915,  0.0934, -0.0269,  0.0328, -0.0765, -0.1260,\n",
            "         0.1347,  0.0791, -0.0312,  0.0684, -0.1724, -0.0233, -0.2403, -0.1687,\n",
            "         0.1835, -0.1667,  0.1287,  0.0461, -0.2041, -0.1078, -0.2228,  0.0412,\n",
            "         0.3016,  0.1369,  0.0348, -0.0387,  0.0779,  0.1526,  0.0879, -0.0167,\n",
            "         0.3181, -0.0197,  0.0194, -0.1361, -0.0562,  0.0602,  0.1556,  0.0078,\n",
            "        -0.0184,  0.0937, -0.0874,  0.0962,  0.0252,  0.1454,  0.0230,  0.0397,\n",
            "         0.0630, -0.0941, -0.1835, -0.0721, -0.0687,  0.0677,  0.0930, -0.0074,\n",
            "        -0.2915,  0.1137, -0.1361,  0.1459, -0.1826, -0.0798,  0.1163, -0.1918,\n",
            "         0.1106, -0.0970, -0.0422,  0.0098, -0.0664, -0.0664,  0.2876,  0.1460,\n",
            "         0.0148, -0.0777,  0.1757,  0.0029,  0.0216, -0.0096,  0.0115, -0.0295,\n",
            "        -0.3905, -0.1220,  0.1159,  0.0853,  0.0513, -0.1958,  0.3488,  0.0927,\n",
            "         0.0379,  0.1278, -0.3545,  0.0889, -0.0270, -0.2565, -0.2941,  0.0331,\n",
            "         0.0415, -0.2917, -0.0398,  0.1826, -0.1515, -0.1086,  0.0190,  0.0959,\n",
            "         0.2840, -0.0701,  0.1520, -0.0782, -0.1903, -0.0398, -0.0242, -0.2045,\n",
            "        -0.2412,  0.2124, -0.0006, -0.0758, -0.0070, -0.0768,  0.0988, -0.0323,\n",
            "        -0.0774,  0.0607,  0.0923, -0.1115, -0.1267,  0.3473,  0.1263, -0.2467,\n",
            "         0.0934,  0.1803, -0.0449,  0.1487,  0.0760, -0.0414, -0.2238,  0.0715,\n",
            "         0.0537, -0.0337,  0.0186, -0.0613, -0.1364, -0.0414, -0.0557,  0.1092,\n",
            "        -0.2154,  0.1437,  0.0581,  0.0468,  0.0238, -0.0762,  0.1983,  0.3029,\n",
            "        -0.1703, -0.1515,  0.0232, -0.0451,  0.2039, -0.0259, -0.2133, -0.0282,\n",
            "         0.0780,  0.2580,  0.2473,  0.0392,  0.1152, -0.0184, -0.4279,  0.0835,\n",
            "        -0.0474,  0.0706, -0.2383, -0.1369, -0.0056,  0.1088,  0.0074, -0.0955,\n",
            "        -0.1007, -0.1362,  0.0441, -0.2914, -0.0999,  0.0026, -0.2298,  0.0318,\n",
            "         0.1908, -0.2169, -0.1366,  0.0361, -0.1737,  0.0970,  0.0256, -0.1325,\n",
            "         0.1211,  0.0144,  0.1850,  0.0477,  0.1740,  0.2167,  0.2676,  0.0263,\n",
            "        -0.0434, -0.0884, -0.0112, -0.1029,  0.0561,  0.0565, -0.2449, -0.0485,\n",
            "         0.2984, -0.0564, -0.0007,  0.2080, -0.1568,  0.1597, -0.0725,  0.1935,\n",
            "        -0.1708, -0.2378, -0.1415, -0.1106,  0.2134,  0.0780, -0.0053,  0.1755,\n",
            "         0.1248,  0.1060,  0.0049, -0.1346, -0.1437,  0.1197, -0.1385, -0.1207,\n",
            "         0.2233, -0.0121, -0.1947,  0.0503,  0.0490,  0.0286,  0.0023,  0.0076,\n",
            "         0.3773, -0.0784,  0.0972,  0.1510,  0.0518,  0.0395,  0.0952,  0.0263],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.2015, -0.0056, -0.0352,  0.0612,  0.0474,  0.1854,  0.0421, -0.0747,\n",
            "        -0.0846,  0.0747, -0.0915,  0.0934, -0.0269,  0.0328, -0.0765, -0.1260,\n",
            "         0.1347,  0.0791, -0.0312,  0.0684, -0.1724, -0.0233, -0.2403, -0.1687,\n",
            "         0.1835, -0.1667,  0.1287,  0.0461, -0.2041, -0.1078, -0.2228,  0.0412,\n",
            "         0.3016,  0.1369,  0.0348, -0.0387,  0.0779,  0.1526,  0.0879, -0.0167,\n",
            "         0.3181, -0.0197,  0.0194, -0.1361, -0.0562,  0.0602,  0.1556,  0.0078,\n",
            "        -0.0184,  0.0937, -0.0874,  0.0962,  0.0252,  0.1454,  0.0230,  0.0397,\n",
            "         0.0630, -0.0941, -0.1835, -0.0721, -0.0687,  0.0677,  0.0930, -0.0074,\n",
            "        -0.2915,  0.1137, -0.1361,  0.1459, -0.1826, -0.0798,  0.1163, -0.1918,\n",
            "         0.1106, -0.0970, -0.0422,  0.0098, -0.0664, -0.0664,  0.2876,  0.1460,\n",
            "         0.0148, -0.0777,  0.1757,  0.0029,  0.0216, -0.0096,  0.0115, -0.0295,\n",
            "        -0.3905, -0.1220,  0.1159,  0.0853,  0.0513, -0.1958,  0.3488,  0.0927,\n",
            "         0.0379,  0.1278, -0.3545,  0.0889, -0.0270, -0.2565, -0.2941,  0.0331,\n",
            "         0.0415, -0.2917, -0.0398,  0.1826, -0.1515, -0.1086,  0.0190,  0.0959,\n",
            "         0.2840, -0.0701,  0.1520, -0.0782, -0.1903, -0.0398, -0.0242, -0.2045,\n",
            "        -0.2412,  0.2124, -0.0006, -0.0758, -0.0070, -0.0768,  0.0988, -0.0323,\n",
            "        -0.0774,  0.0607,  0.0923, -0.1115, -0.1267,  0.3473,  0.1263, -0.2467,\n",
            "         0.0934,  0.1803, -0.0449,  0.1487,  0.0760, -0.0414, -0.2238,  0.0715,\n",
            "         0.0537, -0.0337,  0.0186, -0.0613, -0.1364, -0.0414, -0.0557,  0.1092,\n",
            "        -0.2154,  0.1437,  0.0581,  0.0468,  0.0238, -0.0762,  0.1983,  0.3029,\n",
            "        -0.1703, -0.1515,  0.0232, -0.0451,  0.2039, -0.0259, -0.2133, -0.0282,\n",
            "         0.0780,  0.2580,  0.2473,  0.0392,  0.1152, -0.0184, -0.4279,  0.0835,\n",
            "        -0.0474,  0.0706, -0.2383, -0.1369, -0.0056,  0.1088,  0.0074, -0.0955,\n",
            "        -0.1007, -0.1362,  0.0441, -0.2914, -0.0999,  0.0026, -0.2298,  0.0318,\n",
            "         0.1908, -0.2169, -0.1366,  0.0361, -0.1737,  0.0970,  0.0256, -0.1325,\n",
            "         0.1211,  0.0144,  0.1850,  0.0477,  0.1740,  0.2167,  0.2676,  0.0263,\n",
            "        -0.0434, -0.0884, -0.0112, -0.1029,  0.0561,  0.0565, -0.2449, -0.0485,\n",
            "         0.2984, -0.0564, -0.0007,  0.2080, -0.1568,  0.1597, -0.0725,  0.1935,\n",
            "        -0.1708, -0.2378, -0.1415, -0.1106,  0.2134,  0.0780, -0.0053,  0.1755,\n",
            "         0.1248,  0.1060,  0.0049, -0.1346, -0.1437,  0.1197, -0.1385, -0.1207,\n",
            "         0.2233, -0.0121, -0.1947,  0.0503,  0.0490,  0.0286,  0.0023,  0.0076,\n",
            "         0.3773, -0.0784,  0.0972,  0.1510,  0.0518,  0.0395,  0.0952,  0.0263],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 3  \u001b[0m\n",
            "Actual input word = pas\n",
            "Embedding of input word 3 = tensor([-1.0258e-01,  9.4131e-02, -2.0600e-01,  2.0763e-01,  1.1106e-02,\n",
            "         1.5808e-01, -1.0047e-01, -1.2841e-01,  1.2442e-01, -1.2903e-01,\n",
            "        -1.2073e-01, -3.8196e-02, -2.8247e-02, -1.5487e-02, -1.7953e-01,\n",
            "        -1.4977e-01,  4.8387e-03, -1.0272e-01, -4.0294e-03, -1.4952e-01,\n",
            "        -1.1002e-01, -1.6555e-01, -5.8055e-02,  6.6843e-02,  3.8886e-01,\n",
            "         1.2719e-01,  1.7506e-01,  3.6207e-02, -3.1860e-02, -1.3146e-01,\n",
            "        -2.1170e-01,  2.5449e-01,  1.9106e-01,  1.7773e-01,  1.0170e-01,\n",
            "        -1.0286e-01, -6.9797e-02, -1.5851e-02, -8.8353e-03, -1.7589e-02,\n",
            "         1.1156e-01,  1.4455e-01,  1.0712e-01, -2.0596e-01, -2.6225e-01,\n",
            "        -7.2089e-02,  3.3089e-02,  4.1166e-02,  1.0257e-01,  1.7232e-01,\n",
            "         2.3017e-02, -1.9643e-02, -5.5104e-02, -2.3249e-02,  9.2633e-02,\n",
            "        -1.4451e-01,  7.0210e-02, -5.1271e-02, -1.6033e-03,  4.5768e-03,\n",
            "        -4.4537e-02, -1.0478e-01, -8.2380e-02, -1.3382e-01, -1.6062e-02,\n",
            "         2.1435e-01, -7.1988e-02, -9.7558e-02,  8.0644e-02,  2.1992e-01,\n",
            "        -3.0702e-02, -1.7826e-01,  3.3817e-02,  7.4431e-02,  4.3247e-02,\n",
            "        -1.6271e-01, -1.6082e-01, -5.6487e-02,  2.9613e-02, -2.9238e-01,\n",
            "         5.8518e-02, -4.6642e-02,  2.1637e-01,  1.2856e-01, -1.2087e-01,\n",
            "        -2.7351e-02,  4.3393e-02,  1.9233e-01, -1.8787e-01, -1.1399e-01,\n",
            "         4.1254e-01, -2.4431e-02,  1.4556e-01, -8.9112e-02,  1.4948e-01,\n",
            "         1.0685e-01, -7.3819e-02,  1.0337e-01, -1.3618e-01,  1.7210e-01,\n",
            "        -1.2949e-02,  1.8114e-02, -3.0354e-01,  6.4743e-02,  8.2768e-02,\n",
            "        -5.8902e-02,  1.0840e-01,  8.2511e-02, -3.5654e-02, -1.0586e-01,\n",
            "         2.3079e-02, -5.9153e-02,  2.6655e-01, -1.0443e-01,  7.3986e-02,\n",
            "         1.4858e-01, -1.9814e-01, -6.4377e-02,  3.7920e-02,  1.3428e-01,\n",
            "        -2.4971e-01, -1.3505e-01, -7.8080e-02, -9.2601e-03, -8.5932e-02,\n",
            "        -3.5824e-01,  1.9048e-01, -8.9904e-03, -8.5226e-02, -9.2921e-03,\n",
            "        -2.2349e-02,  3.0389e-02, -1.9144e-01,  3.8622e-01,  9.5920e-02,\n",
            "        -2.0634e-02,  1.7635e-01,  1.8859e-01,  1.1652e-01,  1.0182e-01,\n",
            "        -3.5805e-02,  1.7074e-01, -5.2204e-02, -1.3607e-01, -1.0606e-01,\n",
            "        -1.3175e-01, -6.1943e-03, -1.3730e-01, -2.3402e-01, -5.7972e-02,\n",
            "         4.1603e-02,  2.0046e-02, -1.4194e-01,  3.5245e-04,  1.0857e-01,\n",
            "        -1.2941e-01,  7.5972e-02,  9.3793e-02,  4.3817e-02,  2.4971e-01,\n",
            "        -1.6950e-01, -1.0565e-01, -1.1187e-02, -6.3562e-03,  3.0493e-01,\n",
            "        -7.4252e-02, -1.4566e-01, -2.8644e-02,  5.7506e-02, -9.1729e-03,\n",
            "         1.3179e-01,  2.7824e-01,  1.5157e-01, -1.4399e-01, -1.2910e-01,\n",
            "        -1.1293e-02, -5.1211e-02,  4.6543e-02,  2.7501e-02,  1.7630e-01,\n",
            "        -1.2654e-01,  1.8323e-01,  2.1264e-01, -7.4742e-02,  2.0894e-02,\n",
            "         1.7758e-01, -8.6731e-03, -2.4261e-01, -1.1766e-01,  1.7881e-02,\n",
            "        -1.9909e-02,  1.5834e-01, -4.0719e-02, -2.0799e-01,  1.1732e-01,\n",
            "         1.1061e-01,  1.8921e-02,  2.7577e-01,  5.3323e-02, -1.8915e-01,\n",
            "         9.6332e-02, -8.9438e-02,  2.3768e-02,  8.3528e-03, -1.0785e-03,\n",
            "         1.8509e-01,  2.3513e-01, -4.1937e-02, -2.3750e-02, -1.0966e-01,\n",
            "         7.2010e-02,  5.2217e-02,  2.3376e-01,  9.5348e-02, -1.4187e-01,\n",
            "         1.3407e-02,  3.9716e-01, -9.5848e-03,  1.0249e-01,  4.2180e-02,\n",
            "         1.5579e-01, -9.1774e-02, -3.7658e-02, -3.1185e-02,  2.9396e-02,\n",
            "        -1.5649e-01, -1.4707e-01,  5.7171e-02,  2.4213e-01,  1.5432e-01,\n",
            "         3.2308e-03,  1.1015e-01,  1.7271e-01,  7.9619e-02,  7.6624e-02,\n",
            "        -2.6840e-01, -1.1819e-02, -2.8910e-02,  3.7601e-03, -4.8756e-03,\n",
            "         1.8048e-01,  1.3608e-01, -1.5711e-01,  3.3081e-03,  8.2037e-02,\n",
            "        -1.0883e-01,  5.0018e-02, -1.7747e-01,  9.1692e-02, -1.8516e-01,\n",
            "         2.0489e-02, -8.9187e-02, -6.6283e-03, -2.4689e-02,  1.7986e-01,\n",
            "         8.5388e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-1.0258e-01,  9.4131e-02, -2.0600e-01,  2.0763e-01,  1.1106e-02,\n",
            "         1.5808e-01, -1.0047e-01, -1.2841e-01,  1.2442e-01, -1.2903e-01,\n",
            "        -1.2073e-01, -3.8196e-02, -2.8247e-02, -1.5487e-02, -1.7953e-01,\n",
            "        -1.4977e-01,  4.8387e-03, -1.0272e-01, -4.0294e-03, -1.4952e-01,\n",
            "        -1.1002e-01, -1.6555e-01, -5.8055e-02,  6.6843e-02,  3.8886e-01,\n",
            "         1.2719e-01,  1.7506e-01,  3.6207e-02, -3.1860e-02, -1.3146e-01,\n",
            "        -2.1170e-01,  2.5449e-01,  1.9106e-01,  1.7773e-01,  1.0170e-01,\n",
            "        -1.0286e-01, -6.9797e-02, -1.5851e-02, -8.8353e-03, -1.7589e-02,\n",
            "         1.1156e-01,  1.4455e-01,  1.0712e-01, -2.0596e-01, -2.6225e-01,\n",
            "        -7.2089e-02,  3.3089e-02,  4.1166e-02,  1.0257e-01,  1.7232e-01,\n",
            "         2.3017e-02, -1.9643e-02, -5.5104e-02, -2.3249e-02,  9.2633e-02,\n",
            "        -1.4451e-01,  7.0210e-02, -5.1271e-02, -1.6033e-03,  4.5768e-03,\n",
            "        -4.4537e-02, -1.0478e-01, -8.2380e-02, -1.3382e-01, -1.6062e-02,\n",
            "         2.1435e-01, -7.1988e-02, -9.7558e-02,  8.0644e-02,  2.1992e-01,\n",
            "        -3.0702e-02, -1.7826e-01,  3.3817e-02,  7.4431e-02,  4.3247e-02,\n",
            "        -1.6271e-01, -1.6082e-01, -5.6487e-02,  2.9613e-02, -2.9238e-01,\n",
            "         5.8518e-02, -4.6642e-02,  2.1637e-01,  1.2856e-01, -1.2087e-01,\n",
            "        -2.7351e-02,  4.3393e-02,  1.9233e-01, -1.8787e-01, -1.1399e-01,\n",
            "         4.1254e-01, -2.4431e-02,  1.4556e-01, -8.9112e-02,  1.4948e-01,\n",
            "         1.0685e-01, -7.3819e-02,  1.0337e-01, -1.3618e-01,  1.7210e-01,\n",
            "        -1.2949e-02,  1.8114e-02, -3.0354e-01,  6.4743e-02,  8.2768e-02,\n",
            "        -5.8902e-02,  1.0840e-01,  8.2511e-02, -3.5654e-02, -1.0586e-01,\n",
            "         2.3079e-02, -5.9153e-02,  2.6655e-01, -1.0443e-01,  7.3986e-02,\n",
            "         1.4858e-01, -1.9814e-01, -6.4377e-02,  3.7920e-02,  1.3428e-01,\n",
            "        -2.4971e-01, -1.3505e-01, -7.8080e-02, -9.2601e-03, -8.5932e-02,\n",
            "        -3.5824e-01,  1.9048e-01, -8.9904e-03, -8.5226e-02, -9.2921e-03,\n",
            "        -2.2349e-02,  3.0389e-02, -1.9144e-01,  3.8622e-01,  9.5920e-02,\n",
            "        -2.0634e-02,  1.7635e-01,  1.8859e-01,  1.1652e-01,  1.0182e-01,\n",
            "        -3.5805e-02,  1.7074e-01, -5.2204e-02, -1.3607e-01, -1.0606e-01,\n",
            "        -1.3175e-01, -6.1943e-03, -1.3730e-01, -2.3402e-01, -5.7972e-02,\n",
            "         4.1603e-02,  2.0046e-02, -1.4194e-01,  3.5245e-04,  1.0857e-01,\n",
            "        -1.2941e-01,  7.5972e-02,  9.3793e-02,  4.3817e-02,  2.4971e-01,\n",
            "        -1.6950e-01, -1.0565e-01, -1.1187e-02, -6.3562e-03,  3.0493e-01,\n",
            "        -7.4252e-02, -1.4566e-01, -2.8644e-02,  5.7506e-02, -9.1729e-03,\n",
            "         1.3179e-01,  2.7824e-01,  1.5157e-01, -1.4399e-01, -1.2910e-01,\n",
            "        -1.1293e-02, -5.1211e-02,  4.6543e-02,  2.7501e-02,  1.7630e-01,\n",
            "        -1.2654e-01,  1.8323e-01,  2.1264e-01, -7.4742e-02,  2.0894e-02,\n",
            "         1.7758e-01, -8.6731e-03, -2.4261e-01, -1.1766e-01,  1.7881e-02,\n",
            "        -1.9909e-02,  1.5834e-01, -4.0719e-02, -2.0799e-01,  1.1732e-01,\n",
            "         1.1061e-01,  1.8921e-02,  2.7577e-01,  5.3323e-02, -1.8915e-01,\n",
            "         9.6332e-02, -8.9438e-02,  2.3768e-02,  8.3528e-03, -1.0785e-03,\n",
            "         1.8509e-01,  2.3513e-01, -4.1937e-02, -2.3750e-02, -1.0966e-01,\n",
            "         7.2010e-02,  5.2217e-02,  2.3376e-01,  9.5348e-02, -1.4187e-01,\n",
            "         1.3407e-02,  3.9716e-01, -9.5848e-03,  1.0249e-01,  4.2180e-02,\n",
            "         1.5579e-01, -9.1774e-02, -3.7658e-02, -3.1185e-02,  2.9396e-02,\n",
            "        -1.5649e-01, -1.4707e-01,  5.7171e-02,  2.4213e-01,  1.5432e-01,\n",
            "         3.2308e-03,  1.1015e-01,  1.7271e-01,  7.9619e-02,  7.6624e-02,\n",
            "        -2.6840e-01, -1.1819e-02, -2.8910e-02,  3.7601e-03, -4.8756e-03,\n",
            "         1.8048e-01,  1.3608e-01, -1.5711e-01,  3.3081e-03,  8.2037e-02,\n",
            "        -1.0883e-01,  5.0018e-02, -1.7747e-01,  9.1692e-02, -1.8516e-01,\n",
            "         2.0489e-02, -8.9187e-02, -6.6283e-03, -2.4689e-02,  1.7986e-01,\n",
            "         8.5388e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 4  \u001b[0m\n",
            "Actual input word = jouer\n",
            "Embedding of input word 4 = tensor([-0.1890,  0.0767, -0.0571, -0.1689, -0.0133,  0.1249, -0.0586, -0.1425,\n",
            "         0.0370, -0.2696,  0.0427, -0.0906, -0.1534, -0.1398, -0.2551, -0.0389,\n",
            "         0.1893,  0.0147,  0.0448, -0.1701, -0.4447,  0.0587,  0.1217,  0.2730,\n",
            "         0.1676,  0.1864,  0.1264, -0.1244, -0.0864,  0.0733, -0.0388,  0.0771,\n",
            "         0.1116,  0.0606,  0.3328, -0.1755,  0.1207, -0.3485, -0.0733,  0.0382,\n",
            "        -0.0085, -0.1331,  0.2530, -0.1579, -0.0862,  0.0673,  0.1225,  0.0063,\n",
            "         0.2506, -0.0355,  0.0506, -0.0078,  0.0715, -0.0582,  0.1154,  0.0168,\n",
            "        -0.1628,  0.1940,  0.0138,  0.0987, -0.0299, -0.0447, -0.2175, -0.2782,\n",
            "         0.1784,  0.0286,  0.1422, -0.2712,  0.2971, -0.0994, -0.2100,  0.0228,\n",
            "        -0.1442,  0.0559,  0.0337,  0.1104, -0.1936,  0.0031, -0.2097,  0.0226,\n",
            "        -0.0519,  0.0357,  0.0043,  0.0523, -0.1540,  0.1386,  0.2014, -0.0033,\n",
            "         0.0898, -0.0394,  0.1476,  0.1764,  0.2101,  0.0348, -0.0199,  0.1255,\n",
            "        -0.2560,  0.0108, -0.0985, -0.0246,  0.1838, -0.2423,  0.0037, -0.0247,\n",
            "        -0.0051, -0.0344, -0.0680, -0.0891,  0.1451, -0.1583,  0.1097, -0.1421,\n",
            "        -0.0021,  0.1293,  0.0845,  0.1860, -0.1894,  0.0331,  0.0235,  0.0905,\n",
            "        -0.2055, -0.2763,  0.1322, -0.1556,  0.0274, -0.0491,  0.0449,  0.1118,\n",
            "        -0.0897,  0.0722, -0.1661,  0.1172, -0.2377,  0.1387,  0.1518,  0.0883,\n",
            "         0.2174,  0.1409,  0.1117,  0.2830,  0.0939,  0.2422,  0.0535, -0.1147,\n",
            "        -0.0274, -0.1895, -0.0326, -0.1578, -0.0584, -0.1633,  0.2136,  0.0506,\n",
            "        -0.1561, -0.2111,  0.1914,  0.0554,  0.0051,  0.2604,  0.1011,  0.1122,\n",
            "        -0.1344, -0.1343,  0.1672,  0.0991,  0.2027,  0.0435, -0.0700, -0.0516,\n",
            "         0.1294, -0.1102,  0.0678,  0.1106, -0.0018, -0.2187, -0.0398,  0.1825,\n",
            "         0.0691, -0.1007, -0.1258,  0.2057,  0.0740, -0.1481,  0.1737,  0.0824,\n",
            "         0.1822,  0.0511, -0.0860, -0.0305,  0.0447, -0.1166,  0.0162,  0.1254,\n",
            "        -0.0830, -0.1191,  0.2617,  0.0704,  0.1445, -0.2265,  0.0136, -0.0847,\n",
            "        -0.0150,  0.0137, -0.1772,  0.0025,  0.1093,  0.0134,  0.1015, -0.1752,\n",
            "        -0.1359, -0.2367, -0.0239,  0.1881, -0.0439, -0.0346,  0.0896,  0.0555,\n",
            "         0.0475,  0.1165,  0.0786,  0.0541, -0.0089,  0.0376, -0.1984,  0.1046,\n",
            "        -0.2646,  0.0687,  0.0629,  0.0395, -0.0591, -0.0651,  0.0816,  0.2978,\n",
            "         0.1692, -0.1778,  0.1122, -0.0618,  0.1647,  0.0764,  0.0361,  0.2243,\n",
            "         0.0320,  0.0846,  0.0304, -0.0795,  0.2545, -0.0231,  0.1321, -0.2376,\n",
            "         0.3283, -0.0618, -0.1472, -0.2420, -0.0126,  0.2207, -0.1190,  0.0294],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.1890,  0.0767, -0.0571, -0.1689, -0.0133,  0.1249, -0.0586, -0.1425,\n",
            "         0.0370, -0.2696,  0.0427, -0.0906, -0.1534, -0.1398, -0.2551, -0.0389,\n",
            "         0.1893,  0.0147,  0.0448, -0.1701, -0.4447,  0.0587,  0.1217,  0.2730,\n",
            "         0.1676,  0.1864,  0.1264, -0.1244, -0.0864,  0.0733, -0.0388,  0.0771,\n",
            "         0.1116,  0.0606,  0.3328, -0.1755,  0.1207, -0.3485, -0.0733,  0.0382,\n",
            "        -0.0085, -0.1331,  0.2530, -0.1579, -0.0862,  0.0673,  0.1225,  0.0063,\n",
            "         0.2506, -0.0355,  0.0506, -0.0078,  0.0715, -0.0582,  0.1154,  0.0168,\n",
            "        -0.1628,  0.1940,  0.0138,  0.0987, -0.0299, -0.0447, -0.2175, -0.2782,\n",
            "         0.1784,  0.0286,  0.1422, -0.2712,  0.2971, -0.0994, -0.2100,  0.0228,\n",
            "        -0.1442,  0.0559,  0.0337,  0.1104, -0.1936,  0.0031, -0.2097,  0.0226,\n",
            "        -0.0519,  0.0357,  0.0043,  0.0523, -0.1540,  0.1386,  0.2014, -0.0033,\n",
            "         0.0898, -0.0394,  0.1476,  0.1764,  0.2101,  0.0348, -0.0199,  0.1255,\n",
            "        -0.2560,  0.0108, -0.0985, -0.0246,  0.1838, -0.2423,  0.0037, -0.0247,\n",
            "        -0.0051, -0.0344, -0.0680, -0.0891,  0.1451, -0.1583,  0.1097, -0.1421,\n",
            "        -0.0021,  0.1293,  0.0845,  0.1860, -0.1894,  0.0331,  0.0235,  0.0905,\n",
            "        -0.2055, -0.2763,  0.1322, -0.1556,  0.0274, -0.0491,  0.0449,  0.1118,\n",
            "        -0.0897,  0.0722, -0.1661,  0.1172, -0.2377,  0.1387,  0.1518,  0.0883,\n",
            "         0.2174,  0.1409,  0.1117,  0.2830,  0.0939,  0.2422,  0.0535, -0.1147,\n",
            "        -0.0274, -0.1895, -0.0326, -0.1578, -0.0584, -0.1633,  0.2136,  0.0506,\n",
            "        -0.1561, -0.2111,  0.1914,  0.0554,  0.0051,  0.2604,  0.1011,  0.1122,\n",
            "        -0.1344, -0.1343,  0.1672,  0.0991,  0.2027,  0.0435, -0.0700, -0.0516,\n",
            "         0.1294, -0.1102,  0.0678,  0.1106, -0.0018, -0.2187, -0.0398,  0.1825,\n",
            "         0.0691, -0.1007, -0.1258,  0.2057,  0.0740, -0.1481,  0.1737,  0.0824,\n",
            "         0.1822,  0.0511, -0.0860, -0.0305,  0.0447, -0.1166,  0.0162,  0.1254,\n",
            "        -0.0830, -0.1191,  0.2617,  0.0704,  0.1445, -0.2265,  0.0136, -0.0847,\n",
            "        -0.0150,  0.0137, -0.1772,  0.0025,  0.1093,  0.0134,  0.1015, -0.1752,\n",
            "        -0.1359, -0.2367, -0.0239,  0.1881, -0.0439, -0.0346,  0.0896,  0.0555,\n",
            "         0.0475,  0.1165,  0.0786,  0.0541, -0.0089,  0.0376, -0.1984,  0.1046,\n",
            "        -0.2646,  0.0687,  0.0629,  0.0395, -0.0591, -0.0651,  0.0816,  0.2978,\n",
            "         0.1692, -0.1778,  0.1122, -0.0618,  0.1647,  0.0764,  0.0361,  0.2243,\n",
            "         0.0320,  0.0846,  0.0304, -0.0795,  0.2545, -0.0231,  0.1321, -0.2376,\n",
            "         0.3283, -0.0618, -0.1472, -0.2420, -0.0126,  0.2207, -0.1190,  0.0294],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 5  \u001b[0m\n",
            "Actual input word = a\n",
            "Embedding of input word 5 = tensor([-0.1341, -0.0442, -0.1788, -0.2252,  0.0707,  0.0577,  0.0985,  0.0079,\n",
            "        -0.0844, -0.2286,  0.0428, -0.0182, -0.1107, -0.0474, -0.2231, -0.1205,\n",
            "         0.0536,  0.2183,  0.2993, -0.0303,  0.0194, -0.1616, -0.1068,  0.2371,\n",
            "         0.0499, -0.1537,  0.0402, -0.0050,  0.0534, -0.0533, -0.0379, -0.0561,\n",
            "        -0.0415, -0.1075,  0.0741, -0.1019,  0.0957, -0.2642, -0.1608, -0.0781,\n",
            "         0.2336, -0.2844,  0.1571, -0.0298, -0.1401, -0.0025, -0.0277,  0.1579,\n",
            "         0.2184,  0.0625,  0.0145,  0.0213,  0.1131, -0.0217,  0.1556, -0.0159,\n",
            "        -0.0884,  0.2642, -0.0849,  0.1976,  0.0485,  0.0029, -0.2026, -0.1116,\n",
            "        -0.0673,  0.0040,  0.1824, -0.0877,  0.2923,  0.0228, -0.1885,  0.0300,\n",
            "         0.0957,  0.0303, -0.0215, -0.1896, -0.3512, -0.1578,  0.0042, -0.0418,\n",
            "        -0.2178,  0.1240, -0.2747,  0.0788, -0.0881,  0.1220,  0.1424,  0.3371,\n",
            "         0.0366, -0.0164,  0.2824,  0.2101, -0.0898, -0.1166, -0.0753,  0.2588,\n",
            "         0.0109, -0.0204, -0.0856, -0.0584,  0.1637, -0.1669,  0.2361,  0.0295,\n",
            "         0.0904, -0.1238,  0.0277, -0.0687,  0.2732, -0.1101,  0.1199, -0.1953,\n",
            "        -0.0555,  0.0934,  0.2178,  0.1600, -0.0450,  0.1402, -0.0625, -0.0382,\n",
            "        -0.0382, -0.0927,  0.1839,  0.0647,  0.1458,  0.0839, -0.1270,  0.2475,\n",
            "         0.0316,  0.0671,  0.1322,  0.1325, -0.1469, -0.0458,  0.0916, -0.0894,\n",
            "         0.1091,  0.1021, -0.1087, -0.0558,  0.1271,  0.1721,  0.1739,  0.0361,\n",
            "         0.1753, -0.0193, -0.1826, -0.2337, -0.0881, -0.2191,  0.1375, -0.0511,\n",
            "        -0.1549, -0.1435,  0.2215, -0.1778, -0.0302,  0.1649,  0.0810,  0.1369,\n",
            "        -0.0616, -0.1098,  0.1305,  0.0072,  0.1837,  0.3402, -0.1166, -0.1198,\n",
            "         0.2952, -0.2723,  0.0476,  0.1837,  0.2223, -0.1628, -0.0490,  0.0170,\n",
            "         0.1771,  0.0810, -0.1522,  0.3867,  0.1435, -0.0833,  0.2275, -0.1814,\n",
            "         0.1362,  0.0706,  0.0389, -0.0023,  0.1485, -0.1872,  0.0323,  0.2203,\n",
            "        -0.0646, -0.2486, -0.1040,  0.1359,  0.1013, -0.0621,  0.0590, -0.1398,\n",
            "        -0.2438,  0.0081, -0.3088, -0.2180,  0.1126, -0.0113, -0.1268, -0.2554,\n",
            "         0.1589, -0.0849, -0.1447,  0.1706, -0.0401,  0.0590,  0.0961, -0.0413,\n",
            "         0.0866,  0.0550,  0.1515,  0.0766, -0.0188,  0.1012, -0.0700,  0.1798,\n",
            "         0.0666, -0.1516,  0.0345, -0.1035,  0.0964, -0.2305,  0.1433,  0.0717,\n",
            "        -0.0591, -0.0716,  0.1404,  0.1939,  0.0155, -0.0198, -0.0408,  0.0523,\n",
            "        -0.1020,  0.0353, -0.0195,  0.1258,  0.2160, -0.1654,  0.0962, -0.1672,\n",
            "         0.2059, -0.0488, -0.2791, -0.1864, -0.0391,  0.1295, -0.0702,  0.1668],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.1341, -0.0442, -0.1788, -0.2252,  0.0707,  0.0577,  0.0985,  0.0079,\n",
            "        -0.0844, -0.2286,  0.0428, -0.0182, -0.1107, -0.0474, -0.2231, -0.1205,\n",
            "         0.0536,  0.2183,  0.2993, -0.0303,  0.0194, -0.1616, -0.1068,  0.2371,\n",
            "         0.0499, -0.1537,  0.0402, -0.0050,  0.0534, -0.0533, -0.0379, -0.0561,\n",
            "        -0.0415, -0.1075,  0.0741, -0.1019,  0.0957, -0.2642, -0.1608, -0.0781,\n",
            "         0.2336, -0.2844,  0.1571, -0.0298, -0.1401, -0.0025, -0.0277,  0.1579,\n",
            "         0.2184,  0.0625,  0.0145,  0.0213,  0.1131, -0.0217,  0.1556, -0.0159,\n",
            "        -0.0884,  0.2642, -0.0849,  0.1976,  0.0485,  0.0029, -0.2026, -0.1116,\n",
            "        -0.0673,  0.0040,  0.1824, -0.0877,  0.2923,  0.0228, -0.1885,  0.0300,\n",
            "         0.0957,  0.0303, -0.0215, -0.1896, -0.3512, -0.1578,  0.0042, -0.0418,\n",
            "        -0.2178,  0.1240, -0.2747,  0.0788, -0.0881,  0.1220,  0.1424,  0.3371,\n",
            "         0.0366, -0.0164,  0.2824,  0.2101, -0.0898, -0.1166, -0.0753,  0.2588,\n",
            "         0.0109, -0.0204, -0.0856, -0.0584,  0.1637, -0.1669,  0.2361,  0.0295,\n",
            "         0.0904, -0.1238,  0.0277, -0.0687,  0.2732, -0.1101,  0.1199, -0.1953,\n",
            "        -0.0555,  0.0934,  0.2178,  0.1600, -0.0450,  0.1402, -0.0625, -0.0382,\n",
            "        -0.0382, -0.0927,  0.1839,  0.0647,  0.1458,  0.0839, -0.1270,  0.2475,\n",
            "         0.0316,  0.0671,  0.1322,  0.1325, -0.1469, -0.0458,  0.0916, -0.0894,\n",
            "         0.1091,  0.1021, -0.1087, -0.0558,  0.1271,  0.1721,  0.1739,  0.0361,\n",
            "         0.1753, -0.0193, -0.1826, -0.2337, -0.0881, -0.2191,  0.1375, -0.0511,\n",
            "        -0.1549, -0.1435,  0.2215, -0.1778, -0.0302,  0.1649,  0.0810,  0.1369,\n",
            "        -0.0616, -0.1098,  0.1305,  0.0072,  0.1837,  0.3402, -0.1166, -0.1198,\n",
            "         0.2952, -0.2723,  0.0476,  0.1837,  0.2223, -0.1628, -0.0490,  0.0170,\n",
            "         0.1771,  0.0810, -0.1522,  0.3867,  0.1435, -0.0833,  0.2275, -0.1814,\n",
            "         0.1362,  0.0706,  0.0389, -0.0023,  0.1485, -0.1872,  0.0323,  0.2203,\n",
            "        -0.0646, -0.2486, -0.1040,  0.1359,  0.1013, -0.0621,  0.0590, -0.1398,\n",
            "        -0.2438,  0.0081, -0.3088, -0.2180,  0.1126, -0.0113, -0.1268, -0.2554,\n",
            "         0.1589, -0.0849, -0.1447,  0.1706, -0.0401,  0.0590,  0.0961, -0.0413,\n",
            "         0.0866,  0.0550,  0.1515,  0.0766, -0.0188,  0.1012, -0.0700,  0.1798,\n",
            "         0.0666, -0.1516,  0.0345, -0.1035,  0.0964, -0.2305,  0.1433,  0.0717,\n",
            "        -0.0591, -0.0716,  0.1404,  0.1939,  0.0155, -0.0198, -0.0408,  0.0523,\n",
            "        -0.1020,  0.0353, -0.0195,  0.1258,  0.2160, -0.1654,  0.0962, -0.1672,\n",
            "         0.2059, -0.0488, -0.2791, -0.1864, -0.0391,  0.1295, -0.0702,  0.1668],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 6  \u001b[0m\n",
            "Actual input word = ce\n",
            "Embedding of input word 6 = tensor([ 8.4425e-02, -4.3656e-03, -2.4147e-01, -1.9407e-01, -1.2733e-01,\n",
            "        -9.6941e-02, -1.2063e-01, -2.4021e-01, -9.5616e-02, -3.9377e-02,\n",
            "        -8.2283e-02,  7.0107e-02, -7.4009e-02, -9.5043e-02, -9.2948e-02,\n",
            "        -8.5780e-02,  1.3783e-01,  8.5168e-02,  1.4060e-01, -2.5677e-02,\n",
            "         5.6572e-02, -8.0172e-02, -2.3774e-02, -5.3433e-02,  5.7423e-02,\n",
            "         6.4292e-03, -1.8378e-02,  2.1225e-01,  2.5938e-02, -8.7137e-02,\n",
            "         1.1563e-01,  5.7838e-02, -5.0513e-02, -8.0658e-02,  7.5758e-02,\n",
            "        -5.3279e-02, -4.3235e-02, -1.3323e-01,  9.2932e-02, -2.1549e-01,\n",
            "         2.5175e-01,  7.8872e-02, -1.6165e-02, -3.5468e-02, -4.0755e-02,\n",
            "        -1.6862e-02,  7.3727e-02,  8.3655e-02,  1.8698e-01,  5.4511e-02,\n",
            "         2.4574e-02,  1.1134e-01, -3.0804e-02, -2.0740e-02,  3.6199e-01,\n",
            "        -1.5595e-01, -3.2686e-01,  7.8429e-02, -1.6802e-01,  1.4033e-01,\n",
            "        -2.6670e-01,  6.8985e-02, -5.3706e-02,  6.3780e-02,  2.1999e-02,\n",
            "         8.7501e-02,  3.6913e-02, -1.7655e-01, -4.6007e-02,  1.0191e-01,\n",
            "         8.5409e-02, -3.8302e-05,  1.6513e-01,  2.7097e-01, -8.5480e-02,\n",
            "        -2.3630e-02,  6.0287e-02, -2.7367e-01,  1.5889e-01,  3.0295e-02,\n",
            "        -2.6895e-01,  5.0124e-02, -8.6040e-02,  1.3080e-01,  1.4083e-02,\n",
            "        -1.1088e-02,  2.6825e-02, -4.9277e-02,  4.8195e-02, -5.1594e-02,\n",
            "         2.5316e-02,  1.1271e-01,  4.8945e-02, -2.0659e-02, -1.7872e-01,\n",
            "         7.9942e-02, -7.9290e-04,  9.5146e-02,  2.2488e-01,  3.8042e-03,\n",
            "         1.2306e-01, -1.4364e-01,  2.5692e-01, -7.5899e-02, -4.8084e-02,\n",
            "        -3.3513e-02, -1.9764e-03, -4.6818e-02, -7.5711e-02, -3.2213e-01,\n",
            "         1.1466e-01,  7.7474e-02,  2.6270e-02,  4.6634e-02, -1.1131e-01,\n",
            "         2.4611e-01, -9.8454e-02,  2.2531e-01,  8.5385e-02, -1.9377e-02,\n",
            "        -8.2453e-02, -6.7527e-02,  2.2921e-01, -6.9065e-02, -2.0020e-02,\n",
            "         2.0706e-01, -8.2253e-03,  1.0298e-01, -7.5891e-02,  9.8691e-02,\n",
            "         4.6865e-02, -4.4734e-02, -7.1137e-02, -1.0619e-01,  1.4784e-01,\n",
            "         1.2995e-01,  2.4660e-01, -1.0373e-01,  1.1440e-01,  7.5446e-02,\n",
            "         9.1554e-03, -1.5986e-02,  1.5493e-01,  6.4412e-02,  8.4910e-02,\n",
            "        -1.3902e-01,  1.1332e-01, -1.8354e-01,  8.0640e-02, -1.1211e-01,\n",
            "         4.7903e-02, -2.3223e-01, -1.8734e-01, -6.0231e-02,  2.7544e-01,\n",
            "        -7.3152e-02,  2.9275e-02, -1.2820e-01, -1.3665e-01,  1.3789e-01,\n",
            "        -8.3321e-02,  1.7317e-02,  1.2282e-01,  1.0223e-01,  1.6021e-01,\n",
            "         9.5380e-03, -1.2674e-01,  3.5502e-02, -1.0132e-03, -2.8237e-01,\n",
            "         3.2478e-01,  1.3909e-01, -1.4137e-01, -2.6801e-01, -2.6901e-02,\n",
            "         1.4383e-01, -4.0931e-03, -2.0791e-01, -5.3386e-02,  2.3972e-01,\n",
            "         6.8793e-02, -8.6721e-02,  1.9577e-01,  1.3275e-01, -6.9039e-03,\n",
            "        -1.3056e-01, -1.1581e-01,  5.4468e-02, -4.2679e-03, -2.3606e-02,\n",
            "         1.6399e-01, -1.2853e-01, -8.3000e-02, -3.8937e-01,  9.0380e-03,\n",
            "         1.2714e-02, -1.9670e-02, -3.9802e-02,  9.3378e-02,  1.8399e-01,\n",
            "        -1.8816e-02,  3.0296e-02,  2.3965e-02,  4.0248e-02,  8.4672e-03,\n",
            "        -7.7084e-02, -4.2925e-02,  1.9165e-01,  1.3214e-01, -7.0830e-02,\n",
            "        -2.5967e-01, -1.6242e-02, -1.4311e-01, -2.4269e-01, -9.0505e-02,\n",
            "        -5.4609e-03, -7.9247e-04, -1.1383e-01,  2.3431e-01, -5.3487e-02,\n",
            "         4.4143e-02,  1.5735e-01,  1.3381e-01,  7.1133e-02,  2.1740e-02,\n",
            "        -2.9541e-01,  3.0230e-01, -4.9639e-02,  1.9630e-01,  5.9079e-03,\n",
            "         6.1937e-03,  6.5078e-02, -7.3955e-03, -1.2416e-01,  8.3949e-02,\n",
            "         1.2828e-01, -8.3406e-02, -5.2084e-02, -8.6737e-02,  1.4343e-02,\n",
            "        -8.1801e-02, -5.7775e-02,  1.4403e-01,  6.7147e-03,  2.4668e-02,\n",
            "         2.1722e-02,  4.8154e-02, -1.1729e-01, -6.6030e-02,  4.3113e-02,\n",
            "        -2.0607e-01, -1.1765e-01, -1.9489e-01,  2.8721e-01,  1.1712e-02,\n",
            "         3.1678e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([ 8.4425e-02, -4.3656e-03, -2.4147e-01, -1.9407e-01, -1.2733e-01,\n",
            "        -9.6941e-02, -1.2063e-01, -2.4021e-01, -9.5616e-02, -3.9377e-02,\n",
            "        -8.2283e-02,  7.0107e-02, -7.4009e-02, -9.5043e-02, -9.2948e-02,\n",
            "        -8.5780e-02,  1.3783e-01,  8.5168e-02,  1.4060e-01, -2.5677e-02,\n",
            "         5.6572e-02, -8.0172e-02, -2.3774e-02, -5.3433e-02,  5.7423e-02,\n",
            "         6.4292e-03, -1.8378e-02,  2.1225e-01,  2.5938e-02, -8.7137e-02,\n",
            "         1.1563e-01,  5.7838e-02, -5.0513e-02, -8.0658e-02,  7.5758e-02,\n",
            "        -5.3279e-02, -4.3235e-02, -1.3323e-01,  9.2932e-02, -2.1549e-01,\n",
            "         2.5175e-01,  7.8872e-02, -1.6165e-02, -3.5468e-02, -4.0755e-02,\n",
            "        -1.6862e-02,  7.3727e-02,  8.3655e-02,  1.8698e-01,  5.4511e-02,\n",
            "         2.4574e-02,  1.1134e-01, -3.0804e-02, -2.0740e-02,  3.6199e-01,\n",
            "        -1.5595e-01, -3.2686e-01,  7.8429e-02, -1.6802e-01,  1.4033e-01,\n",
            "        -2.6670e-01,  6.8985e-02, -5.3706e-02,  6.3780e-02,  2.1999e-02,\n",
            "         8.7501e-02,  3.6913e-02, -1.7655e-01, -4.6007e-02,  1.0191e-01,\n",
            "         8.5409e-02, -3.8302e-05,  1.6513e-01,  2.7097e-01, -8.5480e-02,\n",
            "        -2.3630e-02,  6.0287e-02, -2.7367e-01,  1.5889e-01,  3.0295e-02,\n",
            "        -2.6895e-01,  5.0124e-02, -8.6040e-02,  1.3080e-01,  1.4083e-02,\n",
            "        -1.1088e-02,  2.6825e-02, -4.9277e-02,  4.8195e-02, -5.1594e-02,\n",
            "         2.5316e-02,  1.1271e-01,  4.8945e-02, -2.0659e-02, -1.7872e-01,\n",
            "         7.9942e-02, -7.9290e-04,  9.5146e-02,  2.2488e-01,  3.8042e-03,\n",
            "         1.2306e-01, -1.4364e-01,  2.5692e-01, -7.5899e-02, -4.8084e-02,\n",
            "        -3.3513e-02, -1.9764e-03, -4.6818e-02, -7.5711e-02, -3.2213e-01,\n",
            "         1.1466e-01,  7.7474e-02,  2.6270e-02,  4.6634e-02, -1.1131e-01,\n",
            "         2.4611e-01, -9.8454e-02,  2.2531e-01,  8.5385e-02, -1.9377e-02,\n",
            "        -8.2453e-02, -6.7527e-02,  2.2921e-01, -6.9065e-02, -2.0020e-02,\n",
            "         2.0706e-01, -8.2253e-03,  1.0298e-01, -7.5891e-02,  9.8691e-02,\n",
            "         4.6865e-02, -4.4734e-02, -7.1137e-02, -1.0619e-01,  1.4784e-01,\n",
            "         1.2995e-01,  2.4660e-01, -1.0373e-01,  1.1440e-01,  7.5446e-02,\n",
            "         9.1554e-03, -1.5986e-02,  1.5493e-01,  6.4412e-02,  8.4910e-02,\n",
            "        -1.3902e-01,  1.1332e-01, -1.8354e-01,  8.0640e-02, -1.1211e-01,\n",
            "         4.7903e-02, -2.3223e-01, -1.8734e-01, -6.0231e-02,  2.7544e-01,\n",
            "        -7.3152e-02,  2.9275e-02, -1.2820e-01, -1.3665e-01,  1.3789e-01,\n",
            "        -8.3321e-02,  1.7317e-02,  1.2282e-01,  1.0223e-01,  1.6021e-01,\n",
            "         9.5380e-03, -1.2674e-01,  3.5502e-02, -1.0132e-03, -2.8237e-01,\n",
            "         3.2478e-01,  1.3909e-01, -1.4137e-01, -2.6801e-01, -2.6901e-02,\n",
            "         1.4383e-01, -4.0931e-03, -2.0791e-01, -5.3386e-02,  2.3972e-01,\n",
            "         6.8793e-02, -8.6721e-02,  1.9577e-01,  1.3275e-01, -6.9039e-03,\n",
            "        -1.3056e-01, -1.1581e-01,  5.4468e-02, -4.2679e-03, -2.3606e-02,\n",
            "         1.6399e-01, -1.2853e-01, -8.3000e-02, -3.8937e-01,  9.0380e-03,\n",
            "         1.2714e-02, -1.9670e-02, -3.9802e-02,  9.3378e-02,  1.8399e-01,\n",
            "        -1.8816e-02,  3.0296e-02,  2.3965e-02,  4.0248e-02,  8.4672e-03,\n",
            "        -7.7084e-02, -4.2925e-02,  1.9165e-01,  1.3214e-01, -7.0830e-02,\n",
            "        -2.5967e-01, -1.6242e-02, -1.4311e-01, -2.4269e-01, -9.0505e-02,\n",
            "        -5.4609e-03, -7.9247e-04, -1.1383e-01,  2.3431e-01, -5.3487e-02,\n",
            "         4.4143e-02,  1.5735e-01,  1.3381e-01,  7.1133e-02,  2.1740e-02,\n",
            "        -2.9541e-01,  3.0230e-01, -4.9639e-02,  1.9630e-01,  5.9079e-03,\n",
            "         6.1937e-03,  6.5078e-02, -7.3955e-03, -1.2416e-01,  8.3949e-02,\n",
            "         1.2828e-01, -8.3406e-02, -5.2084e-02, -8.6737e-02,  1.4343e-02,\n",
            "        -8.1801e-02, -5.7775e-02,  1.4403e-01,  6.7147e-03,  2.4668e-02,\n",
            "         2.1722e-02,  4.8154e-02, -1.1729e-01, -6.6030e-02,  4.3113e-02,\n",
            "        -2.0607e-01, -1.1765e-01, -1.9489e-01,  2.8721e-01,  1.1712e-02,\n",
            "         3.1678e-02], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 7  \u001b[0m\n",
            "Actual input word = jeu\n",
            "Embedding of input word 7 = tensor([-0.0757, -0.0461, -0.1687, -0.0586,  0.0729, -0.0181,  0.0179, -0.0990,\n",
            "        -0.1556,  0.1070,  0.0588,  0.0193, -0.0538,  0.1635, -0.0246,  0.0326,\n",
            "         0.1493, -0.1671,  0.2539, -0.0170,  0.0677,  0.0941, -0.0742, -0.1234,\n",
            "         0.0327, -0.1231, -0.1491,  0.2085,  0.0639, -0.1541,  0.1165,  0.1772,\n",
            "        -0.0976, -0.0548, -0.1134, -0.1418,  0.1146, -0.0833,  0.1583, -0.3357,\n",
            "         0.2319,  0.1263, -0.0840, -0.0652,  0.0459, -0.0721, -0.0897, -0.0759,\n",
            "         0.1512, -0.1557,  0.0045, -0.1322, -0.1062, -0.1739,  0.0307, -0.0291,\n",
            "        -0.0648,  0.0156, -0.0522,  0.0403,  0.0122,  0.1179, -0.0607, -0.0043,\n",
            "        -0.0674,  0.1294,  0.1381, -0.1996,  0.2214, -0.0558,  0.1962,  0.0253,\n",
            "         0.0303,  0.1061, -0.1256, -0.1581, -0.0966,  0.1111, -0.0830, -0.1395,\n",
            "        -0.1045, -0.1474,  0.0285,  0.0307,  0.1865, -0.1548,  0.1676, -0.1670,\n",
            "         0.0696, -0.1712,  0.0699, -0.1642, -0.1471, -0.1187, -0.0088,  0.1252,\n",
            "        -0.0230,  0.3069,  0.0371,  0.0034,  0.1652,  0.0300,  0.0919, -0.0802,\n",
            "        -0.3297,  0.1450, -0.1467,  0.0059, -0.0425, -0.0282,  0.2274,  0.0328,\n",
            "         0.1422,  0.0736, -0.1265,  0.1485, -0.0367,  0.1248,  0.0955, -0.1423,\n",
            "        -0.0348, -0.0764,  0.0354, -0.0599, -0.0225,  0.2263, -0.1790,  0.1238,\n",
            "         0.0445,  0.0407,  0.0913,  0.0039, -0.1645, -0.3197,  0.1047,  0.0585,\n",
            "         0.1367, -0.2016, -0.0649, -0.2436, -0.1876,  0.0793, -0.0063,  0.0015,\n",
            "         0.0010,  0.0377, -0.0036, -0.1419,  0.0114,  0.0693, -0.0842, -0.0844,\n",
            "        -0.0389,  0.1223,  0.0430, -0.0275,  0.0695,  0.0278, -0.2180, -0.1083,\n",
            "         0.0121, -0.0546,  0.1560,  0.0238, -0.0056,  0.1847, -0.2845,  0.0396,\n",
            "        -0.0306, -0.1216,  0.0152,  0.1427,  0.0105, -0.2120,  0.2039, -0.0525,\n",
            "         0.1450, -0.1217, -0.1208,  0.3408,  0.0694,  0.0136, -0.0243, -0.0083,\n",
            "         0.0628, -0.1273, -0.0026, -0.1190,  0.0253,  0.0353,  0.1644, -0.2273,\n",
            "        -0.1229, -0.2719, -0.1344, -0.1774, -0.0764, -0.0400, -0.0204, -0.0291,\n",
            "        -0.0356,  0.1049,  0.0251,  0.0529,  0.1354,  0.0359,  0.1057, -0.0022,\n",
            "        -0.0230,  0.0920,  0.0336, -0.0168,  0.0066, -0.1199,  0.0027,  0.3210,\n",
            "        -0.0260,  0.1782,  0.3500, -0.0426,  0.1322,  0.0118, -0.0115,  0.0602,\n",
            "         0.0715, -0.1935,  0.3417, -0.0983,  0.0824, -0.1863,  0.1696, -0.0775,\n",
            "        -0.2080, -0.1239,  0.0929, -0.0534, -0.1100,  0.0374, -0.0750,  0.0234,\n",
            "         0.0917,  0.0371, -0.0424, -0.0124,  0.0488, -0.0168,  0.3563,  0.0077,\n",
            "        -0.0546,  0.0590,  0.0036, -0.2226, -0.0357, -0.0710, -0.1739,  0.1837],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.0757, -0.0461, -0.1687, -0.0586,  0.0729, -0.0181,  0.0179, -0.0990,\n",
            "        -0.1556,  0.1070,  0.0588,  0.0193, -0.0538,  0.1635, -0.0246,  0.0326,\n",
            "         0.1493, -0.1671,  0.2539, -0.0170,  0.0677,  0.0941, -0.0742, -0.1234,\n",
            "         0.0327, -0.1231, -0.1491,  0.2085,  0.0639, -0.1541,  0.1165,  0.1772,\n",
            "        -0.0976, -0.0548, -0.1134, -0.1418,  0.1146, -0.0833,  0.1583, -0.3357,\n",
            "         0.2319,  0.1263, -0.0840, -0.0652,  0.0459, -0.0721, -0.0897, -0.0759,\n",
            "         0.1512, -0.1557,  0.0045, -0.1322, -0.1062, -0.1739,  0.0307, -0.0291,\n",
            "        -0.0648,  0.0156, -0.0522,  0.0403,  0.0122,  0.1179, -0.0607, -0.0043,\n",
            "        -0.0674,  0.1294,  0.1381, -0.1996,  0.2214, -0.0558,  0.1962,  0.0253,\n",
            "         0.0303,  0.1061, -0.1256, -0.1581, -0.0966,  0.1111, -0.0830, -0.1395,\n",
            "        -0.1045, -0.1474,  0.0285,  0.0307,  0.1865, -0.1548,  0.1676, -0.1670,\n",
            "         0.0696, -0.1712,  0.0699, -0.1642, -0.1471, -0.1187, -0.0088,  0.1252,\n",
            "        -0.0230,  0.3069,  0.0371,  0.0034,  0.1652,  0.0300,  0.0919, -0.0802,\n",
            "        -0.3297,  0.1450, -0.1467,  0.0059, -0.0425, -0.0282,  0.2274,  0.0328,\n",
            "         0.1422,  0.0736, -0.1265,  0.1485, -0.0367,  0.1248,  0.0955, -0.1423,\n",
            "        -0.0348, -0.0764,  0.0354, -0.0599, -0.0225,  0.2263, -0.1790,  0.1238,\n",
            "         0.0445,  0.0407,  0.0913,  0.0039, -0.1645, -0.3197,  0.1047,  0.0585,\n",
            "         0.1367, -0.2016, -0.0649, -0.2436, -0.1876,  0.0793, -0.0063,  0.0015,\n",
            "         0.0010,  0.0377, -0.0036, -0.1419,  0.0114,  0.0693, -0.0842, -0.0844,\n",
            "        -0.0389,  0.1223,  0.0430, -0.0275,  0.0695,  0.0278, -0.2180, -0.1083,\n",
            "         0.0121, -0.0546,  0.1560,  0.0238, -0.0056,  0.1847, -0.2845,  0.0396,\n",
            "        -0.0306, -0.1216,  0.0152,  0.1427,  0.0105, -0.2120,  0.2039, -0.0525,\n",
            "         0.1450, -0.1217, -0.1208,  0.3408,  0.0694,  0.0136, -0.0243, -0.0083,\n",
            "         0.0628, -0.1273, -0.0026, -0.1190,  0.0253,  0.0353,  0.1644, -0.2273,\n",
            "        -0.1229, -0.2719, -0.1344, -0.1774, -0.0764, -0.0400, -0.0204, -0.0291,\n",
            "        -0.0356,  0.1049,  0.0251,  0.0529,  0.1354,  0.0359,  0.1057, -0.0022,\n",
            "        -0.0230,  0.0920,  0.0336, -0.0168,  0.0066, -0.1199,  0.0027,  0.3210,\n",
            "        -0.0260,  0.1782,  0.3500, -0.0426,  0.1322,  0.0118, -0.0115,  0.0602,\n",
            "         0.0715, -0.1935,  0.3417, -0.0983,  0.0824, -0.1863,  0.1696, -0.0775,\n",
            "        -0.2080, -0.1239,  0.0929, -0.0534, -0.1100,  0.0374, -0.0750,  0.0234,\n",
            "         0.0917,  0.0371, -0.0424, -0.0124,  0.0488, -0.0168,  0.3563,  0.0077,\n",
            "        -0.0546,  0.0590,  0.0036, -0.2226, -0.0357, -0.0710, -0.1739,  0.1837],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 8  \u001b[0m\n",
            "Actual input word = .\n",
            "Embedding of input word 8 = tensor([-0.0579,  0.1689, -0.1333,  0.0440, -0.0266, -0.0011,  0.0603, -0.0740,\n",
            "         0.0107, -0.3106,  0.0266,  0.0528,  0.0219,  0.1869, -0.0935,  0.0216,\n",
            "        -0.0048, -0.1381,  0.3545, -0.0887, -0.0787, -0.2029, -0.0675,  0.0742,\n",
            "        -0.1256,  0.1132, -0.2626,  0.0436, -0.0034, -0.0219,  0.2895,  0.1864,\n",
            "        -0.0420, -0.0311,  0.0965, -0.1860,  0.2117, -0.2148,  0.2001, -0.0826,\n",
            "        -0.0010,  0.0125, -0.1094, -0.0818,  0.0277,  0.1379, -0.1395,  0.1128,\n",
            "         0.2386, -0.0720, -0.0524, -0.0070,  0.1745, -0.0836,  0.0213, -0.0462,\n",
            "        -0.0090,  0.0520, -0.0125,  0.0251, -0.1102, -0.1818,  0.1115,  0.1220,\n",
            "         0.0385, -0.2967,  0.0064, -0.0019,  0.0311, -0.1098,  0.0626,  0.0873,\n",
            "         0.0772, -0.0516, -0.0661,  0.0598, -0.0666,  0.0548, -0.1628,  0.0679,\n",
            "         0.0162, -0.0785,  0.3284, -0.0226,  0.0192, -0.2614,  0.2953, -0.0142,\n",
            "        -0.2045, -0.1593,  0.1234, -0.1492, -0.0574, -0.1531,  0.1241, -0.0977,\n",
            "         0.0582,  0.3059,  0.1938,  0.1886,  0.1182,  0.0229,  0.2251, -0.0533,\n",
            "        -0.1689, -0.0126, -0.1685, -0.0016,  0.0894, -0.1330,  0.2870,  0.1770,\n",
            "        -0.0113, -0.1065,  0.0299,  0.0150, -0.0489,  0.0450, -0.1258, -0.1145,\n",
            "        -0.1105, -0.4288,  0.0914, -0.2579,  0.0255, -0.1074, -0.2034, -0.0838,\n",
            "        -0.0897,  0.3271,  0.0310,  0.0956, -0.1119, -0.1659, -0.0471, -0.0258,\n",
            "         0.0349, -0.0123, -0.0751, -0.0561, -0.1474,  0.1604, -0.0602, -0.0697,\n",
            "         0.1083, -0.1558,  0.0259, -0.0394,  0.1088, -0.1219, -0.0144,  0.0227,\n",
            "         0.0780, -0.0306,  0.0560,  0.0747,  0.1582,  0.1932,  0.0648,  0.1877,\n",
            "         0.0034,  0.0392,  0.1959,  0.0976, -0.1179,  0.0668, -0.0896, -0.0428,\n",
            "        -0.0029, -0.0543, -0.2190,  0.0739, -0.1833, -0.2505,  0.0335, -0.1487,\n",
            "         0.1757,  0.0956, -0.0609,  0.0954, -0.0374, -0.1707,  0.0150, -0.1127,\n",
            "         0.1292, -0.1301,  0.0264, -0.0006, -0.0708, -0.2055, -0.0074, -0.0353,\n",
            "        -0.3079, -0.1324,  0.0159, -0.0901,  0.0587, -0.1904,  0.0329, -0.0917,\n",
            "        -0.1868,  0.1291, -0.0983,  0.2295, -0.0460,  0.0782,  0.2174, -0.0860,\n",
            "        -0.0632,  0.0147,  0.2532, -0.1983, -0.0559, -0.1347, -0.1047,  0.2557,\n",
            "        -0.0375,  0.1112,  0.2426,  0.0513,  0.0338,  0.0226, -0.0265, -0.0354,\n",
            "         0.2254,  0.0701,  0.1000, -0.0354, -0.1091, -0.0935,  0.1870, -0.1131,\n",
            "        -0.0137,  0.0294,  0.0387, -0.1793,  0.1030, -0.0578, -0.1171,  0.1292,\n",
            "        -0.0330,  0.1033, -0.0823, -0.1447,  0.0184, -0.0575,  0.1345,  0.3284,\n",
            "        -0.1468,  0.0193, -0.0402, -0.1906,  0.0019,  0.1148,  0.1021,  0.1823],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([-0.0579,  0.1689, -0.1333,  0.0440, -0.0266, -0.0011,  0.0603, -0.0740,\n",
            "         0.0107, -0.3106,  0.0266,  0.0528,  0.0219,  0.1869, -0.0935,  0.0216,\n",
            "        -0.0048, -0.1381,  0.3545, -0.0887, -0.0787, -0.2029, -0.0675,  0.0742,\n",
            "        -0.1256,  0.1132, -0.2626,  0.0436, -0.0034, -0.0219,  0.2895,  0.1864,\n",
            "        -0.0420, -0.0311,  0.0965, -0.1860,  0.2117, -0.2148,  0.2001, -0.0826,\n",
            "        -0.0010,  0.0125, -0.1094, -0.0818,  0.0277,  0.1379, -0.1395,  0.1128,\n",
            "         0.2386, -0.0720, -0.0524, -0.0070,  0.1745, -0.0836,  0.0213, -0.0462,\n",
            "        -0.0090,  0.0520, -0.0125,  0.0251, -0.1102, -0.1818,  0.1115,  0.1220,\n",
            "         0.0385, -0.2967,  0.0064, -0.0019,  0.0311, -0.1098,  0.0626,  0.0873,\n",
            "         0.0772, -0.0516, -0.0661,  0.0598, -0.0666,  0.0548, -0.1628,  0.0679,\n",
            "         0.0162, -0.0785,  0.3284, -0.0226,  0.0192, -0.2614,  0.2953, -0.0142,\n",
            "        -0.2045, -0.1593,  0.1234, -0.1492, -0.0574, -0.1531,  0.1241, -0.0977,\n",
            "         0.0582,  0.3059,  0.1938,  0.1886,  0.1182,  0.0229,  0.2251, -0.0533,\n",
            "        -0.1689, -0.0126, -0.1685, -0.0016,  0.0894, -0.1330,  0.2870,  0.1770,\n",
            "        -0.0113, -0.1065,  0.0299,  0.0150, -0.0489,  0.0450, -0.1258, -0.1145,\n",
            "        -0.1105, -0.4288,  0.0914, -0.2579,  0.0255, -0.1074, -0.2034, -0.0838,\n",
            "        -0.0897,  0.3271,  0.0310,  0.0956, -0.1119, -0.1659, -0.0471, -0.0258,\n",
            "         0.0349, -0.0123, -0.0751, -0.0561, -0.1474,  0.1604, -0.0602, -0.0697,\n",
            "         0.1083, -0.1558,  0.0259, -0.0394,  0.1088, -0.1219, -0.0144,  0.0227,\n",
            "         0.0780, -0.0306,  0.0560,  0.0747,  0.1582,  0.1932,  0.0648,  0.1877,\n",
            "         0.0034,  0.0392,  0.1959,  0.0976, -0.1179,  0.0668, -0.0896, -0.0428,\n",
            "        -0.0029, -0.0543, -0.2190,  0.0739, -0.1833, -0.2505,  0.0335, -0.1487,\n",
            "         0.1757,  0.0956, -0.0609,  0.0954, -0.0374, -0.1707,  0.0150, -0.1127,\n",
            "         0.1292, -0.1301,  0.0264, -0.0006, -0.0708, -0.2055, -0.0074, -0.0353,\n",
            "        -0.3079, -0.1324,  0.0159, -0.0901,  0.0587, -0.1904,  0.0329, -0.0917,\n",
            "        -0.1868,  0.1291, -0.0983,  0.2295, -0.0460,  0.0782,  0.2174, -0.0860,\n",
            "        -0.0632,  0.0147,  0.2532, -0.1983, -0.0559, -0.1347, -0.1047,  0.2557,\n",
            "        -0.0375,  0.1112,  0.2426,  0.0513,  0.0338,  0.0226, -0.0265, -0.0354,\n",
            "         0.2254,  0.0701,  0.1000, -0.0354, -0.1091, -0.0935,  0.1870, -0.1131,\n",
            "        -0.0137,  0.0294,  0.0387, -0.1793,  0.1030, -0.0578, -0.1171,  0.1292,\n",
            "        -0.0330,  0.1033, -0.0823, -0.1447,  0.0184, -0.0575,  0.1345,  0.3284,\n",
            "        -0.1468,  0.0193, -0.0402, -0.1906,  0.0019,  0.1148,  0.1021,  0.1823],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1mTime step 9  \u001b[0m\n",
            "Actual input word = <EOS>\n",
            "Embedding of input word 9 = tensor([ 0.0738,  0.1837,  0.1527,  0.1025,  0.0536, -0.0414, -0.0770, -0.0985,\n",
            "         0.1387, -0.0492,  0.0291, -0.0296, -0.1478, -0.0221, -0.1831,  0.0469,\n",
            "        -0.0026, -0.3309,  0.1031, -0.1595, -0.1379,  0.0172, -0.0621,  0.1119,\n",
            "         0.0166,  0.0078, -0.3408, -0.1463, -0.1351,  0.1169,  0.1366,  0.0447,\n",
            "        -0.1423,  0.0346, -0.0199, -0.2908,  0.1369,  0.0225,  0.2534, -0.1907,\n",
            "         0.0046,  0.0396, -0.0082,  0.1142,  0.0082,  0.1444, -0.1863,  0.1637,\n",
            "        -0.0596,  0.1912, -0.2284, -0.1375, -0.0491,  0.2413,  0.0821, -0.2802,\n",
            "        -0.1062, -0.0177, -0.0739,  0.1752, -0.3163,  0.0823,  0.0451,  0.1975,\n",
            "         0.0091, -0.3020,  0.0237,  0.1210, -0.0464,  0.0319,  0.0416, -0.1745,\n",
            "         0.1173, -0.1614, -0.0193, -0.0072, -0.0452,  0.2376, -0.2374,  0.1479,\n",
            "         0.0352, -0.1063, -0.0617,  0.0931, -0.0194, -0.1880,  0.1686,  0.0046,\n",
            "        -0.2147, -0.1502, -0.0086, -0.2165,  0.0844, -0.0610,  0.0586, -0.2925,\n",
            "        -0.1517,  0.3276,  0.1524,  0.1119,  0.2829, -0.0697,  0.2074, -0.1203,\n",
            "         0.0561,  0.2859, -0.1708,  0.1970,  0.0068,  0.0800,  0.0031,  0.0381,\n",
            "         0.2004,  0.0470, -0.1664,  0.0697, -0.2155,  0.1598,  0.0674, -0.0062,\n",
            "        -0.1676, -0.2910,  0.0539,  0.0068, -0.0931,  0.0570, -0.0454, -0.0879,\n",
            "        -0.2053,  0.2919, -0.0850,  0.1186, -0.0441, -0.2110,  0.0691, -0.1003,\n",
            "         0.0453, -0.0537,  0.0159,  0.0517, -0.0435,  0.1360, -0.0688,  0.0358,\n",
            "         0.0803, -0.1625, -0.0127,  0.0635,  0.0854, -0.3164, -0.2142,  0.2225,\n",
            "        -0.0082, -0.0837,  0.0014,  0.2565,  0.0572,  0.0272,  0.1001, -0.0445,\n",
            "        -0.3109,  0.0339,  0.1186,  0.1090, -0.1884,  0.0294, -0.0168, -0.0123,\n",
            "        -0.1414,  0.2193, -0.2696, -0.0175, -0.0230, -0.1946, -0.2589, -0.1380,\n",
            "         0.1513,  0.1471, -0.0429,  0.2262,  0.0167,  0.0682,  0.0828, -0.2155,\n",
            "         0.0363, -0.2132, -0.0528,  0.0089,  0.0880, -0.1866,  0.0505, -0.0986,\n",
            "        -0.1808,  0.0155, -0.0090, -0.1218, -0.0086,  0.0135, -0.0836, -0.0688,\n",
            "        -0.1599,  0.2821,  0.0181,  0.0896,  0.1774,  0.0506,  0.2781,  0.0361,\n",
            "         0.0184, -0.2131,  0.0537, -0.0441, -0.1461,  0.1038, -0.2416,  0.1285,\n",
            "        -0.1572, -0.0242,  0.2450,  0.1024,  0.1541, -0.0404, -0.1568, -0.1231,\n",
            "         0.0085, -0.0842,  0.0533,  0.0940,  0.0678,  0.1769,  0.0207, -0.0623,\n",
            "         0.0156, -0.1405,  0.0414, -0.0918,  0.1964,  0.0312, -0.1900, -0.0371,\n",
            "         0.0919,  0.0924, -0.1191, -0.2598, -0.1821,  0.1364,  0.0911,  0.2422,\n",
            "        -0.1258, -0.0059, -0.1962, -0.0084,  0.0761, -0.0224,  0.1139, -0.0054],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Encoder output at this time step = tensor([ 0.0738,  0.1837,  0.1527,  0.1025,  0.0536, -0.0414, -0.0770, -0.0985,\n",
            "         0.1387, -0.0492,  0.0291, -0.0296, -0.1478, -0.0221, -0.1831,  0.0469,\n",
            "        -0.0026, -0.3309,  0.1031, -0.1595, -0.1379,  0.0172, -0.0621,  0.1119,\n",
            "         0.0166,  0.0078, -0.3408, -0.1463, -0.1351,  0.1169,  0.1366,  0.0447,\n",
            "        -0.1423,  0.0346, -0.0199, -0.2908,  0.1369,  0.0225,  0.2534, -0.1907,\n",
            "         0.0046,  0.0396, -0.0082,  0.1142,  0.0082,  0.1444, -0.1863,  0.1637,\n",
            "        -0.0596,  0.1912, -0.2284, -0.1375, -0.0491,  0.2413,  0.0821, -0.2802,\n",
            "        -0.1062, -0.0177, -0.0739,  0.1752, -0.3163,  0.0823,  0.0451,  0.1975,\n",
            "         0.0091, -0.3020,  0.0237,  0.1210, -0.0464,  0.0319,  0.0416, -0.1745,\n",
            "         0.1173, -0.1614, -0.0193, -0.0072, -0.0452,  0.2376, -0.2374,  0.1479,\n",
            "         0.0352, -0.1063, -0.0617,  0.0931, -0.0194, -0.1880,  0.1686,  0.0046,\n",
            "        -0.2147, -0.1502, -0.0086, -0.2165,  0.0844, -0.0610,  0.0586, -0.2925,\n",
            "        -0.1517,  0.3276,  0.1524,  0.1119,  0.2829, -0.0697,  0.2074, -0.1203,\n",
            "         0.0561,  0.2859, -0.1708,  0.1970,  0.0068,  0.0800,  0.0031,  0.0381,\n",
            "         0.2004,  0.0470, -0.1664,  0.0697, -0.2155,  0.1598,  0.0674, -0.0062,\n",
            "        -0.1676, -0.2910,  0.0539,  0.0068, -0.0931,  0.0570, -0.0454, -0.0879,\n",
            "        -0.2053,  0.2919, -0.0850,  0.1186, -0.0441, -0.2110,  0.0691, -0.1003,\n",
            "         0.0453, -0.0537,  0.0159,  0.0517, -0.0435,  0.1360, -0.0688,  0.0358,\n",
            "         0.0803, -0.1625, -0.0127,  0.0635,  0.0854, -0.3164, -0.2142,  0.2225,\n",
            "        -0.0082, -0.0837,  0.0014,  0.2565,  0.0572,  0.0272,  0.1001, -0.0445,\n",
            "        -0.3109,  0.0339,  0.1186,  0.1090, -0.1884,  0.0294, -0.0168, -0.0123,\n",
            "        -0.1414,  0.2193, -0.2696, -0.0175, -0.0230, -0.1946, -0.2589, -0.1380,\n",
            "         0.1513,  0.1471, -0.0429,  0.2262,  0.0167,  0.0682,  0.0828, -0.2155,\n",
            "         0.0363, -0.2132, -0.0528,  0.0089,  0.0880, -0.1866,  0.0505, -0.0986,\n",
            "        -0.1808,  0.0155, -0.0090, -0.1218, -0.0086,  0.0135, -0.0836, -0.0688,\n",
            "        -0.1599,  0.2821,  0.0181,  0.0896,  0.1774,  0.0506,  0.2781,  0.0361,\n",
            "         0.0184, -0.2131,  0.0537, -0.0441, -0.1461,  0.1038, -0.2416,  0.1285,\n",
            "        -0.1572, -0.0242,  0.2450,  0.1024,  0.1541, -0.0404, -0.1568, -0.1231,\n",
            "         0.0085, -0.0842,  0.0533,  0.0940,  0.0678,  0.1769,  0.0207, -0.0623,\n",
            "         0.0156, -0.1405,  0.0414, -0.0918,  0.1964,  0.0312, -0.1900, -0.0371,\n",
            "         0.0919,  0.0924, -0.1191, -0.2598, -0.1821,  0.1364,  0.0911,  0.2422,\n",
            "        -0.1258, -0.0059, -0.1962, -0.0084,  0.0761, -0.0224,  0.1139, -0.0054],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdqlvZPQ8tpS",
        "outputId": "c3cc4a4e-26b7-4fcc-f125-bbbc4e7bd01f"
      },
      "source": [
        "encoder_outputs.shape, encoder_hidden.shape"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_VLnxpbSymL",
        "outputId": "a9352016-0977-4b1b-964f-b8ab91dff255"
      },
      "source": [
        "encoder_outputs[0:2]"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.7404e-03, -1.2574e-02, -7.9709e-02, -2.2541e-01,  2.0221e-01,\n",
              "         -3.9594e-02,  1.4831e-01, -1.2166e-01, -2.3984e-02,  3.7537e-02,\n",
              "         -1.3545e-01,  3.0687e-02, -2.9208e-03, -1.3461e-01,  1.5627e-02,\n",
              "          1.3022e-01,  2.1747e-02, -1.2067e-01,  3.4441e-02,  2.6524e-01,\n",
              "         -2.0081e-01, -7.3790e-02, -1.1992e-01,  7.4761e-02,  4.9959e-02,\n",
              "         -5.7309e-02, -3.3934e-03,  9.9923e-02, -2.7905e-02,  1.1331e-01,\n",
              "         -9.8245e-02,  2.3474e-02, -1.6201e-01,  2.8248e-01,  4.5370e-04,\n",
              "          1.8438e-02,  1.6254e-01,  2.5445e-02, -9.2324e-02,  9.9566e-02,\n",
              "          4.3235e-02,  1.4791e-02,  2.1224e-02, -1.2188e-01, -1.1304e-01,\n",
              "          4.1086e-02,  1.0605e-02,  1.9316e-02, -1.7542e-01, -7.0712e-02,\n",
              "         -4.9099e-02, -6.4439e-03,  8.4479e-02,  1.4176e-01,  9.0914e-02,\n",
              "          1.9221e-01,  2.0863e-01,  3.1516e-02,  7.4726e-02,  1.8408e-02,\n",
              "          3.3193e-02,  2.8756e-02,  5.4822e-02, -1.1065e-01,  1.3199e-02,\n",
              "         -1.7515e-01, -1.3164e-01, -1.5275e-01, -6.2487e-02, -5.2374e-02,\n",
              "          8.4159e-02,  3.3590e-01,  4.6157e-03,  3.1924e-02,  5.4948e-02,\n",
              "          1.6527e-01,  1.2701e-01, -1.7934e-01,  4.8617e-02,  1.2399e-01,\n",
              "         -7.7840e-02, -6.8525e-02, -7.4728e-02, -2.4921e-01,  6.6092e-02,\n",
              "          6.7926e-02,  1.1641e-01,  1.1550e-01, -1.4369e-01, -8.5842e-02,\n",
              "          3.8797e-02,  1.9765e-01,  9.0110e-02,  1.0933e-01,  6.1769e-02,\n",
              "          1.1876e-01, -1.8151e-01, -1.0070e-02, -2.5931e-01, -1.3194e-01,\n",
              "         -4.6608e-02, -9.7133e-02, -3.6880e-03,  4.9007e-02, -3.1443e-02,\n",
              "         -3.5084e-01,  5.8218e-02, -1.2325e-01,  2.3131e-02,  5.8090e-02,\n",
              "          1.5486e-01,  1.1947e-01,  1.6777e-01,  5.5942e-02,  2.5701e-02,\n",
              "          5.0261e-02, -1.9091e-01, -3.1058e-02, -1.0889e-01,  1.5941e-02,\n",
              "         -1.5661e-01,  8.7630e-02,  1.6249e-01,  6.5845e-02, -1.2574e-01,\n",
              "         -1.1056e-01, -5.3128e-02,  7.4478e-02,  1.5295e-02,  1.0134e-01,\n",
              "         -6.0471e-02,  1.7040e-02, -1.6875e-01,  2.2089e-01, -2.1587e-02,\n",
              "          4.7071e-02,  8.8537e-02,  4.5972e-03,  2.8527e-02, -4.7885e-02,\n",
              "          3.2380e-02, -4.5627e-02, -1.1279e-01,  4.9798e-02,  1.5307e-01,\n",
              "          1.8866e-01, -9.4590e-02, -1.1085e-01, -2.9748e-02,  5.1831e-02,\n",
              "         -1.6676e-01, -1.6152e-02,  1.8859e-01, -1.6424e-01, -1.5780e-01,\n",
              "          1.4081e-01, -9.7888e-03,  2.2236e-02,  2.9923e-01, -2.1588e-01,\n",
              "          2.0221e-01, -1.5070e-01, -1.1986e-01, -6.2842e-02, -3.9710e-02,\n",
              "          5.7863e-02, -7.4018e-03,  4.7224e-02,  1.4657e-02,  7.8310e-03,\n",
              "         -2.4714e-03,  9.4963e-02,  6.5055e-02,  1.3206e-01, -5.3701e-02,\n",
              "          5.4320e-02,  3.5012e-02, -1.2744e-01,  4.1376e-02, -4.3073e-02,\n",
              "         -1.6005e-01, -1.1851e-01, -1.4920e-01, -5.5358e-02, -2.4309e-02,\n",
              "          1.4672e-01,  1.0709e-01, -1.1625e-01,  8.3350e-02,  4.2860e-02,\n",
              "         -7.4062e-02,  1.9749e-02,  9.1351e-02, -6.5075e-03,  5.1320e-02,\n",
              "          4.4563e-02, -6.2671e-02,  1.3640e-01, -1.7275e-01, -5.9474e-02,\n",
              "         -9.5417e-02, -9.3218e-02, -8.4253e-03,  1.6743e-01,  3.5937e-02,\n",
              "          3.6061e-03,  2.5401e-01,  3.4617e-02, -1.4159e-02, -1.1884e-02,\n",
              "          1.4856e-02,  3.1548e-02,  1.4647e-01, -1.3334e-01,  1.6163e-01,\n",
              "          1.0397e-01,  1.9726e-01,  5.9293e-02, -3.2403e-01, -2.4518e-01,\n",
              "          1.6628e-01, -1.0183e-01, -1.7268e-01, -5.3286e-02, -1.5993e-01,\n",
              "         -1.7158e-01,  8.3295e-02, -3.1103e-01, -3.2140e-02, -2.4236e-01,\n",
              "         -1.8092e-02,  5.9446e-02,  8.7990e-02,  1.0586e-01, -2.0268e-01,\n",
              "         -1.5600e-01, -8.7719e-02,  1.6516e-01,  1.2719e-01, -1.3480e-01,\n",
              "         -5.8427e-02,  2.2348e-01,  2.3927e-02,  1.5146e-01, -1.4254e-01,\n",
              "         -4.1002e-02, -3.2237e-02,  3.3867e-02, -1.8553e-01, -2.3881e-02,\n",
              "         -3.6675e-02, -1.3531e-01,  3.1247e-02,  1.3167e-01,  5.5930e-02,\n",
              "          1.0087e-01],\n",
              "        [-3.0305e-02,  3.7694e-02,  3.0864e-02, -3.9056e-02,  2.8671e-02,\n",
              "          1.7517e-01,  1.2107e-01, -8.0150e-02, -1.9217e-01,  3.3253e-03,\n",
              "         -2.9697e-01, -7.7618e-02,  1.8089e-01, -2.8413e-01,  1.4238e-01,\n",
              "          4.4699e-02,  6.8107e-02,  1.0066e-01, -5.4100e-02,  5.6840e-02,\n",
              "          2.7732e-02, -3.1711e-01,  1.4865e-01, -3.7703e-02,  8.5816e-02,\n",
              "         -1.1863e-01, -8.6223e-03, -9.6734e-03, -5.8366e-04,  3.8014e-02,\n",
              "         -1.7592e-01,  2.2739e-02,  1.3150e-01,  1.4589e-01, -3.0228e-03,\n",
              "          7.6171e-02, -4.3913e-02,  8.0587e-02, -2.9274e-02, -1.2337e-01,\n",
              "          1.1036e-01, -1.8046e-01,  1.3142e-01, -3.4802e-01, -3.7229e-02,\n",
              "         -1.3292e-01, -4.9211e-02,  2.6290e-02, -3.1135e-01,  1.2645e-01,\n",
              "         -1.5080e-01,  4.3625e-02,  3.3105e-02,  4.9387e-02,  1.1138e-02,\n",
              "          3.8271e-01,  5.0407e-03, -7.3291e-02, -7.8971e-02, -1.0280e-01,\n",
              "         -3.1646e-02,  2.3424e-01,  2.6274e-01, -1.8468e-01, -1.5207e-01,\n",
              "         -1.2150e-01, -1.1243e-01,  2.1658e-01, -1.2677e-02,  1.7980e-02,\n",
              "          6.2932e-02,  1.4329e-01,  5.9587e-03, -1.0345e-01,  1.7881e-01,\n",
              "          2.4014e-02, -3.8641e-02,  5.5746e-02,  8.7145e-02,  2.3612e-01,\n",
              "         -5.0616e-02, -3.8400e-02, -1.5322e-01, -1.8202e-01,  5.8360e-02,\n",
              "          3.2825e-01,  2.4026e-01,  2.0451e-01, -3.0773e-01,  1.2567e-01,\n",
              "          1.5091e-01,  2.2167e-01,  6.4176e-02, -2.0219e-02,  2.1282e-01,\n",
              "          3.6229e-01,  9.2716e-02, -5.2712e-02, -2.4072e-01,  7.5824e-02,\n",
              "         -5.9769e-02, -2.2925e-01, -2.3508e-01,  2.2204e-01,  3.2184e-02,\n",
              "         -2.2884e-01, -2.0680e-01, -6.0944e-03, -1.5827e-01,  9.1670e-02,\n",
              "          1.5052e-01,  8.2806e-02,  2.0282e-01,  5.3975e-02,  3.0020e-02,\n",
              "         -2.5687e-01, -8.2558e-02, -2.1634e-01,  1.1622e-01, -2.5890e-01,\n",
              "         -1.1661e-01,  3.8121e-03,  6.4024e-02,  6.9124e-02, -9.7166e-02,\n",
              "         -3.5579e-02,  9.7996e-02,  1.4634e-01,  7.3206e-02,  1.2615e-01,\n",
              "         -1.2315e-01, -1.1725e-01, -1.1920e-01,  3.2937e-01,  2.8553e-02,\n",
              "         -2.8121e-02,  5.5615e-02,  1.0382e-01,  4.7851e-02,  2.1057e-03,\n",
              "          3.3733e-02,  1.0336e-01, -6.1787e-02, -1.2162e-02,  1.6010e-01,\n",
              "          1.0411e-01, -1.4344e-01, -1.9172e-01,  2.8203e-02,  2.4970e-01,\n",
              "         -8.2186e-02,  5.0877e-02, -2.5270e-02, -1.0048e-01, -4.0504e-02,\n",
              "          7.8087e-02,  8.4244e-02, -8.5973e-02,  2.1341e-01, -6.1240e-02,\n",
              "          3.5512e-02, -1.6859e-01,  4.4553e-02, -1.7007e-01,  4.3023e-02,\n",
              "          3.8577e-03, -2.4335e-01, -6.9042e-02,  9.5058e-02, -2.0611e-02,\n",
              "          1.3292e-01,  1.5261e-01,  2.6665e-01,  3.9614e-02, -1.5694e-01,\n",
              "          4.3606e-02, -8.4237e-02,  1.5954e-02, -2.3135e-01, -1.3377e-03,\n",
              "         -3.2588e-02, -6.5666e-02, -2.6581e-01,  1.7667e-01, -7.5400e-02,\n",
              "         -5.6797e-02, -1.9771e-01, -2.6984e-01, -1.2775e-01,  2.4865e-01,\n",
              "         -1.1503e-01,  9.2206e-02,  6.6984e-02, -7.0141e-02, -2.8172e-01,\n",
              "          2.1681e-01, -1.9547e-01,  7.6768e-03, -2.8772e-02, -1.5661e-01,\n",
              "          1.1968e-01,  4.2182e-02,  5.8420e-02,  2.4670e-02,  8.9106e-02,\n",
              "          1.2659e-01,  2.6097e-01,  6.9148e-02, -1.7640e-01,  1.1584e-01,\n",
              "         -2.2605e-02, -1.4990e-01,  1.1148e-01,  2.7394e-04, -3.7362e-02,\n",
              "         -1.0834e-01,  4.4021e-01, -7.6258e-02,  5.7872e-02, -5.0859e-02,\n",
              "          1.1922e-01,  5.0538e-02, -3.7229e-01, -1.1490e-02, -9.2234e-02,\n",
              "         -1.0123e-01, -8.2394e-02, -1.5059e-01,  1.1023e-01, -1.7246e-01,\n",
              "         -5.3477e-02,  2.9653e-03, -1.6153e-01,  8.1870e-02, -1.3147e-01,\n",
              "         -2.8181e-01, -1.4494e-01,  2.9916e-01, -6.3313e-02, -7.8265e-02,\n",
              "         -3.8483e-02,  2.5539e-02, -1.8500e-01,  1.2806e-01, -1.7966e-01,\n",
              "          4.4629e-03,  4.0111e-03,  7.2859e-02,  1.1091e-01,  1.5510e-01,\n",
              "         -4.5166e-02,  1.1891e-01, -4.7062e-02,  5.7472e-02,  1.3160e-01,\n",
              "          3.8889e-02]], device='cuda:0', grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hg46z1Tllmf"
      },
      "source": [
        "\n",
        "Cool! Next let's build out Decoder where we have attention in-built.\n",
        "\n",
        "# Decoder with Attention\n",
        "\n",
        "Here is the plan. \n",
        "\n",
        "1. First input to the decoder will be SOS_token, later inputs would be the words it predicted (unless we implement teacher forcing)\n",
        "2. decoder/LSTM's hidden state will be initialized with the encoder's last hidden state\n",
        "3. we will use LSTM's hidden state and last prediction to generate attention weight using a FC layer. \n",
        "4. this attention weight will be used to weigh the encoder_outputs using batch matric multiplication. This will give us a NEW view on how to look at encoder_states.\n",
        "5. this attention applied encoder_states will then be concatenated with the input, and then sent a linear layer and _then_ sent to the LSTM. \n",
        "6. LSTM's output will be sent to a FC layer to predict one of the output_language words\n",
        "\n",
        "Let's prepare all the inputs we need to do this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NVJALEFUrH4",
        "outputId": "b04468d8-af9c-4712-c5c4-078abc0090ef"
      },
      "source": [
        "decoder_input = torch.tensor([[SOS_token]],device = device) #First input to the decoder will be SOS_token\n",
        "decoder_hidden,decoder_cell = encoder_hidden,encoder_cell #decoder/LSTM's hidden state will be initialized with the encoder's last hidden state,similarly cell state is also initiated\n",
        "decoder_hidden.shape,decoder_input.shape\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrqrzzj5mlP9"
      },
      "source": [
        "Let's create embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_2J2ksMmZGm",
        "outputId": "547196d7-f341-4b25-9292-fe8a8e4aaf67"
      },
      "source": [
        "output_size = output_lang.n_words\n",
        "embedding = nn. Embedding(output_size,256).to(device)\n",
        "embedded = embedding(decoder_input)\n",
        "embedded.shape,output_size"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), 2803)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddV5LxCpnE5X"
      },
      "source": [
        "we will use LSTM's hidden state and last prediction to generate attention weight using a FC layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HhWdjrSnJTm",
        "outputId": "9de059c4-2fc3-4df0-d7c1-43fa53a3f1f1"
      },
      "source": [
        "attn_weights_layer = nn.Linear(256 * 2, MAX_LENGTH).to(device)#since input to attention weights will be concatenated tensor of previous hidden state and output\n",
        "decoder_hidden.shape,embedded.shape# since previous output is current input\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBVxJwgApEml",
        "outputId": "f6589223-9846-4cef-eb38-8fe51284fbd0"
      },
      "source": [
        "decoder_hidden[0].shape"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV3LBVQvoaFN",
        "outputId": "da0137a7-0082-43c8-82bd-0efbab5abdf9"
      },
      "source": [
        "torch.cat((decoder_hidden[0],embedded[0]), dim =1).shape"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR4JeIOpoBCw",
        "outputId": "e7b84d93-5661-4bcd-b44b-cc543a776124"
      },
      "source": [
        "attn_weights = attn_weights_layer(torch.cat((decoder_hidden[0],embedded[0]), dim =1))\n",
        "\n",
        "attn_weights.shape,attn_weights"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 10]),\n",
              " tensor([[ 0.1070,  0.7801,  0.4179, -0.5705, -0.1452, -0.0084,  0.1125, -0.2949,\n",
              "          -0.4868,  0.6633]], device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpQkNSz8wj_r"
      },
      "source": [
        "Will take softmax of these weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8birhuhRwnEU",
        "outputId": "b7e3ee87-ef22-474e-f0d7-74d53d35a2f5"
      },
      "source": [
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_weights"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0955, 0.1872, 0.1303, 0.0485, 0.0742, 0.0851, 0.0960, 0.0639, 0.0527,\n",
              "         0.1666]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDK7qcRfpj5z"
      },
      "source": [
        "Next , let's apply attention weights to encoder outputs to see which word has to concentrate during each time step.Before that let's reshape attn_weights and encoder_output as matrix multiplication will accept only 3D tensors as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXf4AKsOqJZo",
        "outputId": "4aff2406-ce22-41d3-c29b-52e029191d1b"
      },
      "source": [
        "attn_weights.shape,encoder_outputs.shape"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 10]), torch.Size([10, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziAzPCTeqcGb",
        "outputId": "130de816-5a5f-4175-9a2f-2c78245ecbc2"
      },
      "source": [
        "attn_weights.unsqueeze(0).shape,encoder_outputs.unsqueeze(0).shape"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 10]), torch.Size([1, 10, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23CaKT7ypZsZ",
        "outputId": "6bb32175-1097-4667-bb66-56a3dc8e6dd5"
      },
      "source": [
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "attn_applied.shape"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMgx5hUoqntR"
      },
      "source": [
        "So, now we have this 256dm attn_applied encoder_outputs capturing what we should focus on on this step. We also have the input we already generated. That's 256dm again. LSTM is gonna take 256 only. So we need to concatenate them, send to a linear layer to reduce dimensions, and then send to LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fa8ziX3p12I",
        "outputId": "c30be1fa-8d48-4681-b9ed-02cacfa56077"
      },
      "source": [
        "input_layer_lstm = nn.Linear(256 * 2 , 256).to(device)\n",
        "input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "decoder_hidden.shape, input_to_lstm.shape\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS-XT_-zsC2s"
      },
      "source": [
        "Now let's build our decoder LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK8W0enirD2R",
        "outputId": "2d31a0b1-8648-413e-eef3-216b1e5dd13b"
      },
      "source": [
        "lstm = nn.LSTM(256,256).to(device)\n",
        "decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "decoder_output.shape,decoder_hidden.shape"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax724LWus4cV"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIHbZ-_MuL7b",
        "outputId": "73e0c34c-c632-4ae8-b294-302cc50cf4fb"
      },
      "source": [
        "output = F.relu(decoder_output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "output, output.shape"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0004, 0.0003, 0.0004,  ..., 0.0004, 0.0004, 0.0003]],\n",
              "        device='cuda:0', grad_fn=<SoftmaxBackward>), torch.Size([1, 2803]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbLi-BUivMQy",
        "outputId": "048c2c4e-99b6-48de-ca90-482cfe5d250c"
      },
      "source": [
        "output.data.topk(1)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([[0.0004]], device='cuda:0'), indices=tensor([[1881]], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aHHkpDBvWZy"
      },
      "source": [
        "Says highest value is 0.0004 and it is present at idx 1301"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM3N27Movd96",
        "outputId": "1881911a-4e0f-45fb-9691-19edc846b2e0"
      },
      "source": [
        "top_value, top_id =output.data.topk(1)\n",
        "top_word = output_lang.index2word[top_id.item()]\n",
        "top_value.item(), top_id.item(), top_word"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0004146175633650273, 1881, 'or')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxssPZxOv9cl"
      },
      "source": [
        " Let's combine all of these steps, we we have just processed 1 input till now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BFKde8LcvzTm",
        "outputId": "ad8051bf-ba6c-4fd3-ddfd-c4aeac39f13c"
      },
      "source": [
        "decoder_input = torch.tensor([[SOS_token]],device = device) #First input to the decoder will be SOS_token\n",
        "decoder_hidden,decoder_cell = encoder_hidden,encoder_cell\n",
        "output_size = output_lang.n_words\n",
        "embedding = nn.Embedding(output_size, 256).to(device)\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weight_layer = nn.Linear(256 * 2, MAX_LENGTH).to(device)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "input_layer_lstm = nn.Linear(256 * 2 , 256).to(device)\n",
        "input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "lstm = nn.LSTM(256,256).to(device)\n",
        "decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)\n",
        "output = F.relu(decoder_output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_id= output.data.topk(1)\n",
        "output_lang.index2word[top_id.item()]"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'he'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h4R9GRrxywl",
        "outputId": "dbcca563-6a61-4b0f-f25a-cd11c3f1ba86"
      },
      "source": [
        "embedding = nn.Embedding(output_size, 256).to(device)\n",
        "attn_weight_layer = nn.Linear(256 * 2, 10).to(device)\n",
        "input_layer_lstm = nn.Linear(256 * 2 , 256).to(device)\n",
        "lstm = nn.LSTM(256,256).to(device)\n",
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)\n",
        "\n",
        "decoder_input = torch.tensor([[SOS_token]],device = device) #First input to the decoder will be SOS_token\n",
        "decoder_hidden,decoder_cell = encoder_hidden,encoder_cell\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "output = F.relu(decoder_output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_id= output.data.topk(1)\n",
        "output_lang.index2word[top_id.item()], attn_weights"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('war',\n",
              " tensor([[0.0684, 0.1058, 0.0787, 0.0922, 0.0985, 0.1298, 0.1720, 0.0598, 0.0686,\n",
              "          0.1262]], device='cuda:0', grad_fn=<SoftmaxBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80RfCoU6wtZ"
      },
      "source": [
        "We will past last predicted word as input here , No teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfmPKdiKysp3",
        "outputId": "f468274a-89a9-4eea-bff2-b10196245fdd"
      },
      "source": [
        "\n",
        "decoder_input = torch.tensor([[top_id.item()]],device = device) #First input to the decoder will be SOS_token\n",
        "decoder_hidden,decoder_cell = encoder_hidden,encoder_cell\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "output = F.relu(decoder_output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_id= output.data.topk(1)\n",
        "output_lang.index2word[top_id.item()], attn_weights"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('acrobat',\n",
              " tensor([[0.1060, 0.1252, 0.1086, 0.0914, 0.0873, 0.1351, 0.0602, 0.0621, 0.0535,\n",
              "          0.1706]], device='cuda:0', grad_fn=<SoftmaxBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgio_9DmzDvf",
        "outputId": "d2df0f79-33e6-468b-ad1b-bedc61f528b0"
      },
      "source": [
        "\n",
        "decoder_input = torch.tensor([[top_id.item()]],device = device) \n",
        "decoder_hidden,decoder_cell = encoder_hidden,encoder_cell\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "output = F.relu(decoder_output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_id= output.data.topk(1)\n",
        "output_lang.index2word[top_id.item()], attn_weights"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('easily',\n",
              " tensor([[0.0852, 0.0970, 0.0521, 0.1161, 0.1062, 0.1120, 0.0884, 0.0575, 0.0976,\n",
              "          0.1878]], device='cuda:0', grad_fn=<SoftmaxBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ez50VGQ6sib"
      },
      "source": [
        "Now let's apply full teacher forcing, we will be sending output indices as inputs to decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24k-RFS8zQZU",
        "outputId": "c5c0cdae-d363-40e9-86e7-fc9d57b033a7"
      },
      "source": [
        "pred = []\n",
        "pred_idx = []\n",
        "for i in range(4):\n",
        "  decoder_input = torch.tensor([[output_indices[i]]],device = device) \n",
        "  decoder_hidden,decoder_cell = encoder_hidden,encoder_cell\n",
        "  output_size = output_lang.n_words\n",
        "  embedded = embedding(decoder_input)\n",
        "  attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "  attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "  attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "  input_to_lstm = input_layer_lstm(torch.cat((embedded[0], attn_applied[0]),dim = 1))\n",
        "  input_to_lstm = input_to_lstm.unsqueeze(0)\n",
        "  decoder_output,(decoder_hidden , decoder_cell) = lstm( input_to_lstm, (decoder_hidden , decoder_cell))\n",
        "  output = F.relu(decoder_output)\n",
        "  output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "  top_value, top_id= output.data.topk(1)\n",
        "  output_lang.index2word[top_id.item()], attn_weights\n",
        "  pred.append(output_lang.index2word[top_id.item()])\n",
        "  pred_idx.append(top_id.item())\n",
        "  print('\\033[1m' +\"iteration {}  \\033[0m\".format(i))\n",
        "  print('Actual output word = {}'.format(output_sentence.split(\" \")[i]))\n",
        "  print('Predicted output word = {}'.format(output_lang.index2word[top_id.item()]))\n",
        "  print('Actual output index = {}'.format(output_indices[i]))\n",
        "  print('Predicted output word = {}'.format( top_id.item()))\n",
        "  print(\"attention weights are as follows , {}\".format(attn_weights))\n",
        "  print(\"-----------------------------------------------------\")"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1miteration 0  \u001b[0m\n",
            "Actual output word = i\n",
            "Predicted output word = shizuoka\n",
            "Actual output index = 2\n",
            "Predicted output word = 1020\n",
            "attention weights are as follows , tensor([[0.0839, 0.0805, 0.1422, 0.1088, 0.0913, 0.0734, 0.0709, 0.1302, 0.1144,\n",
            "         0.1044]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1miteration 1  \u001b[0m\n",
            "Actual output word = m\n",
            "Predicted output word = husband\n",
            "Actual output index = 3\n",
            "Predicted output word = 586\n",
            "attention weights are as follows , tensor([[0.0389, 0.1585, 0.0543, 0.0873, 0.1292, 0.1652, 0.1287, 0.0957, 0.0567,\n",
            "         0.0854]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1miteration 2  \u001b[0m\n",
            "Actual output word = not\n",
            "Predicted output word = they\n",
            "Actual output index = 147\n",
            "Predicted output word = 221\n",
            "attention weights are as follows , tensor([[0.0537, 0.1014, 0.0668, 0.0881, 0.1242, 0.0896, 0.0825, 0.1809, 0.0720,\n",
            "         0.1408]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "-----------------------------------------------------\n",
            "\u001b[1miteration 3  \u001b[0m\n",
            "Actual output word = going\n",
            "Predicted output word = apple\n",
            "Actual output index = 61\n",
            "Predicted output word = 1188\n",
            "attention weights are as follows , tensor([[0.0787, 0.1345, 0.0969, 0.1438, 0.1265, 0.0760, 0.0918, 0.1384, 0.0727,\n",
            "         0.0408]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9Qve3671H9u",
        "outputId": "6609b6e4-4172-418f-d05a-8d00c0df32ad"
      },
      "source": [
        "  print('Actual input sentence = {}'.format(input_sentence))\n",
        "  print('Actual output sentence = {}'.format(output_sentence))\n",
        "  print('Actual output indices = {}'.format(output_indices))\n",
        "  print('Predicted output sentence = {}'.format(\" \".join(pred)))\n",
        "  print('Predicted output indices = {}'.format(pred_idx))\n",
        "  "
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual input sentence = je ne vais pas jouer a ce jeu .\n",
            "Actual output sentence = i m not going to play this game .\n",
            "Actual output indices = [2, 3, 147, 61, 532, 2070, 797, 1519, 4, 1]\n",
            "Predicted output sentence = shizuoka husband they apple\n",
            "Predicted output indices = [1020, 586, 221, 1188]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}