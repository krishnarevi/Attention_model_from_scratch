{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w5_Capstone.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db5224b74aa445d09e6f1845e1f887ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6a88827b32c40babf6df42e630a896d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_264aa58364b54992bb4cb68ee2b878f5",
              "IPY_MODEL_8191ede3ba584c14826a6778f11eeb8c",
              "IPY_MODEL_1b42d990772648cb80299ea9b2cc7239"
            ]
          }
        },
        "d6a88827b32c40babf6df42e630a896d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "264aa58364b54992bb4cb68ee2b878f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b545b4ae51674b6a9f5e5d56c0c57a59",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebcc4d7f10874f1c9ca5e8796eeda261"
          }
        },
        "8191ede3ba584c14826a6778f11eeb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e229c9e09d374595920abe6f85c9e7f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 384,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 384,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d350d0e825a45129e7b76d6566b6ef0"
          }
        },
        "1b42d990772648cb80299ea9b2cc7239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44c30c66f4444c9d89fbc75fd419d9b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 384/384 [00:00&lt;00:00, 8.41kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e71159e96744e1489afd11e55eaf99f"
          }
        },
        "b545b4ae51674b6a9f5e5d56c0c57a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebcc4d7f10874f1c9ca5e8796eeda261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e229c9e09d374595920abe6f85c9e7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d350d0e825a45129e7b76d6566b6ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44c30c66f4444c9d89fbc75fd419d9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e71159e96744e1489afd11e55eaf99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e505e7ae6fe483889597ada9555872a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_074b44ef712a43318dce9d27ef74bfb2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_015c9a61b3f94f0ca244aea170f9e1f3",
              "IPY_MODEL_bcd0bda8504a46cf8de5f8fce9189c92",
              "IPY_MODEL_eb5f59d180054ab0b2bb8dd1c19bdfb0"
            ]
          }
        },
        "074b44ef712a43318dce9d27ef74bfb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "015c9a61b3f94f0ca244aea170f9e1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51ad9cd9ad05495aa58e8798a3fa3c07",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a4208707c45405c9c0d7cd9c9a90f1d"
          }
        },
        "bcd0bda8504a46cf8de5f8fce9189c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8348c9b65973423c8c1f157b701548f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4bee843f12284f4385fb4023934f5f39"
          }
        },
        "eb5f59d180054ab0b2bb8dd1c19bdfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c46ae18465c4cc89fe8b1f68e72dc28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 891kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f221389df624e1c9dae713942a4c2e3"
          }
        },
        "51ad9cd9ad05495aa58e8798a3fa3c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a4208707c45405c9c0d7cd9c9a90f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8348c9b65973423c8c1f157b701548f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4bee843f12284f4385fb4023934f5f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c46ae18465c4cc89fe8b1f68e72dc28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f221389df624e1c9dae713942a4c2e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "678fadec0ac94b0ab4028a63b9918c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1117f05f1675478bb79af47adc581c0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e681c7f00b31416ab8cb01cdbed7a91b",
              "IPY_MODEL_a65a91c7bb0e487092db023bbe012d25",
              "IPY_MODEL_e11bebbe802b4b6b9a7030a11cc06aba"
            ]
          }
        },
        "1117f05f1675478bb79af47adc581c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e681c7f00b31416ab8cb01cdbed7a91b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92b053fb20f24751acdb9a5e50bb06b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e9f2e14379b49a4a6a773045ff14bb5"
          }
        },
        "a65a91c7bb0e487092db023bbe012d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8b994a4aec7849d48220c3e4946e9966",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 327051810,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 327051810,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff606ed63f4d42ce8184b3c66335c96c"
          }
        },
        "e11bebbe802b4b6b9a7030a11cc06aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3fff19532f864d7ea2e69282d108af24",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 327M/327M [00:08&lt;00:00, 33.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb71bf7d806f486b922b5599d1d468fc"
          }
        },
        "92b053fb20f24751acdb9a5e50bb06b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e9f2e14379b49a4a6a773045ff14bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b994a4aec7849d48220c3e4946e9966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff606ed63f4d42ce8184b3c66335c96c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fff19532f864d7ea2e69282d108af24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb71bf7d806f486b922b5599d1d468fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a0c95f22719440ba98ec980d980b126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e0fff0958bd4e51b9b875f2a7fa4285",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3da4ec7fb064796899d41d366da1d62",
              "IPY_MODEL_31f90dd70d314da78058f886c7e156e8",
              "IPY_MODEL_af2c0614f4f642fe9e721f5c2259e3a0"
            ]
          }
        },
        "0e0fff0958bd4e51b9b875f2a7fa4285": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3da4ec7fb064796899d41d366da1d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd4072a7ccb64a87a912b5d919f13131",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0cfca4bce3fb4bcc882094e79f74b6ca"
          }
        },
        "31f90dd70d314da78058f886c7e156e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b2fe7ad7ee74ee39ccd949e1d25e512",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0927c39691a94508935ec6fbe6622b8c"
          }
        },
        "af2c0614f4f642fe9e721f5c2259e3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdea972463fb494a8ff25248afdbf592",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26.0/26.0 [00:00&lt;00:00, 918B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8df5900e4e7a46eb933d01597863d9dc"
          }
        },
        "dd4072a7ccb64a87a912b5d919f13131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0cfca4bce3fb4bcc882094e79f74b6ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b2fe7ad7ee74ee39ccd949e1d25e512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0927c39691a94508935ec6fbe6622b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdea972463fb494a8ff25248afdbf592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8df5900e4e7a46eb933d01597863d9dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1d37ed5017b4494b3bb7b7ab93cb269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7126aa7b8bff46dea217b1de44c015bf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25775577d0be4aa9b674eb83a7db06ff",
              "IPY_MODEL_10c8bd91b55a46538312012f177d267d",
              "IPY_MODEL_a38d30984d544416a2ee9767a6c636bc"
            ]
          }
        },
        "7126aa7b8bff46dea217b1de44c015bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25775577d0be4aa9b674eb83a7db06ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a883c616b01347ffba7242d756d77c80",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af7cd0534d5946709b00b748dcb7271e"
          }
        },
        "10c8bd91b55a46538312012f177d267d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9336d964c98b489da01c6f6106e95df7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1600,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1600,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a413b1d59d40454fb3a1915e50c0e1a2"
          }
        },
        "a38d30984d544416a2ee9767a6c636bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_632ea67014ec4d4ba892ecabff3be3ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.60k/1.60k [00:00&lt;00:00, 56.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8944c3c2062b4f59b9eca9b7216dc8dc"
          }
        },
        "a883c616b01347ffba7242d756d77c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af7cd0534d5946709b00b748dcb7271e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9336d964c98b489da01c6f6106e95df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a413b1d59d40454fb3a1915e50c0e1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "632ea67014ec4d4ba892ecabff3be3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8944c3c2062b4f59b9eca9b7216dc8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08003b8fb5c2400a851984191038d80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a338a0152533404a9d3d6191173e7d1c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ca0536b7d74468ba4e74ba0e6f1ab3b",
              "IPY_MODEL_e576285b75bb4f67a4643545e516fea2",
              "IPY_MODEL_933a4d998b614a86a863430f18763d35"
            ]
          }
        },
        "a338a0152533404a9d3d6191173e7d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ca0536b7d74468ba4e74ba0e6f1ab3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7bb73e38c2a4ca8a34829046603183d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61b3c9417f604bee81f7d79cdca72a8d"
          }
        },
        "e576285b75bb4f67a4643545e516fea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e140cafac6b430cbed4ab2aa7ca7124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ca3fc45a3fa4735a0a2d00a06474494"
          }
        },
        "933a4d998b614a86a863430f18763d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd7d966fcc1941278b1d870718bc74c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.34MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a75873de60894d97804fb24874f76d8a"
          }
        },
        "f7bb73e38c2a4ca8a34829046603183d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61b3c9417f604bee81f7d79cdca72a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e140cafac6b430cbed4ab2aa7ca7124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ca3fc45a3fa4735a0a2d00a06474494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd7d966fcc1941278b1d870718bc74c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a75873de60894d97804fb24874f76d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "160eebf159ff4442bf39b51f9315b270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ec0f92eb72747e090cd5b798d8e50d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a8c5aec41be4e1e8839a7be37233962",
              "IPY_MODEL_c841d9a5a0dd453aac41a330b3bcb674",
              "IPY_MODEL_e563acb58c6741f98b0eeff8d9c598af"
            ]
          }
        },
        "9ec0f92eb72747e090cd5b798d8e50d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a8c5aec41be4e1e8839a7be37233962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d046df188954422e8dfbac1f9a2354b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8dad62f656634bbcba0cb403d51ab3fb"
          }
        },
        "c841d9a5a0dd453aac41a330b3bcb674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8680e1c2389a4b2185eed39ed5a667dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16cf1f44d0f04eb785a8b85fc2f78498"
          }
        },
        "e563acb58c6741f98b0eeff8d9c598af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5e0479d78d44702b93e5ce6f0fbfa4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 786kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2bd389479bc4fff90696cdb9b596ea4"
          }
        },
        "d046df188954422e8dfbac1f9a2354b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8dad62f656634bbcba0cb403d51ab3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8680e1c2389a4b2185eed39ed5a667dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16cf1f44d0f04eb785a8b85fc2f78498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5e0479d78d44702b93e5ce6f0fbfa4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2bd389479bc4fff90696cdb9b596ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1eceaecbf5204e5aa6ee06d3bdbb550d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39b1d63eb53746e99fab65583b19943a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f6cefb9e4ad34750823dd059675bc8d2",
              "IPY_MODEL_65ffe1df1e0947a9811d75f2b281648d",
              "IPY_MODEL_ebfaaa035c4d403189f20b0621643a64"
            ]
          }
        },
        "39b1d63eb53746e99fab65583b19943a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6cefb9e4ad34750823dd059675bc8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18aa7bf069154be8b01d1e4558a3e9c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db460b615833484eb2dad2bc0bd13046"
          }
        },
        "65ffe1df1e0947a9811d75f2b281648d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fcd81ac332a54d3cae252fba4035f818",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84abf42494aa468f9e7b2dbaffeb8e39"
          }
        },
        "ebfaaa035c4d403189f20b0621643a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c498d59273b5431eaea01d219f71858d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 3.82MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bc4587b53c44df29d43624ac7d883f7"
          }
        },
        "18aa7bf069154be8b01d1e4558a3e9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db460b615833484eb2dad2bc0bd13046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcd81ac332a54d3cae252fba4035f818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84abf42494aa468f9e7b2dbaffeb8e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c498d59273b5431eaea01d219f71858d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bc4587b53c44df29d43624ac7d883f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d649ef856358421ea91a1bd7bc0bd262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d43074c2d2514e30a989a215e070a860",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1acd1f076a3840559deb93b0caa8473b",
              "IPY_MODEL_e5db5564168c4c4688b9badf31ee8ef3",
              "IPY_MODEL_4da5e034ca1c4582a78a95c7f4749089"
            ]
          }
        },
        "d43074c2d2514e30a989a215e070a860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1acd1f076a3840559deb93b0caa8473b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_755e538076dc4a6dbf462bfa3bed27e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2286e5acc0942e3a7bf4ad1ae86fdac"
          }
        },
        "e5db5564168c4c4688b9badf31ee8ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e306614a80e3473398d3cd49be9883d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1018571383,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1018571383,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_414f56732334442fac0084b16142a5f4"
          }
        },
        "4da5e034ca1c4582a78a95c7f4749089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1016741cdec7467fa04bb052e5242e52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.02G/1.02G [00:26&lt;00:00, 32.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6369e3f409b4cc79f83312e6ece9a4d"
          }
        },
        "755e538076dc4a6dbf462bfa3bed27e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2286e5acc0942e3a7bf4ad1ae86fdac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e306614a80e3473398d3cd49be9883d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "414f56732334442fac0084b16142a5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1016741cdec7467fa04bb052e5242e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6369e3f409b4cc79f83312e6ece9a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/Attention_model_from_scratch/blob/main/w5_Capstone_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ6oIhPQExDl"
      },
      "source": [
        "In this notebook, we show how we can take advantage of these recent advances to train a long form question answering system which takes in a question, fetches relevant passages from a document corpus, and writes a multi-sentence answer based on the question and retrieved passages.In the last few months have seen some significant progress in cases where direct supervision is available, or with extensive task-specific pretraining. Here, we show how our custom dataset allows us to train a dense retrieval system without access to either, making dense retrieval models more accessible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqWP9ITEwE4"
      },
      "source": [
        "### 1- Preliminaries\n",
        "The implementation presented here relies on the Hugging Face 🤗transformers and 🤗nlp libraries. Wikipedia indexing relies on faiss for the dense version. You can get all of these by running:\n",
        "\n",
        "<!-- pip install elasticsearch -->\n",
        "pip install faiss_gpu\n",
        "pip install nlp\n",
        "pip install transformers\n",
        "<!-- \n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz\n",
        "tar -xzvf elasticsearch-7.7.1-linux-x86_64.tar.gz -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAsRpBK4DhS_",
        "outputId": "dbf617a6-cb50-4b29-9576-a886dc843790"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 18 06:32:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u14eRYn09Kp",
        "outputId": "61b3b449-3c1a-4738-a503-623c33f3cb16"
      },
      "source": [
        "!pip install faiss_gpu nlp transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss_gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7 MB 9.5 kB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 36.5 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.62.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, nlp, faiss-gpu\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed faiss-gpu-1.7.1.post2 huggingface-hub-0.0.17 nlp-0.4.0 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VgSm5-serJa"
      },
      "source": [
        "import functools\n",
        "import math\n",
        "import os  # noqa: F401\n",
        "from random import choice, randint\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import faiss  # noqa: F401\n",
        "import nlp  # noqa: F401\n",
        "import pandas as pd\n",
        "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddm2PdNBlkvZ",
        "outputId": "de483ee9-ea24-4a9c-d4dd-0e3efc916cfb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xS7XbaolnKg"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/END')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EgAlp0clpW5"
      },
      "source": [
        "# from lfqa_utils import *"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpLMu5SeE5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1952a29-368b-4b79-f86b-9d288d0142c1"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "path = '/content/drive/MyDrive/END'\n",
        "os.chdir(path)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB1FubpEnOWG",
        "outputId": "f26b9fc4-bddf-4b34-e328-b659d3040eca"
      },
      "source": [
        "folder = \"retriever_models\"\n",
        "# os.chdir(path)\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"retriever_models directory exists\")\n",
        "else:\n",
        "    print(\"retriever_models directory Doesn't exists, creating one\")\n",
        "    os.mkdir(folder)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current dir is: /content/drive/MyDrive/END\n",
            "retriever_models directory Doesn't exists, creating one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faCLqxJImdZU",
        "outputId": "af62bb5f-1b8c-41a9-caed-2dcf374ac884"
      },
      "source": [
        "folder = \"seq2seq_models\"\n",
        "# os.chdir(path)\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"seq2seq_models directory exists\")\n",
        "else:\n",
        "    print(\"seq2seq_models directory Doesn't exists, creating one\")\n",
        "    os.mkdir(folder)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current dir is: /content/drive/MyDrive/END\n",
            "seq2seq_models directory Doesn't exists, creating one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGAmr8IqmNNU"
      },
      "source": [
        "## 2. Data Description\n",
        "Let's recap: we are interested in the task of Long Form Question Answering. As in other Question Answering tasks, the model is presented with a question, and is required to generate a natural language answer. Whereas a majority of QA datasets contain mostly factoid questions, where the answer, such as a date or the name of a single entity, can be expressed in a few words or single sentence, Long Form QA focuses on questions which call for an explanation consisting of a few sentences or a few paragraphs.\n",
        "\n",
        "In order to teach a model to answer such questions, we use questions and answers written by Reddit users. Note that the nlp.load_dataset command above actually downloaded questions and their associated answers from the r/explainlikeimfive, r/askscience, and r/AskHistorians subreddits. We focus here on the ELI5/explainlikeimfive part to train the system, as these examples tend to be a little simpler.\n",
        "\n",
        "Let's look at one item from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v6YJLUleqY"
      },
      "source": [
        "with open(path+'/train_data.json') as f:\n",
        "        train = json.load(f)\n",
        "with open(path+'/test_data.json') as f:\n",
        "        test = json.load(f)\n",
        "with open(path+'/context_master.json') as f:\n",
        "        passage_snippets = json.load(f)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrV7bVHQeyfg",
        "outputId": "3571a052-beaa-48d4-f38a-6b815157de8f"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'x': 'Maxout Layer',\n",
              " 'y': 'The Maxout layer can be implemented as follows \\npython\\nclass Maxout(nn.Module):\\n def __init__(self, d_in, d_out, pool_size):\\n super().__init__()\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\n def forward(self, inputs):\\n shape = list(inputs.size())\\n shape[-1] = self.d_out\\n shape.append(self.pool_size)\\n max_dim = len(shape) - 1\\n out = self.lin(inputs)\\n m, i = out.view(*shape).max(max_dim)\\n return m\\n',\n",
              " 'z': 'For ones who need Maxout, I changed the above code to make it work. \\npython\\nclass Maxout(nn.Module):\\n def __init__(self, d_in, d_out, pool_size):\\n super().__init__()\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\n def forward(self, inputs):\\n shape = list(inputs.size())\\n shape[-1] = self.d_out\\n shape.append(self.pool_size)\\n max_dim = len(shape) - 1\\n out = self.lin(inputs)\\n m, i = out.view(*shape).max(max_dim)\\n return m\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKLzwpWfQNQ",
        "outputId": "2f1e6ef9-0598-4920-c029-969e73ba6ccf"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9140"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVg6eAz9e6aP",
        "outputId": "20a48a26-383a-4483-9b89-ad9fe8f4ab64"
      },
      "source": [
        "test[100]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 101,\n",
              " 'x': 'What do Variable(tensor, requires_grad) return instead of Variables?',\n",
              " 'y': 'Tensors',\n",
              " 'z': 'The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected, but they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors with the same method names.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZUuSCe1fSy_",
        "outputId": "0ac90fc0-bfbf-43ee-dce5-671a152714c5"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2286"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspIob65hT14"
      },
      "source": [
        "### 2 Retrieving Support Documents with an Dense retriever Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io61B76Fh7ro"
      },
      "source": [
        "The sparse retriever works by finding passages which feature the words from the query. However, it has no way to know a priori which of these words are more important in context, and seems to struggle with understanding the central theme of the query.\n",
        "\n",
        "Thankfully, some recent works have taken advantage of advances in pre-trained contextual word representations to solve this problem. Models such as DPR or REALM for example learn to compute a vector representation of the query, as well as vector representations of Wikipedia passages in such a way that the passages that best answers a question maximize the dot product between the two representations. Retrieval is then reduced to a Maximum Inner Product Search, which can be executed efficiently using systems like FAISS.\n",
        "\n",
        "These successes are very encouraging for our Open-Domain Long Form QA application. However, our task and setup do not quite meet the requirements of either of either of these approaches. On the one hand, the DPR system is trained using gold passage annotations: most major QA dataset tell the system which Wikipedia passage contains the answer. Unfortunately, we do not have such annotations for our data. On the other hand, while REALM is trained without passage supervision, it requires a pretty expensive pre-training step with an Inverse Cloze Task (100,000 steps with batch size 4096), and the ability to re-compute the embeddings of all Wikipedia passages regularly during training.\n",
        "\n",
        "In order to train a similar dense retrieval system at reduced cost without having access to gold passage annotation, we will have to take advantage of another unique feature of our dataset, namely the fact that the long form answers are quite similar in style to thesupport documents we want to index. Our hypothesis then is that if we train a system to embed the questions and answers in our dataset in a way that allows us to easily match questions to answers, then using the answer embedder on support documents should allow us to similarly match questions to supporting evidence from document corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PbDg4xciQn3"
      },
      "source": [
        "4.a - Contrastive Training with ELI5 In-Batch Negatives\n",
        "As mentioned above, we want to train a system to produce question and answer embeddings, such that the dot product between the representation of a question and any of its answers is greater than between it and answers of all of the other questions in the dataset.\n",
        "\n",
        "Unfortunately, actually comparing all questions to all answers before taking every single gradient step is computationally prohibitive: instead, we follow previous work in simply processing medium to large batches of question-answer pairs, and making sure that the dot product of a question with its answer is larger than with all other answers in the batch, and vice versa.\n",
        "\n",
        "We use a cross-entropy loss for the multinomial distribution over all of the answers (or questions) in a batch, and make use of PyTorch gradient checkpointing to be able to use large batches with limited GPU memory: you can find all implementation details in the RetrievalQAEmbedder class in eli5_utils.py.\n",
        "\n",
        "We use a single BERT-style pre-trained model to embed the questions and answers, and learn different projection matrices to bring both representations down to dimension 128: the projection matrices are trained from scratch as the sentence embedding model is fine-tuned. We found that the 8-layer distilled version of BERT from the Well-Read Students Learn Better paper performed as well or better as full BERT for a notable gain in computation speed: if you want an even faster model, that work provides pre-trained models spanning the full range of computation/accuracy trade-offs.\n",
        "\n",
        "The retriever model can than be trained with the following code: with batch size 32/512 on a single Tesla T4 GPU, one can run 10 training epochs in under 1 hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJa0ofK7f65J"
      },
      "source": [
        "###############\n",
        "# retriever training\n",
        "###############\n",
        "class ELI5DatasetQARetriver(Dataset):\n",
        "    def __init__(self, examples_array, num_rows, extra_answer_threshold=2, min_answer_length=1, training=True, n_samples=None):\n",
        "        self.data = examples_array\n",
        "        self.answer_thres = extra_answer_threshold\n",
        "        self.min_length = min_answer_length\n",
        "        self.training = training\n",
        "        self.n_samples = num_rows if n_samples is None else n_samples\n",
        "        self.num_rows = num_rows\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        example = self.data[idx]\n",
        "        question = example[\"x\"]\n",
        "        answer = example[\"y\"]\n",
        "        return (question, answer)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx % self.num_rows)\n",
        "\n",
        "\n",
        "class RetrievalQAEmbedder(torch.nn.Module):\n",
        "    def __init__(self, sent_encoder, dim):\n",
        "        super(RetrievalQAEmbedder, self).__init__()\n",
        "        self.sent_encoder = sent_encoder\n",
        "        self.output_dim = 128\n",
        "        self.project_q = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.project_a = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.ce_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def embed_sentences_checkpointed(self, input_ids, attention_mask, checkpoint_batch_size=-1):\n",
        "        # reproduces BERT forward pass with checkpointing\n",
        "        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
        "            return self.sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
        "        else:\n",
        "            # prepare implicit variables\n",
        "            device = input_ids.device\n",
        "            input_shape = input_ids.size()\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "            head_mask = [None] * self.sent_encoder.config.num_hidden_layers\n",
        "            extended_attention_mask: torch.Tensor = self.sent_encoder.get_extended_attention_mask(\n",
        "                attention_mask, input_shape, device\n",
        "            )\n",
        "\n",
        "            # define function for checkpointing\n",
        "            def partial_encode(*inputs):\n",
        "                encoder_outputs = self.sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask,)\n",
        "                sequence_output = encoder_outputs[0]\n",
        "                pooled_output = self.sent_encoder.pooler(sequence_output)\n",
        "                return pooled_output\n",
        "\n",
        "            # run embedding layer on everything at once\n",
        "            embedding_output = self.sent_encoder.embeddings(\n",
        "                input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
        "            )\n",
        "            # run encoding and pooling on one mini-batch at a time\n",
        "            pooled_output_list = []\n",
        "            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
        "                b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
        "                pooled_output_list.append(pooled_output)\n",
        "            return torch.cat(pooled_output_list, dim=0)\n",
        "\n",
        "    def embed_questions(self, q_ids, q_mask, checkpoint_batch_size=-1):\n",
        "        q_reps = self.embed_sentences_checkpointed(q_ids, q_mask, checkpoint_batch_size)\n",
        "        return self.project_q(q_reps)\n",
        "\n",
        "    def embed_answers(self, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        a_reps = self.embed_sentences_checkpointed(a_ids, a_mask, checkpoint_batch_size)\n",
        "        return self.project_a(a_reps)\n",
        "\n",
        "    def forward(self, q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        device = q_ids.device\n",
        "        q_reps = self.embed_questions(q_ids, q_mask, checkpoint_batch_size)\n",
        "        a_reps = self.embed_answers(a_ids, a_mask, checkpoint_batch_size)\n",
        "        compare_scores = torch.mm(q_reps, a_reps.t())#cosine similarity\n",
        "        loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))#cross entrophy loss\n",
        "        loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
        "        loss = (loss_qa + loss_aq) / 2\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_qa_retriever_model(model_name=\"google/bert_uncased_L-8_H-512_A-8\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    # run bert_model on a dummy batch to get output dimension\n",
        "    d_ids = torch.LongTensor(\n",
        "        [[bert_model.config.bos_token_id if bert_model.config.bos_token_id is not None else 1]]\n",
        "    ).to(device)\n",
        "    d_mask = torch.LongTensor([[1]]).to(device)\n",
        "    sent_dim = bert_model(d_ids, attention_mask=d_mask)[1].shape[-1]\n",
        "    qa_embedder = RetrievalQAEmbedder(bert_model, sent_dim).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        qa_embedder.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, qa_embedder\n",
        "\n",
        "\n",
        "def make_qa_retriever_batch(qa_list, tokenizer, max_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        " \n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    \n",
        "    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),torch.LongTensor(q_toks[\"attention_mask\"]).to(device),)\n",
        "    # print(len(a_ls))\n",
        "\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    # TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
        "    # print(a_toks)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "\n",
        "    return (q_ids, q_mask, a_ids, a_mask)\n",
        "\n",
        "\n",
        "def train_qa_retriever_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    \n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # print(next(iter(data_loader)).shape)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        # print(\"q_ids\",q_ids.shape)\n",
        "        # print(\" q_mask,\", q_mask.shape)\n",
        "        # print(\"A_id\", a_ids.shape)\n",
        "        q_ids, q_mask, a_ids, a_mask = batch\n",
        "        pre_loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
        "        loss = pre_loss.sum()\n",
        "        # optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def evaluate_qa_retriever(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=eval_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    tot_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            q_ids, q_mask, a_ids, a_mask = batch\n",
        "            loss = model(q_ids, q_mask, a_ids, a_mask)\n",
        "            tot_loss += loss.item()\n",
        "        return tot_loss / (step + 1)\n",
        "\n",
        "\n",
        "def train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args):\n",
        "    qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
        "    qar_scheduler = get_linear_schedule_with_warmup(\n",
        "        qar_optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=(qar_args.num_epochs + 1) * math.ceil(len(qar_train_dset) / qar_args.batch_size),\n",
        "    )\n",
        "    for e in range(qar_args.num_epochs):\n",
        "        train_qa_retriever_epoch(qar_model, qar_train_dset, qar_tokenizer, qar_optimizer, qar_scheduler, qar_args, e)\n",
        "        m_save_dict = {\n",
        "            \"model\": qar_model.state_dict(),\n",
        "            \"optimizer\": qar_optimizer.state_dict(),\n",
        "            \"scheduler\": qar_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(qar_args.model_save_name))\n",
        "        # torch.save(m_save_dict, \"{}_{}.pth\".format(qar_args.model_save_name, e))\n",
        "        eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
        "        print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906,
          "referenced_widgets": [
            "db5224b74aa445d09e6f1845e1f887ce",
            "d6a88827b32c40babf6df42e630a896d",
            "264aa58364b54992bb4cb68ee2b878f5",
            "8191ede3ba584c14826a6778f11eeb8c",
            "1b42d990772648cb80299ea9b2cc7239",
            "b545b4ae51674b6a9f5e5d56c0c57a59",
            "ebcc4d7f10874f1c9ca5e8796eeda261",
            "e229c9e09d374595920abe6f85c9e7f0",
            "5d350d0e825a45129e7b76d6566b6ef0",
            "44c30c66f4444c9d89fbc75fd419d9b6",
            "6e71159e96744e1489afd11e55eaf99f",
            "0e505e7ae6fe483889597ada9555872a",
            "074b44ef712a43318dce9d27ef74bfb2",
            "015c9a61b3f94f0ca244aea170f9e1f3",
            "bcd0bda8504a46cf8de5f8fce9189c92",
            "eb5f59d180054ab0b2bb8dd1c19bdfb0",
            "51ad9cd9ad05495aa58e8798a3fa3c07",
            "8a4208707c45405c9c0d7cd9c9a90f1d",
            "8348c9b65973423c8c1f157b701548f7",
            "4bee843f12284f4385fb4023934f5f39",
            "7c46ae18465c4cc89fe8b1f68e72dc28",
            "2f221389df624e1c9dae713942a4c2e3",
            "678fadec0ac94b0ab4028a63b9918c70",
            "1117f05f1675478bb79af47adc581c0c",
            "e681c7f00b31416ab8cb01cdbed7a91b",
            "a65a91c7bb0e487092db023bbe012d25",
            "e11bebbe802b4b6b9a7030a11cc06aba",
            "92b053fb20f24751acdb9a5e50bb06b9",
            "0e9f2e14379b49a4a6a773045ff14bb5",
            "8b994a4aec7849d48220c3e4946e9966",
            "ff606ed63f4d42ce8184b3c66335c96c",
            "3fff19532f864d7ea2e69282d108af24",
            "eb71bf7d806f486b922b5599d1d468fc"
          ]
        },
        "id": "O30WJrHHhVcS",
        "outputId": "2aefdc3a-ddca-44d8-c905-ec8bc8058260"
      },
      "source": [
        "# training arguments\n",
        "class ArgumentsQAR():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512\n",
        "        self.max_length = 128\n",
        "        self.checkpoint_batch_size = 32\n",
        "        self.print_freq = 100\n",
        "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-768_A-12\"\n",
        "        self.model_save_name = \"retriever_model_l-8_h-768_b-512-512\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs =10\n",
        "\n",
        "qar_args = ArgumentsQAR()\n",
        "\n",
        "# prepare torch Dataset objects\n",
        "qar_train_dset = ELI5DatasetQARetriver(train,num_rows=len(train), training=True)\n",
        "qar_valid_dset = ELI5DatasetQARetriver(test,num_rows=len(test), training=False)\n",
        "\n",
        "# load pre-trained BERT and make model\n",
        "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
        "        model_name=qar_args.pretrained_model_name,\n",
        "        from_file=None,\n",
        "        device=\"cuda\"\n",
        ")\n",
        "\n",
        "# train the model\n",
        "train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db5224b74aa445d09e6f1845e1f887ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e505e7ae6fe483889597ada9555872a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "678fadec0ac94b0ab4028a63b9918c70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/327M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-8_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of    17 \t L: 6.504 \t -- 10.617\n",
            " 0     1 of    17 \t L: 6.502 \t -- 21.007\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    0: 5.879\n",
            " 1     0 of    17 \t L: 6.031 \t -- 10.398\n",
            " 1     1 of    17 \t L: 6.080 \t -- 20.783\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    1: 5.535\n",
            " 2     0 of    17 \t L: 5.596 \t -- 10.395\n",
            " 2     1 of    17 \t L: 5.560 \t -- 20.792\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    2: 5.126\n",
            " 3     0 of    17 \t L: 5.112 \t -- 10.401\n",
            " 3     1 of    17 \t L: 5.044 \t -- 20.807\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    3: 4.739\n",
            " 4     0 of    17 \t L: 4.427 \t -- 10.394\n",
            " 4     1 of    17 \t L: 4.414 \t -- 20.801\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    4: 4.565\n",
            " 5     0 of    17 \t L: 3.567 \t -- 10.379\n",
            " 5     1 of    17 \t L: 3.595 \t -- 20.756\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    5: 4.659\n",
            " 6     0 of    17 \t L: 2.878 \t -- 10.394\n",
            " 6     1 of    17 \t L: 2.725 \t -- 20.791\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    6: 4.948\n",
            " 7     0 of    17 \t L: 2.071 \t -- 10.414\n",
            " 7     1 of    17 \t L: 2.048 \t -- 20.794\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    7: 5.586\n",
            " 8     0 of    17 \t L: 1.379 \t -- 10.388\n",
            " 8     1 of    17 \t L: 1.413 \t -- 20.767\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    8: 6.405\n",
            " 9     0 of    17 \t L: 1.027 \t -- 10.385\n",
            " 9     1 of    17 \t L: 0.990 \t -- 20.771\n",
            "Saving model retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    9: 6.739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nICkWOlSaSbN"
      },
      "source": [
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGh7wbrQunpO"
      },
      "source": [
        "Once the model is trained, it can be used to compute passage embeddings for all document corpus. The make_qa_dense_index method takes advantage of numpy memory-mapping, so embeddings are written directly to disk. Again with a single GPU, computing the full set of passage embeddings should take about1 hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haiyF2cgJh-k"
      },
      "source": [
        "# type(qar_model)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hR82BRPJh5g"
      },
      "source": [
        "# qar_model.save_pretrained('/content/drive/MyDrive/TSAI/Capstone_1/retriever_models/')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ILtJrEMJh00"
      },
      "source": [
        "# qar_model = AutoModel.from_pretrained('/content/').to('cuda')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6p7knHMJhwb"
      },
      "source": [
        "# type(qar)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPMv6noAFnCd"
      },
      "source": [
        "# type(qar_tokenizer)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eimv_jdPF89b"
      },
      "source": [
        "# qar_tokenizer.save_pretrained('/content/drive/MyDrive/TSAI/Capstone_1/ret_tokenizer/')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWCct9PoGUfb"
      },
      "source": [
        "# qar_tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/TSAI/Capstone_1qa_s2s_tokenizer/')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yK4na7_YUEW"
      },
      "source": [
        "\n",
        "###############\n",
        "# ELI5-trained retrieval model usage\n",
        "###############\n",
        "def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda\"):\n",
        "    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n",
        "    return a_reps.numpy()\n",
        "def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda\"):\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n",
        "    return q_reps.numpy()\n",
        "def make_qa_dense_index(\n",
        "    qa_embedder,\n",
        "    tokenizer,\n",
        "    passages_dset,\n",
        "    batch_size=128,\n",
        "    max_length=128,\n",
        "    index_name=\"kilt_passages_reps.dat\",\n",
        "    dtype=\"float32\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    st_time = time()\n",
        "    fp = np.memmap(index_name, dtype=dtype, mode=\"w+\", shape=(len(passages_dset),128))\n",
        "    n_batches = math.ceil(len(passages_dset) / batch_size)\n",
        "    for i in range(n_batches):\n",
        "        passages = [p[\"z\"] for p in passages_dset[i * batch_size : (i + 1) * batch_size]]\n",
        "        reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length, device)\n",
        "        fp[i * batch_size : (i + 1) * batch_size] = reps\n",
        "        if i % 50 == 0:\n",
        "            print(i, time() - st_time)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGkQc1L0Kop9"
      },
      "source": [
        "# os.chdir(r'/content/drive/MyDrive/TSAI/Capstone_1')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fB4Dp5hukVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c688c322-6388-41eb-b412-ca0ada845655"
      },
      "source": [
        "if not os.path.isfile('passages_reps_32_l-8_h-768_b-512-512.dat'):\n",
        "  print(\"hi\")\n",
        "\n",
        "  make_qa_dense_index(\n",
        "          qar_model, qar_tokenizer, passage_snippets, device='cuda',\n",
        "          index_name='passages_reps_32_l-8_h-768_b-512-512.dat' )"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.40659284591674805\n",
            "50 17.19633650779724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01GnGvUo1YPC"
      },
      "source": [
        "### 4.b - Using the Trained Dense Retriever and document Index\n",
        "Now that we have trained our model to compute query and answer embeddings and used it to compute passage embeddings for all our document snippets, let's see whether it can actually find supporting evidence for a new question. Recalling the the two steps to using the dense retriever: we first compute an embedding for a new question, then do Max Inner Product Search with the pre-computed passage representations.\n",
        "\n",
        "The MIPS part can be executed efficiently with the faiss library. Additionally, since we computed 128-dimensional passage embeddings, the whole of the representations fits on a GPU, making retrieval even faster. We can create the faiss_gpu index with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdiIwQw4Qboj"
      },
      "source": [
        "n_ret = 5"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWm_WCC1g--"
      },
      "source": [
        "faiss_res = faiss.StandardGpuResources()\n",
        "passage_reps = np.memmap(\n",
        "            'passages_reps_32_l-8_h-768_b-512-512.dat',\n",
        "            dtype='float32', mode='r',\n",
        "            # shape=(wiki40b_snippets.num_rows, 128)\n",
        "            # wiki40b_snippets.num_rows = 11378343,english sections from wiki40B dataset\n",
        "            shape=(len(passage_snippets), 128)\n",
        ")\n",
        "\n",
        "doc_index_flat = faiss.IndexFlatIP(128)\n",
        "doc_gpu_index = faiss.index_cpu_to_gpu(faiss_res, 0, doc_index_flat)\n",
        "doc_gpu_index.add(passage_reps)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HFtgUv-30K4"
      },
      "source": [
        "\n",
        "def query_qa_dense_index(\n",
        "    question, qa_embedder, tokenizer, doc_passages, doc_index, n_results=n_ret, min_length=1, device=\"cuda\"\n",
        "):\n",
        "    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
        "    D, I = doc_index.search(q_rep, 2 * n_results)\n",
        "    res_passages = [doc_passages[int(i)] for i in I[0]]\n",
        "    support_doc = \"<P> \" + \" <P> \".join([p[\"z\"] for p in res_passages])\n",
        "    res_list = [dict([(k, p[k]) for k in [\"z\"]]) for p in res_passages]\n",
        "    res_list = [res for res in res_list if len(res[\"z\"].split()) > min_length][:n_results]\n",
        "    for r, sc in zip(res_list, D[0]):\n",
        "        r[\"score\"] = float(sc)\n",
        "    return support_doc, res_list"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1hJ2_U12lYQ"
      },
      "source": [
        "Now we can use the query_qa_dense_index function to query the dense index for our running example question :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhJqmCP52wnA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9c3280c-b5a7-44cd-aad0-f13099b84611"
      },
      "source": [
        "question = test[12]['x']\n",
        "question"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In what platform do the modules Conv2d() and Linear() run?'"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYoEC2yU2jUA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "6dcdcf74-8b4a-4c1b-8b1f-16f37b5b3b34"
      },
      "source": [
        "doc, res_list = query_qa_dense_index(question, qar_model, qar_tokenizer, passage_snippets, doc_gpu_index, device='cuda')\n",
        "print(res_list)\n",
        "df = pd.DataFrame({\n",
        "    \n",
        "    'Text': ['--- ' + question] + [res['z'] for res in res_list],\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'z': \"pytorch ships it's own cudnn\", 'score': 20.960006713867188}, {'z': 'logic and:\\n\\na * b\\n\\n\\nlogic or:\\n\\na + b\\n\\n', 'score': 20.245590209960938}, {'z': 'To verify that pytorch uses cudnn:\\n\\n>>> torch.backends.cudnn.version()\\n6021\\n', 'score': 19.688329696655273}, {'z': 'cc malfet', 'score': 19.112272262573242}, {'z': 'cudnn 9.0 is no longer supported. Supported cuda versions are 9.2, 10.1, 10.2', 'score': 18.883193969726562}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row0_col0,#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row1_col0,#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row2_col0,#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row3_col0,#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row4_col0,#T_fd29f2c2_184f_11ec_807b_0242ac1c0002row5_col0{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Text</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >--- In what platform do the modules Conv2d() and Linear() run?</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row1_col0\" class=\"data row1 col0\" >pytorch ships it's own cudnn</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row2_col0\" class=\"data row2 col0\" >logic and:\n",
              "\n",
              "a * b\n",
              "\n",
              "\n",
              "logic or:\n",
              "\n",
              "a + b\n",
              "\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row3_col0\" class=\"data row3 col0\" >To verify that pytorch uses cudnn:\n",
              "\n",
              ">>> torch.backends.cudnn.version()\n",
              "6021\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row4_col0\" class=\"data row4 col0\" >cc malfet</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_fd29f2c2_184f_11ec_807b_0242ac1c0002row5_col0\" class=\"data row5 col0\" >cudnn 9.0 is no longer supported. Supported cuda versions are 9.2, 10.1, 10.2</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f702cf362d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGKrGgr6u6a"
      },
      "source": [
        "### 4.c - Retriever Model Evaluation\n",
        "We have trained a retrieval model that seems to be working a little better than the traditional word-matching based approach, at least on our running example. Before we use it to actually answer questions, however, we would like to be able to get some quantitative evaluation of the performances of both approaches.\n",
        "\n",
        "For the retriever, we want to favor recall over precision: our first priority is to make sure that all of the information needed to write the answers is present in the support document. If there is unrelated information, the generation model can learn to sort it out. We measure this by computing the proportion of words in the high-scoring answers which are present in the retrieved support document. To focus on important words, we also weigh answer words by their Inverse Document Frequency. This gives us the following IDF-recall scoring function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suCtm8mSCI7H"
      },
      "source": [
        "## 5. Generating Answers with a Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8rHQ10Zg33h"
      },
      "source": [
        "# ELI5 seq2seq model training\n",
        "###############\n",
        "class ELI5DatasetS2S(Dataset):\n",
        "    def __init__(\n",
        "        self, examples_array,num_rows, make_doc_fun=None, doc_cache=None, training=True\n",
        "    ):\n",
        "        self.training = training\n",
        "        self.data = examples_array\n",
        "        self.make_doc_function = make_doc_fun\n",
        "        self.doc_cache = {} if doc_cache is None else doc_cache\n",
        "        self.num_rows = num_rows\n",
        "        assert not (make_doc_fun is None and doc_cache is None)\n",
        "        # make index of specific question-answer pairs from multi-answers\n",
        "        if self.training:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "        else:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_id_list)\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        i, j = self.qa_id_list[idx]\n",
        "        example = self.data[i]\n",
        "        question = example[\"x\"] \n",
        "        answer = example[\"y\"]\n",
        "        q_id = example[\"id\"]\n",
        "        if self.make_doc_function is not None:\n",
        "            self.doc_cache[q_id] = self.doc_cache.get(q_id, self.make_doc_function(example[\"x\"]))\n",
        "        document = self.doc_cache[q_id]\n",
        "        in_st = \"question: {} context: {}\".format(\n",
        "            question.lower().strip(), document.lower().strip(),\n",
        "        )\n",
        "        out_st = answer\n",
        "        return (in_st, out_st)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx)\n",
        "\n",
        "\n",
        "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        model.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    labels = a_ids[:, 1:].contiguous().clone()\n",
        "    labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
        "    # print(labels)\n",
        "    model_inputs = {\n",
        "        \"input_ids\": q_ids,\n",
        "        \"attention_mask\": q_mask,\n",
        "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "    # print(\"it'sme\",model_inputs)\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=True):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    if curriculum:\n",
        "        train_sampler = SequentialSampler(dataset)\n",
        "    else:\n",
        "        train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "\n",
        "  \n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch_inputs in enumerate(epoch_iterator):\n",
        "        # print(type(step))\n",
        "        pre_loss = model(**batch_inputs)[0].unsqueeze(dim=0)\n",
        "        # print(pre_loss,'pre_loss')\n",
        "        # print(pre_loss.shape)\n",
        "        # print(pre_loss.sum(),'sum')\n",
        "        loss = pre_loss.sum() / pre_loss.shape[0]\n",
        "        loss.backward()\n",
        "        # optimizer\n",
        "        if step % args.backward_freq == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    train_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch_inputs in enumerate(epoch_iterator):\n",
        "            pre_loss = model(**batch_inputs)[0].unsqueeze(dim=0)\n",
        "            # print(pre_loss,'pre_loss')\n",
        "            # print(pre_loss.shape)\n",
        "            # print(pre_loss.sum(),'sum')\n",
        "            loss = pre_loss.sum() / pre_loss.shape[0]\n",
        "            loc_loss += loss.item()\n",
        "            # print(\"loc loss here\",loc_loss)\n",
        "            loc_steps += 1\n",
        "            if step % args.print_freq == 0:\n",
        "                print(\n",
        "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                    )\n",
        "                )\n",
        "    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time,))\n",
        "\n",
        "\n",
        "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
        "    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
        "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
        "        s2s_optimizer,\n",
        "        num_warmup_steps=400,\n",
        "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
        "    )\n",
        "    for e in range(s2s_args.num_epochs):\n",
        "        # print((e == 0))\n",
        "\n",
        "        train_qa_s2s_epoch(\n",
        "            qa_s2s_model,\n",
        "            s2s_train_dset,\n",
        "            qa_s2s_tokenizer,\n",
        "            s2s_optimizer,\n",
        "            s2s_scheduler,\n",
        "            s2s_args,\n",
        "            e,\n",
        "            curriculum=True,\n",
        "        )\n",
        "        m_save_dict = {\n",
        "            \"model\": qa_s2s_model.state_dict(),\n",
        "            \"optimizer\": s2s_optimizer.state_dict(),\n",
        "            \"scheduler\": s2s_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
        "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
        "        # torch.save(m_save_dict, \"\\{}_{}.pth\".format(s2s_args.model_save_name, e))\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0JOPJ1S995Q"
      },
      "source": [
        "n_ret = 2"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8a0c95f22719440ba98ec980d980b126",
            "0e0fff0958bd4e51b9b875f2a7fa4285",
            "f3da4ec7fb064796899d41d366da1d62",
            "31f90dd70d314da78058f886c7e156e8",
            "af2c0614f4f642fe9e721f5c2259e3a0",
            "dd4072a7ccb64a87a912b5d919f13131",
            "0cfca4bce3fb4bcc882094e79f74b6ca",
            "0b2fe7ad7ee74ee39ccd949e1d25e512",
            "0927c39691a94508935ec6fbe6622b8c",
            "cdea972463fb494a8ff25248afdbf592",
            "8df5900e4e7a46eb933d01597863d9dc",
            "f1d37ed5017b4494b3bb7b7ab93cb269",
            "7126aa7b8bff46dea217b1de44c015bf",
            "25775577d0be4aa9b674eb83a7db06ff",
            "10c8bd91b55a46538312012f177d267d",
            "a38d30984d544416a2ee9767a6c636bc",
            "a883c616b01347ffba7242d756d77c80",
            "af7cd0534d5946709b00b748dcb7271e",
            "9336d964c98b489da01c6f6106e95df7",
            "a413b1d59d40454fb3a1915e50c0e1a2",
            "632ea67014ec4d4ba892ecabff3be3ea",
            "8944c3c2062b4f59b9eca9b7216dc8dc",
            "08003b8fb5c2400a851984191038d80b",
            "a338a0152533404a9d3d6191173e7d1c",
            "9ca0536b7d74468ba4e74ba0e6f1ab3b",
            "e576285b75bb4f67a4643545e516fea2",
            "933a4d998b614a86a863430f18763d35",
            "f7bb73e38c2a4ca8a34829046603183d",
            "61b3c9417f604bee81f7d79cdca72a8d",
            "2e140cafac6b430cbed4ab2aa7ca7124",
            "6ca3fc45a3fa4735a0a2d00a06474494",
            "dd7d966fcc1941278b1d870718bc74c3",
            "a75873de60894d97804fb24874f76d8a",
            "160eebf159ff4442bf39b51f9315b270",
            "9ec0f92eb72747e090cd5b798d8e50d7",
            "9a8c5aec41be4e1e8839a7be37233962",
            "c841d9a5a0dd453aac41a330b3bcb674",
            "e563acb58c6741f98b0eeff8d9c598af",
            "d046df188954422e8dfbac1f9a2354b9",
            "8dad62f656634bbcba0cb403d51ab3fb",
            "8680e1c2389a4b2185eed39ed5a667dc",
            "16cf1f44d0f04eb785a8b85fc2f78498",
            "a5e0479d78d44702b93e5ce6f0fbfa4b",
            "a2bd389479bc4fff90696cdb9b596ea4",
            "1eceaecbf5204e5aa6ee06d3bdbb550d",
            "39b1d63eb53746e99fab65583b19943a",
            "f6cefb9e4ad34750823dd059675bc8d2",
            "65ffe1df1e0947a9811d75f2b281648d",
            "ebfaaa035c4d403189f20b0621643a64",
            "18aa7bf069154be8b01d1e4558a3e9c3",
            "db460b615833484eb2dad2bc0bd13046",
            "fcd81ac332a54d3cae252fba4035f818",
            "84abf42494aa468f9e7b2dbaffeb8e39",
            "c498d59273b5431eaea01d219f71858d",
            "1bc4587b53c44df29d43624ac7d883f7",
            "d649ef856358421ea91a1bd7bc0bd262",
            "d43074c2d2514e30a989a215e070a860",
            "1acd1f076a3840559deb93b0caa8473b",
            "e5db5564168c4c4688b9badf31ee8ef3",
            "4da5e034ca1c4582a78a95c7f4749089",
            "755e538076dc4a6dbf462bfa3bed27e8",
            "f2286e5acc0942e3a7bf4ad1ae86fdac",
            "e306614a80e3473398d3cd49be9883d1",
            "414f56732334442fac0084b16142a5f4",
            "1016741cdec7467fa04bb052e5242e52",
            "d6369e3f409b4cc79f83312e6ece9a4d"
          ]
        },
        "id": "5Fhed0htCIL5",
        "outputId": "2b98c2a3-6668-4df6-c076-7b028dbdfc5c"
      },
      "source": [
        "# pre-computing support documents\n",
        "eli5_train_docs = []\n",
        "for example in train:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer,passage_snippets, doc_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    eli5_train_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "eli5_valid_docs = []\n",
        "for example in test:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer, passage_snippets, doc_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    eli5_valid_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "# training loop proper\n",
        "class ArgumentsS2S():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 2\n",
        "        self.backward_freq = 16\n",
        "        self.max_length = 512\n",
        "        self.print_freq = 100\n",
        "        self.model_save_name = \"eli5_bart_model\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs =3\n",
        "\n",
        "s2s_args = ArgumentsS2S()\n",
        "\n",
        "# eli5_train_docs = json.load(open('precomputed/eli5_train_precomputed_dense_docs.json'))\n",
        "# eli5_valid_docs = json.load(open('precomputed/eli5_valid_precomputed_dense_docs.json'))\n",
        "s2s_train_dset = ELI5DatasetS2S(train,num_rows =len(train), doc_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
        "s2s_valid_dset = ELI5DatasetS2S(test,num_rows =len(test), doc_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
        "\n",
        "qa_s2s_tokenizer, pre_model = make_qa_s2s_model(\n",
        "    model_name=\"facebook/bart-large\",\n",
        "    from_file=None,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "# qa_s2s_model = torch.nn.DataParallel(pre_model)\n",
        "qa_s2s_model =pre_model\n",
        "train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a0c95f22719440ba98ec980d980b126",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1d37ed5017b4494b3bb7b7ab93cb269",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08003b8fb5c2400a851984191038d80b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "160eebf159ff4442bf39b51f9315b270",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eceaecbf5204e5aa6ee06d3bdbb550d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d649ef856358421ea91a1bd7bc0bd262",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of  4570 \t L: 5.819 \t -- 0.416\n",
            " 0     1 of  4570 \t L: 15.740 \t -- 0.733\n",
            " 0   100 of  4570 \t L: 5.276 \t -- 32.972\n",
            " 0   200 of  4570 \t L: 4.891 \t -- 65.283\n",
            " 0   300 of  4570 \t L: 4.272 \t -- 97.957\n",
            " 0   400 of  4570 \t L: 3.781 \t -- 130.289\n",
            " 0   500 of  4570 \t L: 3.553 \t -- 162.859\n",
            " 0   600 of  4570 \t L: 3.406 \t -- 195.469\n",
            " 0   700 of  4570 \t L: 3.168 \t -- 227.510\n",
            " 0   800 of  4570 \t L: 2.922 \t -- 260.009\n",
            " 0   900 of  4570 \t L: 3.137 \t -- 292.243\n",
            " 0  1000 of  4570 \t L: 2.709 \t -- 324.783\n",
            " 0  1100 of  4570 \t L: 3.201 \t -- 357.386\n",
            " 0  1200 of  4570 \t L: 3.112 \t -- 390.004\n",
            " 0  1300 of  4570 \t L: 2.826 \t -- 422.442\n",
            " 0  1400 of  4570 \t L: 2.437 \t -- 455.058\n",
            " 0  1500 of  4570 \t L: 2.806 \t -- 487.680\n",
            " 0  1600 of  4570 \t L: 2.891 \t -- 520.037\n",
            " 0  1700 of  4570 \t L: 2.626 \t -- 552.583\n",
            " 0  1800 of  4570 \t L: 2.548 \t -- 584.690\n",
            " 0  1900 of  4570 \t L: 2.661 \t -- 616.380\n",
            " 0  2000 of  4570 \t L: 2.582 \t -- 648.372\n",
            " 0  2100 of  4570 \t L: 2.425 \t -- 680.207\n",
            " 0  2200 of  4570 \t L: 2.571 \t -- 712.777\n",
            " 0  2300 of  4570 \t L: 2.696 \t -- 745.297\n",
            " 0  2400 of  4570 \t L: 2.564 \t -- 777.872\n",
            " 0  2500 of  4570 \t L: 2.182 \t -- 810.370\n",
            " 0  2600 of  4570 \t L: 2.530 \t -- 842.836\n",
            " 0  2700 of  4570 \t L: 2.230 \t -- 875.370\n",
            " 0  2800 of  4570 \t L: 2.104 \t -- 908.110\n",
            " 0  2900 of  4570 \t L: 2.634 \t -- 940.264\n",
            " 0  3000 of  4570 \t L: 2.184 \t -- 972.493\n",
            " 0  3100 of  4570 \t L: 2.496 \t -- 1004.887\n",
            " 0  3200 of  4570 \t L: 2.525 \t -- 1037.345\n",
            " 0  3300 of  4570 \t L: 2.329 \t -- 1069.084\n",
            " 0  3400 of  4570 \t L: 2.259 \t -- 1100.936\n",
            " 0  3500 of  4570 \t L: 2.479 \t -- 1132.708\n",
            " 0  3600 of  4570 \t L: 2.520 \t -- 1164.764\n",
            " 0  3700 of  4570 \t L: 2.540 \t -- 1196.760\n",
            " 0  3800 of  4570 \t L: 2.338 \t -- 1228.512\n",
            " 0  3900 of  4570 \t L: 2.141 \t -- 1260.917\n",
            " 0  4000 of  4570 \t L: 2.511 \t -- 1293.523\n",
            " 0  4100 of  4570 \t L: 2.518 \t -- 1325.676\n",
            " 0  4200 of  4570 \t L: 1.981 \t -- 1358.267\n",
            " 0  4300 of  4570 \t L: 2.152 \t -- 1390.804\n",
            " 0  4400 of  4570 \t L: 2.368 \t -- 1423.268\n",
            " 0  4500 of  4570 \t L: 2.506 \t -- 1455.832\n",
            "Saving model eli5_bart_model\n",
            "    0 of  1143 \t L: 3.556 \t -- 0.115\n",
            "  100 of  1143 \t L: 3.214 \t -- 11.537\n",
            "  200 of  1143 \t L: 3.301 \t -- 22.975\n",
            "  300 of  1143 \t L: 3.354 \t -- 34.403\n",
            "  400 of  1143 \t L: 3.354 \t -- 45.931\n",
            "  500 of  1143 \t L: 3.355 \t -- 57.347\n",
            "  600 of  1143 \t L: 3.382 \t -- 68.595\n",
            "  700 of  1143 \t L: 3.383 \t -- 80.190\n",
            "  800 of  1143 \t L: 3.376 \t -- 91.414\n",
            "  900 of  1143 \t L: 3.384 \t -- 102.916\n",
            " 1000 of  1143 \t L: 3.393 \t -- 114.344\n",
            " 1100 of  1143 \t L: 3.398 \t -- 125.761\n",
            "Total \t L: 3.391 \t -- 130.572\n",
            " 1     0 of  4570 \t L: 2.861 \t -- 0.387\n",
            " 1     1 of  4570 \t L: 3.251 \t -- 0.709\n",
            " 1   100 of  4570 \t L: 2.248 \t -- 32.958\n",
            " 1   200 of  4570 \t L: 2.561 \t -- 65.584\n",
            " 1   300 of  4570 \t L: 2.298 \t -- 98.160\n",
            " 1   400 of  4570 \t L: 2.324 \t -- 130.788\n",
            " 1   500 of  4570 \t L: 2.302 \t -- 163.290\n",
            " 1   600 of  4570 \t L: 2.195 \t -- 195.625\n",
            " 1   700 of  4570 \t L: 2.233 \t -- 228.152\n",
            " 1   800 of  4570 \t L: 2.117 \t -- 260.776\n",
            " 1   900 of  4570 \t L: 2.313 \t -- 293.017\n",
            " 1  1000 of  4570 \t L: 2.095 \t -- 325.522\n",
            " 1  1100 of  4570 \t L: 2.457 \t -- 358.042\n",
            " 1  1200 of  4570 \t L: 2.350 \t -- 390.573\n",
            " 1  1300 of  4570 \t L: 2.285 \t -- 423.051\n",
            " 1  1400 of  4570 \t L: 1.859 \t -- 455.581\n",
            " 1  1500 of  4570 \t L: 2.285 \t -- 488.120\n",
            " 1  1600 of  4570 \t L: 2.441 \t -- 520.778\n",
            " 1  1700 of  4570 \t L: 2.211 \t -- 553.331\n",
            " 1  1800 of  4570 \t L: 2.106 \t -- 585.953\n",
            " 1  1900 of  4570 \t L: 2.170 \t -- 618.502\n",
            " 1  2000 of  4570 \t L: 2.259 \t -- 651.160\n",
            " 1  2100 of  4570 \t L: 1.934 \t -- 683.770\n",
            " 1  2200 of  4570 \t L: 2.175 \t -- 716.400\n",
            " 1  2300 of  4570 \t L: 2.239 \t -- 749.010\n",
            " 1  2400 of  4570 \t L: 2.198 \t -- 781.648\n",
            " 1  2500 of  4570 \t L: 1.903 \t -- 814.302\n",
            " 1  2600 of  4570 \t L: 2.193 \t -- 846.871\n",
            " 1  2700 of  4570 \t L: 1.934 \t -- 879.408\n",
            " 1  2800 of  4570 \t L: 1.823 \t -- 912.115\n",
            " 1  2900 of  4570 \t L: 2.251 \t -- 944.692\n",
            " 1  3000 of  4570 \t L: 1.822 \t -- 977.309\n",
            " 1  3100 of  4570 \t L: 2.002 \t -- 1009.919\n",
            " 1  3200 of  4570 \t L: 2.027 \t -- 1042.601\n",
            " 1  3300 of  4570 \t L: 1.905 \t -- 1075.207\n",
            " 1  3400 of  4570 \t L: 1.920 \t -- 1107.776\n",
            " 1  3500 of  4570 \t L: 1.915 \t -- 1140.392\n",
            " 1  3600 of  4570 \t L: 2.050 \t -- 1173.104\n",
            " 1  3700 of  4570 \t L: 2.086 \t -- 1205.681\n",
            " 1  3800 of  4570 \t L: 1.978 \t -- 1238.267\n",
            " 1  3900 of  4570 \t L: 1.727 \t -- 1270.897\n",
            " 1  4000 of  4570 \t L: 2.122 \t -- 1303.594\n",
            " 1  4100 of  4570 \t L: 2.121 \t -- 1336.224\n",
            " 1  4200 of  4570 \t L: 1.690 \t -- 1368.794\n",
            " 1  4300 of  4570 \t L: 1.780 \t -- 1401.312\n",
            " 1  4400 of  4570 \t L: 1.910 \t -- 1433.899\n",
            " 1  4500 of  4570 \t L: 1.910 \t -- 1466.409\n",
            "Saving model eli5_bart_model\n",
            "    0 of  1143 \t L: 3.309 \t -- 0.113\n",
            "  100 of  1143 \t L: 3.057 \t -- 11.562\n",
            "  200 of  1143 \t L: 3.158 \t -- 22.986\n",
            "  300 of  1143 \t L: 3.215 \t -- 34.425\n",
            "  400 of  1143 \t L: 3.212 \t -- 45.942\n",
            "  500 of  1143 \t L: 3.207 \t -- 57.347\n",
            "  600 of  1143 \t L: 3.237 \t -- 68.847\n",
            "  700 of  1143 \t L: 3.240 \t -- 80.309\n",
            "  800 of  1143 \t L: 3.235 \t -- 91.389\n",
            "  900 of  1143 \t L: 3.246 \t -- 102.413\n",
            " 1000 of  1143 \t L: 3.253 \t -- 113.325\n",
            " 1100 of  1143 \t L: 3.257 \t -- 123.914\n",
            "Total \t L: 3.251 \t -- 128.507\n",
            " 2     0 of  4570 \t L: 2.216 \t -- 0.388\n",
            " 2     1 of  4570 \t L: 1.944 \t -- 0.712\n",
            " 2   100 of  4570 \t L: 1.829 \t -- 32.412\n",
            " 2   200 of  4570 \t L: 2.091 \t -- 64.468\n",
            " 2   300 of  4570 \t L: 1.914 \t -- 97.168\n",
            " 2   400 of  4570 \t L: 1.956 \t -- 129.874\n",
            " 2   500 of  4570 \t L: 1.909 \t -- 162.479\n",
            " 2   600 of  4570 \t L: 1.777 \t -- 195.042\n",
            " 2   700 of  4570 \t L: 1.780 \t -- 227.704\n",
            " 2   800 of  4570 \t L: 1.764 \t -- 260.396\n",
            " 2   900 of  4570 \t L: 1.970 \t -- 292.993\n",
            " 2  1000 of  4570 \t L: 1.681 \t -- 325.582\n",
            " 2  1100 of  4570 \t L: 2.068 \t -- 358.219\n",
            " 2  1200 of  4570 \t L: 1.899 \t -- 390.865\n",
            " 2  1300 of  4570 \t L: 1.788 \t -- 423.460\n",
            " 2  1400 of  4570 \t L: 1.480 \t -- 456.129\n",
            " 2  1500 of  4570 \t L: 1.859 \t -- 488.795\n",
            " 2  1600 of  4570 \t L: 1.866 \t -- 521.544\n",
            " 2  1700 of  4570 \t L: 1.797 \t -- 554.153\n",
            " 2  1800 of  4570 \t L: 1.683 \t -- 586.848\n",
            " 2  1900 of  4570 \t L: 1.793 \t -- 619.451\n",
            " 2  2000 of  4570 \t L: 1.815 \t -- 652.133\n",
            " 2  2100 of  4570 \t L: 1.604 \t -- 684.775\n",
            " 2  2200 of  4570 \t L: 1.774 \t -- 717.475\n",
            " 2  2300 of  4570 \t L: 1.843 \t -- 750.135\n",
            " 2  2400 of  4570 \t L: 1.783 \t -- 782.810\n",
            " 2  2500 of  4570 \t L: 1.581 \t -- 815.512\n",
            " 2  2600 of  4570 \t L: 1.865 \t -- 847.726\n",
            " 2  2700 of  4570 \t L: 1.559 \t -- 880.379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RXg9GMtSs2P"
      },
      "source": [
        "We now have everything we need to answer any question! Now let's try the full system on our running example along with the first four questions of the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sdw-BbC_oV",
        "outputId": "65a44847-4dd1-49d8-aaab-0c79390188ad"
      },
      "source": [
        "import psutil\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "        bytes /= factor\n",
        "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
        "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================== Memory Information ========================================\n",
            "Total: 25.46GB\n",
            "Available: 22.30GB\n",
            "Used: 5.54GB\n",
            "Percentage: 12.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inSpTFrw9H6s"
      },
      "source": [
        "    import torch\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1xobu8OStdQ"
      },
      "source": [
        "# generate answer from input \"question: ... context: <p> ...\"\n",
        "def qa_s2s_generate(\n",
        "    question_doc,\n",
        "    qa_s2s_model,\n",
        "    qa_s2s_tokenizer,\n",
        "    num_answers=1,\n",
        "    num_beams=None,\n",
        "    min_len=64,\n",
        "    max_len=512,\n",
        "    do_sample=False,\n",
        "    temp=1.0,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    max_input_length=1024,\n",
        "    device=\"cuda:0\",\n",
        "):\n",
        "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n",
        "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
        "    generated_ids = qa_s2s_model.generate(\n",
        "        input_ids=model_inputs[\"input_ids\"],\n",
        "        attention_mask=model_inputs[\"attention_mask\"],\n",
        "        min_length=min_len,\n",
        "        max_length=max_len,\n",
        "        do_sample=do_sample,\n",
        "        early_stopping=True,\n",
        "        num_beams=1 if do_sample else n_beams,\n",
        "        temperature=temp,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        num_return_sequences=num_answers,\n",
        "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
        "    )\n",
        "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49FI64_uS6JZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db3896e4-926d-4fdc-9d5f-5a014f7e3a2c"
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "docs = []\n",
        "\n",
        "for i in [10] + [j for j in range(4)]:\n",
        "    # create support document with the dense index\n",
        "    question = test[i]['x']\n",
        "    doc, res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer,\n",
        "        passage_snippets, doc_gpu_index, device='cuda'\n",
        "    )\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=64,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    questions += [question]\n",
        "    answers += [answer]\n",
        "    docs += [doc]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Question': questions,\n",
        "    'Answer': answers,\n",
        "    'Documents' :docs\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col0,#T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col1,#T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col2,#T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col0,#T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col1,#T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col2,#T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col0,#T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col1,#T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col2,#T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col0,#T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col1,#T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col2,#T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col0,#T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col1,#T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col2{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Question</th>        <th class=\"col_heading level0 col1\" >Answer</th>        <th class=\"col_heading level0 col2\" >Documents</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >`with torch.enable_grad` also works outside a `no_grad` context</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col1\" class=\"data row0 col1\" >It seems there is no difference.\n",
              "Just that one takes a bool as input and not the other.\n",
              "Also  torch.set_grad_enabled(False) can be used as just a function to set the grad mode forever.\n",
              "Great, thanks a lot. Very convenient to use the same piece of code for training and evaluating.</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row0_col2\" class=\"data row0 col2\" ><P> It seems there is no difference: https://stackoverflow.com/a/53447634/6888630\n",
              "There is no difference.\n",
              "Just that one takes a bool as input and not the other.\n",
              "Also  torch.set_grad_enabled(False) can be used as just a function to set the grad mode forever.\n",
              "Great, thanks a lot. Very convenient to use the same piece of code for training and evaluating. <P> Yeah that makes sense. There's really nothing that prevents people from setting `requires_grad` to `False` after they construct the optimizer, and it won't even complain. We should just remove the check. <P> This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc. <P> \"Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`\" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you! <P> Keep in mind that set_grad_enabled is a state of the program (“do you want to keep track of gradients for outputs when the inputs require gradients”) and applies to new tensors, while someparam.requires_grad_(False) (which is the suggested form to disable gradients of tensors and parameters) says “this thing, when used as input, doesn’t require gradients”.\n",
              "To then decide whether a given operation’s output requires gradients, the autograd engine checks if both gradient-mode is enabled and any inputs require gradients.\n",
              "As such these are two distinct knobs you can operate independently, but they have a combined effect.\n",
              "\n",
              "\n",
              "\n",
              " eprox:\n",
              "\n",
              "with torch.set_grad_enabled(mode = True):\n",
              "    self.model.lin2.requires_grad = False # Will this work?\n",
              "   for m in self.model.mlp_f: m.set_grad_enabled = False # Or this?\n",
              "\n",
              "\n",
              "\n",
              "This seems to try to work on Modules, which isn’t a thing (and using requires_grad_ would tell you so, which is why it is preferred). If you wanted something like that, you should go for for p in self.model.mlp_f.parameters(): p.requires_grad_(False) which certainly works.\n",
              "\n",
              "\n",
              "Ok  that is useful. <P> 1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\n",
              "2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\n",
              "3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility <P> Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n",
              "\n",
              "Are zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n",
              "\n",
              "If the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()). <P> torch.is_grad_enabled Returns True if grad mode is currently enabled. <P> The torch.autograd.enable_grad documentation says:\n",
              "\n",
              "\n",
              "  Enables gradient calculation inside a no_grad context. This has no effect outside of no_grad.\n",
              "\n",
              "\n",
              "Given this wording, the following is expected:\n",
              "\n",
              "torch.set_grad_enabled(False)\n",
              "with torch.enable_grad:\n",
              "    # Gradient tracking will NOT be enabled here.\n",
              "torch.set_grad_enabled(True)\n",
              "\n",
              "\n",
              "vs:\n",
              "\n",
              "with torch.no_grad():\n",
              "    with torch.enable_grad:\n",
              "        # Gradient tracking IS enabled here.\n",
              "\n",
              "\n",
              "But as blue-phoenix shows, this is not the case.\n",
              "\n",
              "I raised an issue here.\n",
              " <P> > One question: do we need to do special handling when the `create_graph=True` flag is set?\n",
              " \n",
              " \n",
              " \n",
              " Other than setting create_graph=True when we call autograd.grad, no, I don't think so.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col0\" class=\"data row1 col0\" >\"exp_cuda\" not implemented for 'ComplexDouble'</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col1\" class=\"data row1 col1\" >We will take a look at this. I can reproduce this issue on my skylake machine. Thank you for reporting this issue, @shmsong. The team agrees it's a high priority. <P> CC @driazati for Final constant handling\n",
              "\n",
              "It's a constructor for a Tensor. It's generated by C macros, what you probably want to look for is `THCTensor_(new)` or `THCudaTensor_(New)`</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row1_col2\" class=\"data row1 col2\" ><P> @albanD Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes. <P> @carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return <P> I don't think the GPU behavior is completely wrong, for a mathematical reason: it should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.\n",
              " \n",
              " \n",
              " \n",
              " The way Numpy gets out of this situation, is they ask you for an initial element to handle the case explicitly, and error if you don't provide it. So in the end I agree with your suggested course of action. <P> thanks, we'll look into this! <P> @Markus-Goetz repasting my comment from #17738 (comment)\n",
              "\n",
              "the only determinism we aim to have is hashed on device, and for CPU, single-threaded.\n",
              "Even across different kinds of GPUs, all bets are off.\n",
              "\n",
              "We cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.\n",
              "\n",
              "For example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.\n",
              "\n",
              "This is inconsistent with numpy's, Eigen's, C++ STL etc.\n",
              "\n",
              "If you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression. <P> Hi, I am from @quansight team. I would like to work on this issue. <P> We will take a look at this. I can reproduce this issue on my skylake machine. <P> Thank you for reporting this issue, @shmsong. The team agrees it's a high priority. <P> cc @driazati for Final constant handling <P> @ThisIsIsaac it's a constructor for a Tensor. It's generated by C macros, what you probably want to look for is `THCTensor_(new)` or `THCudaTensor_(new)`</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col0\" class=\"data row2 col0\" >How to correctly use CTC Loss with GRU in pytorch?</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col1\" class=\"data row2 col1\" >The negative log likelihood loss (NLLLoss) is suitable for classification problems, where the output is one out of C classes. Since the classes are discrete, your labels need to be of the long type.\n",
              "In your case, in a comment, you say:\n",
              "\n",
              "I want to create a network that simulates a quadratic function with x as input and sth similar to x**2 as output.\n",
              "\n",
              "However, I don't think this is a very good benchmark. It does many small non-vectorized 1D transforms which is not what tensor libraries are best at. It's also</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row2_col2\" class=\"data row2 col2\" ><P> Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem. <P> criterion is defined as torch.nn.CrossEntropyLoss() in your notebook. As mentioned in documentation of CrossEntropyLoss, it expects probability values returned by model for each of the 'K' classes and corresponding value for ground-truth label as input. Now, probability values are float tensors, while ground-truth label should be a long tensor representing a class (class can not be a float, e.g. 2.3 can not represent a class). hence:\n",
              "\n",
              "loss = criterion(predictions, batch.label.long())\n",
              "\n",
              "\n",
              "should work.\n",
              " <P> Timing with gh-43011 still shows a performance drop with pytorch:\n",
              " \n",
              " \n",
              " \n",
              " zero elapsed time: 3.07e-05 seconds\n",
              " \n",
              " rot elapsed time: 5.59e-05 seconds\n",
              " \n",
              " loop elapsed time: 0.001341 seconds\n",
              " \n",
              " NUMPY TIME elapsed time: 0.0015816 seconds\n",
              " \n",
              " zero elapsed time: 8.81e-05 seconds\n",
              " \n",
              " rot elapsed time: 0.0002193 seconds\n",
              " \n",
              " loop elapsed time: 0.0037979 seconds\n",
              " \n",
              " PYTORCH TIME elapsed time: 0.00421 seconds\n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " However, I don't think this is a very good benchmark. It does many small non-vectorized 1D transforms which is not what tensor libraries are best at. It's also conflating `diag`, `fft`, `abs` and copy-assign into one single benchmark.\n",
              " \n",
              " \n",
              " \n",
              " If I use `timeit` to isolate each line:\n",
              " \n",
              " * diag is ~3 us for numpy and ~5-6 us for pytorch\n",
              " \n",
              " * fft is ~10 us for numpy and 20 us for pytorch\n",
              " \n",
              " * abs is ~9 us for numpy and 12 us for pytorch\n",
              " \n",
              " * assignment is ~1us in numpy and 11 us for pytorch.\n",
              " \n",
              " \n",
              " \n",
              " Every operator has at least 3 us overhead with pytorch. That's pretty bad, but for large tensors I'm sure it balances out a bit better. FFT is about 2x slower for this small 1D tensor, but for a `256 X 512` FFT pytorch is 2x faster than NumPy, even single threaded.\n",
              " \n",
              " \n",
              " \n",
              " The biggest standout is copying from one slice to another which is an order of magnitude slower in pytorch. <P> Take care that Pytorch CTC loss takes log softmax probabilities as input while CTC loss by Baidu does not mention that.\n",
              " <P> The negative log likelihood loss (NLLLoss) is suitable for classification problems, where the output is one out of C classes. Since the classes are discrete, your labels need to be of the long type.\n",
              "In your case, in a comment, you say:\n",
              "\n",
              "I want to create a network that simulates a quadratic function with x as input and sth similar to x**2 as output.\n",
              "\n",
              "This is a regression problem, where the output can have a real, continuous value. For this, you should use a suitable loss function such as the mean squared error loss (MSELoss). So, one way to fix would be changing F.nll_loss in your code to F.mse_loss.\n",
              " <P> \n",
              "Since both outputs the value between 0 and 1. I thought there shouldn’t be an issue.\n",
              "\n",
              "the output of log sigmoid isn’t between 0 and 1, pls refer to docs with link \"https://pytorch.org/docs/stable/nn.html?highlight=logsigmoid#torch.nn.LogSigmoid\"\n",
              "Sorry that was a mistake. I knew they are not.\n",
              "I meant to say sigmod and softmax both output the value between 0 and 1.\n",
              "post is updated\n",
              "To me, the logsigmoid+NLLLoss combination hardly makes any sense, because the objective function only tries to promote the gt_class, but no suppression on the negative ones. Maybe you wanna try sigmoid+bceloss.\n",
              " so much for your advice.\n",
              "the model trains with bce even with sigmoid\n",
              "softmax works as well with bce but only up to a certain point and the training collapse.\n",
              "I am not sure why but I guess it has something to do with the dependency among classes…\n",
              "Do you know if there is a loss function which will be good for both activation function.\n",
              "a naive answer: If you really want to test with a single loss function for both activation functions,  what about L2 loss with one-hot vectors as target?\n",
              "I’m not sure if it will give good performance though.\n",
              "Softmax is actually not an activation function…\n",
              "logsigmoid+nllloss doesn’t make sense mathematically (if you derive the gradients, you’ll find it.)\n",
              "PyTroch documentation says NLLLoss expects log probability though\n",
              "and that is logsigmoid + nllloss is how CrossEntropyLoss is constructed for PyTorch\n",
              "nn.CrossEntropyLoss uses F.log_softmax and nn.NLLLoss internally as shown here with link \"https://github.com/pytorch/pytorch/blob/2e97c82470966df6942f364102690460ea58403e/torch/nn/functional.py#L2028\". <P> Look at the description of F.nll_loss. It expects to get as input not the argmax of the prediction (type torch.long), but rather the full 64x50x43 prediction vectors (of type torch.float). Note that indeed the prediction you provide to F.nll_loss has an extra dimension more than the ground truth targets you provide.\n",
              "\n",
              "In your case, simply remove the argmax:\n",
              "\n",
              "loss = F.nll_loss(output, targets)\n",
              "\n",
              " <P> nn.CrossEntropyLoss() expects target tensors of type Long, but what you're passing is of type Double.\n",
              "Try to change this line\n",
              "from:\n",
              "single_loss = loss_function(y_pred, train_op)\n",
              "to:\n",
              "single_loss = loss_function(y_pred, train_op.long())\n",
              " <P> I was not able to reproduce non-deterministic results as long as bn weights are initialized to a fixed value. You are right that in your case the expected results is 0, as expected results is (x-mean)/var*weight[+bias], and, as long as mean is exactly equal to tensor values (as it should be), result is 0. However, due to some quirks of how cudnn computes output, a small (on the order of 1 ulp) error creeps in. With the default epsilon 1/var is approx 300, weight is on the order of 1, to get the result you are getting x-mean should be on the order of 1e-5. Given that x and mean are 100, it's enough for mean to have a 1e-7 relative error (which is approx 1 ulp) to produce the results you are seeing, which may happen due to fp arithmetic being inexact.  <P> For anyone else with a similar issue, I got it to work. I removed the log_softmax calculation, so this:\n",
              "tag_space = self.hidden2tag(outputs)\n",
              "tag_scores = F.log_softmax(tag_space, dim=0)\n",
              "return tag_scores[-1]\n",
              "\n",
              "becomes this:\n",
              "tag_space = self.hidden2tag(outputs)\n",
              "return tag_space[-1]\n",
              "\n",
              "I also changed NLLLoss to CrossEntropyLoss, (not shown above), and initialized CrossEntropyLoss with no parameters (aka no ignore_index).\n",
              "I am not certain why these changes were necessary (the docs even say that NLLLoss should be run after a log_softmax layer), but they got my model working and brought my loss back to a reasonable range (~0.5).</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col0\" class=\"data row3 col0\" >glibc error while importing torch</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col1\" class=\"data row3 col1\" >Try this if you have added the Anaconda directory to your bash shell PATH environment variable:\n",
              "\n",
              "python -c \"import struct; print(struct.calcsize('P') * 8)\"\n",
              "Checking Wheel Version:\n",
              "64 bit wheels typically contain amd64 or similar in their title\n",
              "32 bit wheels usually contain win32or similar in theirs title\n",
              "Switching to a 64 bit Python Installaion:\n",
              "The default Windows Python installer does not have a 64bit option. To acquire 64 bitPython, navigate to python.org/downloads/windows/, and select a version that specifies x86-64</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row3_col2\" class=\"data row3 col2\" ><P> It looks as though you may have the 32-bit installation of Python, in which case you're issue is this: #16633.\n",
              "\n",
              "Just be aware, that pyTorch doesn't work on 32-bit systems.\n",
              "\n",
              "Please use Windows and Python 64-bit version.\n",
              " <P> Try this if you have added the Anaconda directory to your bash shell PATH environment variable:\n",
              " \n",
              " \n",
              " \n",
              " shell\n",
              " \n",
              " CC=clang CXX=clang++ python setup.py install\n",
              " \n",
              "  <P> The wheel I was trying to install required 32 bit Python, I had 64 bit Python installed. Therefore, the wheel I was trying to install was not compatible with my Python version.\n",
              "Checking Python Version:\n",
              "I confirmed my Python version using the following command:\n",
              "python -c \"import struct; print(struct.calcsize('P') * 8)\"\n",
              "Checking Wheel Version:\n",
              "64 bit wheels typically contain amd64 or similar in their title\n",
              "32 bit wheels typically contain win32or similar in their title\n",
              "Switching to a 64 bit Python Installaion:\n",
              "The default Windows Python installer does not have a 64 bit option. To acquire 64 bit Python, navigate to python.org/downloads/windows/, and select a version that specifies x86-64 (the other versions are all 32 bit).\n",
              "\n",
              "\n",
              "Credit to phd for the comment that led to this solution.\n",
              "A Redditor had the same problem here.\n",
              " <P> Definitely a conda 3.6 thing, fresh install into a 3.5 environment works perfectly :+1: <P> I believe this should now be resolved for `macOS`:\n",
              "\n",
              "pytorch37 ❯ pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
              "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
              "Collecting torch\n",
              " Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.5.0.dev20200110-cp37-none-macosx_10_9_x86_64.whl (82.4MB)\n",
              "\n",
              "Not entirely sure about where we are on windows nightly builds though, cc @peterjc123 <P> Get the Microsoft Visual C++ Redistributable installer from the link in the error, in this case is this.\n",
              "Run the installer and launch again your shell with conda configured when finished\n",
              " <P> Update conda first with `conda update conda` and try again <P> Problem solved. \n",
              "\n",
              "I found what was wrong and I fixed it. The whole problem lies in the fact that Anaconda distribution comes with its own ld linker that is located in /opt/anaconda/compiler_compat/ and it overshadows system ld residing at /usr/bin.\n",
              "\n",
              "To fix my error I ran python setup.py clean and then I temporarily renamed Anaconda's ld linker to ld-old to make it invisible during PyTorch installation.\n",
              " <P> @pjh5 \n",
              "Updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda` fixed it! I'm able to properly install pytorch now by running `conda install pytorch torchvision -c pytorch`. \n",
              "Thanks for the help. <P> I fixed the problem by importing opencv before torch.</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col0\" class=\"data row4 col0\" >TypeError: add(): argument &#39;other&#39; (position 1) must be Tensor, not numpy.ndarray</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col1\" class=\"data row4 col1\" >To solve your problem you may use:\n",
              "\n",
              "temp = list()\n",
              "for key, b in reader:\n",
              "    temp.append(torch.from_numpy(b))\n",
              "labels = torch.cat(temp)\n",
              "For more, you can check the manual here\n",
              "\n",
              "Cheers</td>\n",
              "                        <td id=\"T_198b3d10_185d_11ec_807b_0242ac1c0002row4_col2\" class=\"data row4 col2\" ><P> As the error message explains, c is a tensor. To use torch.cat() you must pass a group of tensors or a list. To solve your problem you may use:\n",
              "\n",
              "temp = list()\n",
              "for key, b in reader:\n",
              "    temp.append(torch.from_numpy(b))\n",
              "labels = torch.cat(temp)\n",
              "\n",
              "\n",
              "For more, you can check the manual here\n",
              "\n",
              "Cheers\n",
              " <P> The pytorch LSTM returns a tuple. So you get this error as your second LSTM layer self.seq2 can not handle this tuple. So, \n",
              "change \n",
              "\n",
              "prefix1=self.seq1(input1) \n",
              "suffix1=self.seq1(input2)\n",
              "\n",
              "\n",
              "to something like this:\n",
              "\n",
              "prefix1_out, prefix1_states = self.seq1(input1) \n",
              "suffix1_out, suffix1_states = self.seq1(input2) \n",
              "\n",
              "\n",
              "and then pass prefix1_out and suffix1_out tensors to the next LSTM layers as\n",
              "\n",
              "prefix2_out, prefix2_states = self.seq2(prefix1_out) \n",
              "suffix2_out, suffix2_states = self.seq2(suffix1_out)\n",
              "\n",
              "\n",
              "And, concat prefix1_out and suffix1_out tensors like this\n",
              "\n",
              "result = torch.cat([out1,out2],1) \n",
              "\n",
              "\n",
              "Also, change \n",
              "\n",
              "r1=F.sigmoid(self.fc1(result)) \n",
              "r2=self.fc2(r1)\n",
              "\n",
              "\n",
              "to something like this:\n",
              "\n",
              "out_ll = self.fc1(result)\n",
              "r1 = nn.Sigmoid() \n",
              "r2 = self.fc2(r1(out_ll))\n",
              "\n",
              " <P> It looks like your input is a numpy array, not torch tensor. You need to convert it first, like input = torch.Tensor(input).\n",
              " <P> Your issue is here:\n",
              "signals = np.stack([src.signal for src in pst_sources], axis=1) # signals shape: [Len, n_signals]\n",
              "\n",
              "It looks like pst_sources is empty, and so you are trying to stack an empty list.\n",
              " <P> As stated by user8426627 you want to change the tensor type, not the data type. Therefore the solution was to add .type(torch.LongTensor) to convert it to a LongTensor.\n",
              "\n",
              "Final code:\n",
              "\n",
              "Ytrain_ = torch.from_numpy(Y_train.values).view(1, -1)[0].type(torch.LongTensor)\n",
              "\n",
              "Test tensor type:\n",
              "\n",
              "Ytrain_.type()\n",
              "\n",
              "'torch.LongTensor'\n",
              " <P> I can't confirm but I believe your problem will be solved by changing:\n",
              "\n",
              "train_y = np.array(train_labels) == 'fake'\n",
              "test_y = np.array(test_labels) == 'fake'\n",
              "\n",
              "\n",
              "to:\n",
              "\n",
              "train_y = (np.array(train_labels) == 'fake').astype(int)\n",
              "test_y = (np.array(test_labels) == 'fake').astype(int)\n",
              "\n",
              "\n",
              "The train_y data is currently an array of type Bool (True or False) and the tensor needs and int (0 or 1).\n",
              " <P> I am not sure the rest of your code is alright, but in order to fix this error, you can convert your rnn_out list to a torch tensor by adding the following line after the ending of your for loop:\n",
              "rnn_out = torch.stack(rnn_out)\n",
              "\n",
              " <P> I had same issue\n",
              "resolved\n",
              "Before converting to Tensor, try this\n",
              "X_train = X_train.astype(np.float32)\n",
              "\n",
              " <P> I would suggest you to check the input type\n",
              "I had the same issue which solved by converting the input type from  int32 to int64.(running on win10)\n",
              "ex:\n",
              "\n",
              "x = torch.tensor(train).to(torch.int64)\n",
              "\n",
              " <P> The error message says it all. The tensors involved contain elements of different data types. By default, w and b have elements of type torch.float32, while data_input is a NumPy array with the Python default floating point type, i.e. double. That datatype will be preserved when you convert with from_numpy. Try using dtype=np.float32 in your np.genfromtxt call.\n",
              "</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f6d894775d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}